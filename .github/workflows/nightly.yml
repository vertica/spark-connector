name: Nightly Testing

on:
  schedule:
    # Nightly at 9:18 am GMT, or 2:18 am Pacific Time, from Sunday to Thursday
    - cron: '18 9 * * 0-4'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Build the project
        run: cd connector && sbt package
      - name: Upload the build artifact
        uses: actions/upload-artifact@v3
        with:
          name: build-jar-file
          path: /home/runner/work/spark-connector/spark-connector/connector/target/scala-2.12/spark-vertica-connector_2.12-*.jar
  run-spark-functional-tests:
    strategy:
      fail-fast: false
      matrix:
        spark: ["[3.0.0, 3.1.0)", "[3.1.0, 3.2.0)", "[3.2.0, 3.3.0)", "[3.3.0, 3.4.0)"]
        vertica: ["10.1.1-0", ","11.1.1-2", "12.0.0-0"]
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Run docker compose
        run: cd docker && docker-compose up -d
        env:
          VERTICA_VERSION: ${{matrix.vertica}}
          SPARK_VERSION: ${{matrix.spark}}
          HADOOP_VERSION: ${{matrix.hadoop}}
      - name: Create db in Vertica
        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
      - name: Replace HDFS core-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
      - name: Replace HDFS hdfs-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
      - name: Copy partitioned parquet data to HDFS container
        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
      - name: Copy partitioned parquet data to hadoop from local
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
      - name: Download the build artifact
        uses: actions/download-artifact@v2
        with:
          name: build-jar-file
          path: ./functional-tests/lib/
      - name: Increase active sessions in database
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
      - name: Copy functional tests to home directory of client container
        run: docker exec docker_client_1 cp -r /spark-connector/functional-tests /home
      - name: Copy version.properties file from client container
        run: docker exec docker_client_1 cp -r /spark-connector/version.properties /home
      - name: Run the integration tests on Spark 3.0
        run: docker exec -w /home/functional-tests docker_client_1 sbt run
      - name: Remove docker containers
        run: cd docker && docker-compose down
  run-remote-tests:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Download the build artifact
        uses: actions/download-artifact@v2
        with:
          name: build-jar-file
          path: ./functional-tests/lib/
      - name: Build the functional-tests project
        run: cd functional-tests && sbt assembly
      - name: Build client image
        run: docker build -t client ./docker
      - name: Run docker compose
        run: cd docker && docker-compose up -d
        env:
          VERTICA_VERSION: $VERTICA_DEFAULT
          HADOOP_VERSION: "3.3.0"
          SPARK_VERSION: "3.2.1"
      - name: Create db in Vertica
        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
      - name: Replace HDFS core-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
      - name: Replace HDFS hdfs-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
      - name: Copy partitioned parquet data to HDFS container
        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
      - name: Copy partitioned parquet data to hadoop from local
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
      - name: Increase active sessions in database
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
      - name: Run the integration tests
        run: docker exec -w /spark-connector/functional-tests docker_client_1 /bin/bash -c './submit-functional-tests.sh "-r -s RemoteTests"'
      - name: Remove docker containers
        run: cd docker && docker-compose down
  run-integration-tests_s3:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Download the build artifact
        uses: actions/download-artifact@v2
        with:
          name: build-jar-file
          path: ./functional-tests/lib/
      - name: Add config options to application.conf
        run: cd functional-tests && ./pipeline-s3-config.sh
        env:
          S3_FILEPATH: ${{secrets.S3_FILEPATH}}
      - name: Build the functional-tests project
        run: cd functional-tests && sbt assembly
      - name: Build client image
        run: docker build -t client ./docker
      - name: Run docker compose
        run: cd docker && docker-compose up -d
        env:
          VERTICA_VERSION: $VERTICA_DEFAULT
          AWS_ACCESS_KEY_ID: ${{secrets.AWS_ACCESS_KEY_ID}}
          AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_SECRET_ACCESS_KEY}}
          HADOOP_VERSION: "3.3.1"
          SPARK_VERSION: "3.2.0"
      - name: Create db in Vertica
        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
      - name: Replace HDFS core-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
      - name: Replace HDFS hdfs-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
      - name: Copy partitioned parquet data to HDFS container
        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
      - name: Copy partitioned parquet data to hadoop from local
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
      - name: Increase active sessions in database
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
      - name: Run the integration tests
        run: docker exec -w /spark-connector/functional-tests docker_client_1 /bin/bash -c "./submit-functional-tests.sh"
      - name: Remove docker containers
        run: cd docker && docker-compose down
  run-integration-tests_gcs:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Download the build artifact
        uses: actions/download-artifact@v2
        with:
          name: build-jar-file
          path: ./functional-tests/lib/
      - name: Add config options to application.conf
        run: cd functional-tests && ./pipeline-gcs-config.sh
        env:
          GCS_FILEPATH: ${{secrets.GCS_FILEPATH}}
      - name: Build the functional-tests project
        run: cd functional-tests && sbt assembly
      - name: Build client image
        run: docker build -t client ./docker
      - name: Run docker compose
        run: cd docker && docker-compose up -d
        env:
          VERTICA_VERSION: $VERTICA_DEFAULT
          GCS_HMAC_KEY_ID: ${{secrets.GCS_HMAC_KEY_ID}}
          GCS_HMAC_KEY_SECRET: ${{secrets.GCS_HMAC_KEY_SECRET}}
          GCS_SERVICE_KEY_ID: ${{secrets.GCS_SERVICE_KEY_ID}}
          GCS_SERVICE_KEY: ${{secrets.GCS_SERVICE_KEY}}
          GCS_SERVICE_EMAIL: ${{secrets.GCS_SERVICE_EMAIL}}
      - name: Create db in Vertica
        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
      - name: Replace HDFS core-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
      - name: Replace HDFS hdfs-site config with our own
        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
      - name: Copy partitioned parquet data to HDFS container
        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
      - name: Copy partitioned parquet data to hadoop from local
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
      - name: Increase active sessions in database
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 20
          max_attempts: 10
          retry_on: error
          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
      - name: Run the integration tests against GCS
        run: docker exec -w /spark-connector/functional-tests docker_client_1 sbt run -DsparkVersion="3.2.1" -DhadoopVersion="3.3.1"
      - name: Remove docker containers
        run: cd docker && docker-compose down
#  slack-workflow-status:
#    name: Post Workflow Status To Slack
#    runs-on: ubuntu-latest
#    needs: [run-integration-tests, run-integration-tests_gcs, run-integration-tests_s3, run-remote-tests]
#    if: failure()
#    steps:
#      - name: Slack Workflow Notification
#        uses: Gamesight/slack-workflow-status@master
#        if: always()
#        with:
#          repo_token: ${{secrets.GITHUB_TOKEN}}
#          slack_webhook_url: ${{secrets.SLACK_CHANNEL_WEBHOOK}}
#          name: 'Nightly tests failed'
#          icon_emoji: ':sadge:'
