# Workflow for running Unit tests and Integration tests
name: test-ci

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Build the project
        run: cd connector && sbt package
      - name: Upload the build artifact
        uses: actions/upload-artifact@v1
        with:
          name: build-jar-file
          path: /home/runner/work/spark-connector/spark-connector/connector/target/scala-2.12/spark-vertica-connector_2.12-3.0.3.jar
  run-analysis:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Run scalastyle
        run: cd connector && sbt scalastyle
  run-unit-tests:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Run unit tests
        run: cd connector && sbt coverage test coverageReport
      - name: Prepare code coverage badge
        if: always()
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-utils
          cd ./connector/target/scala-2.12/scoverage-report
          coverage=$(xmllint --xpath "scoverage/@statement-rate" scoverage.xml |sed 's/statement-rate="//'| sed 's/"//')
          
          if (( $(echo "$coverage >= 80" | bc -l) )) ; then
             COLOR=brightgreen
          else
             COLOR=red
          fi
          
          wget "https://img.shields.io/badge/coverage-$coverage%25-$COLOR" -O ../../../../img/coverage-badge.svg
      - name: Upload coverage badge
        uses: actions/upload-artifact@v1
        with:
          name: coverage-badge.svg
          path: /home/runner/work/spark-connector/spark-connector/img/coverage-badge.svg
  run-integration-tests-spark-3-0-2:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Run docker compose
        run: cd docker && docker-compose up -d
      - name: Create db in Vertica
        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
#      - name: Replace HDFS core-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
#      - name: Replace HDFS hdfs-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
#      - name: Copy partitioned parquet data to HDFS container
#        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
#      - name: Copy partitioned parquet data to hadoop from local
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
#      - name: Download the build artifact
#        uses: actions/download-artifact@v2
#        with:
#          name: build-jar-file
#          path: ./functional-tests/lib/
#      - name: Increase active sessions in database
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
#      - name: Copy functional tests to home directory of client container
#        run: docker exec docker_client_1 cp -r /spark-connector/functional-tests /home
#      - name: Run the integration tests on Spark 3.0
#        run: docker exec -w /home/functional-tests docker_client_1 sbt run -DsparkVersion="3.0.2" -DhadoopVersion="2.4.0"
#      - name: Remove docker containers
#        run: cd docker && docker-compose down
  run-integration-tests-spark-3-1-1:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Build client image
        run: docker build -t client ./docker
#      - name: Run docker compose
#        run: cd docker && docker-compose up -d
#      - name: Create db in Vertica
#        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
#      - name: Replace HDFS core-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
#      - name: Replace HDFS hdfs-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
#      - name: Copy partitioned parquet data to HDFS container
#        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
#      - name: Copy partitioned parquet data to hadoop from local
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
#      - name: Download the build artifact
#        uses: actions/download-artifact@v2
#        with:
#          name: build-jar-file
#          path: ./functional-tests/lib/
#      - name: Increase active sessions in database
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
#      - name: Copy functional tests to home directory of client container
#        run: docker exec docker_client_1 cp -r /spark-connector/functional-tests /home
#      - name: Run the integration tests on Spark 3.1
#        run: docker exec -w /home/functional-tests docker_client_1 sbt run -DsparkVersion="3.1.1" -DhadoopVersion="3.3.1"
#      - name: Remove docker containers
#        run: cd docker && docker-compose down
  run-integration-tests-spark-3-2-0:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Build client image
        run: docker build -t client ./docker
      - name: Run docker compose
#        run: cd docker && docker-compose up -d
#      - name: Create db in Vertica
#        run: docker exec docker_vertica_1 /bin/sh -c "opt/vertica/bin/admintools -t create_db --database=docker --password='' --hosts=localhost"
#      - name: Replace HDFS core-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/core-site.xml /opt/hadoop/etc/hadoop/core-site.xml
#      - name: Replace HDFS hdfs-site config with our own
#        run: docker exec docker_hdfs_1 cp /hadoop/conf/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
#      - name: Copy partitioned parquet data to HDFS container
#        run: docker cp ./functional-tests/src/main/resources/3.1.1 docker_hdfs_1:/partitioned
#      - name: Copy partitioned parquet data to hadoop from local
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_hdfs_1 hadoop fs -copyFromLocal /partitioned /3.1.1
#      - name: Download the build artifact
#        uses: actions/download-artifact@v2
#        with:
#          name: build-jar-file
#          path: ./functional-tests/lib/
#      - name: Increase active sessions in database
#        uses: nick-invision/retry@v2
#        with:
#          timeout_seconds: 20
#          max_attempts: 10
#          retry_on: error
#          command: docker exec docker_vertica_1 vsql -c "ALTER DATABASE docker SET MaxClientSessions=100;"
#      - name: Copy functional tests to home directory of client container
#        run: docker exec docker_client_1 cp -r /spark-connector/functional-tests /home
#      - name: Run the integration tests on Spark 3.2
#        run: docker exec -w /home/functional-tests docker_client_1 sbt run -DsparkVersion="3.2.0" -DhadoopVersion="3.0.1"
#      - name: Remove docker containers
#        run: cd docker && docker-compose down
  publish-code-coverage-badge:
    runs-on: ubuntu-latest
    needs: [run-analysis, run-unit-tests, run-integration-tests-spark-3-2-0, run-integration-tests-spark-3-1-1, run-integration-tests-spark-3-0-2]
    steps:
      - name: Checkout the project
        uses: actions/checkout@v2
      - name: Download badge
        uses: actions/download-artifact@v2
        with:
          name: coverage-badge.svg
          path: ./img/
      - name: Commit badge
        run: |
          git add ./img/coverage-badge.svg
          git commit "Adding code coverage badge"
          git push
#        uses: stefanzweifel/git-auto-commit-action@v4
#        with:
#          commit_options: '--allow-empty --no-verify'
#          commit_message: Commit code coverage
#          skip_dirty_check: true
#          file_pattern: '*.svg'
