<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com\vertica\spark\util\schema\SchemaTools.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>// (c) Copyright [2020-2021] Micro Focus or one of its affiliates.
</span>2 <span style=''>// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
</span>3 <span style=''>// You may not use this file except in compliance with the License.
</span>4 <span style=''>// You may obtain a copy of the License at
</span>5 <span style=''>//
</span>6 <span style=''>// http://www.apache.org/licenses/LICENSE-2.0
</span>7 <span style=''>//
</span>8 <span style=''>// Unless required by applicable law or agreed to in writing, software
</span>9 <span style=''>// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
</span>10 <span style=''>// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span>11 <span style=''>// See the License for the specific language governing permissions and
</span>12 <span style=''>// limitations under the License.
</span>13 <span style=''>
</span>14 <span style=''>package com.vertica.spark.util.schema
</span>15 <span style=''>
</span>16 <span style=''>import com.vertica.spark.datasource.jdbc._
</span>17 <span style=''>import org.apache.spark.sql.types._
</span>18 <span style=''>import java.sql.ResultSetMetaData
</span>19 <span style=''>
</span>20 <span style=''>import cats.data.NonEmptyList
</span>21 <span style=''>import cats.implicits._
</span>22 <span style=''>import scala.util.Either
</span>23 <span style=''>import com.vertica.spark.config.{LogProvider, TableName, TableQuery, TableSource, ValidColumnList}
</span>24 <span style=''>import com.vertica.spark.util.error.ErrorHandling.{ConnectorResult, SchemaResult}
</span>25 <span style=''>import com.vertica.spark.util.error._
</span>26 <span style=''>
</span>27 <span style=''>import scala.util.control.Breaks.{break, breakable}
</span>28 <span style=''>
</span>29 <span style=''>case class ColumnDef(
</span>30 <span style=''>                      label: String,
</span>31 <span style=''>                      colType: Int,
</span>32 <span style=''>                      colTypeName: String,
</span>33 <span style=''>                      size: Int,
</span>34 <span style=''>                      scale: Int,
</span>35 <span style=''>                      signed: Boolean,
</span>36 <span style=''>                      nullable: Boolean,
</span>37 <span style=''>                      metadata: Metadata)
</span>38 <span style=''>
</span>39 <span style=''>/**
</span>40 <span style=''> * Interface for functionality around retrieving and translating schema.
</span>41 <span style=''> */
</span>42 <span style=''>trait SchemaToolsInterface {
</span>43 <span style=''>  /**
</span>44 <span style=''>   * Retrieves the schema of Vertica table in Spark format.
</span>45 <span style=''>   *
</span>46 <span style=''>   * @param jdbcLayer Depedency for communicating with Vertica over JDBC
</span>47 <span style=''>   * @param tableSource The table/query we want the schema of
</span>48 <span style=''>   * @return StructType representing table's schema converted to Spark's schema type.
</span>49 <span style=''>   */
</span>50 <span style=''>  def readSchema(jdbcLayer: JdbcLayerInterface, tableSource: TableSource): ConnectorResult[StructType]
</span>51 <span style=''>
</span>52 <span style=''>  /**
</span>53 <span style=''>   * Retrieves the schema of Vertica table in format of list of column definitions.
</span>54 <span style=''>   *
</span>55 <span style=''>   * @param jdbcLayer Depedency for communicating with Vertica over JDBC
</span>56 <span style=''>   * @param tableSource The table/query we want the schema of.
</span>57 <span style=''>   * @return Sequence of ColumnDef, representing the Vertica structure of schema.
</span>58 <span style=''>   */
</span>59 <span style=''>  def getColumnInfo(jdbcLayer: JdbcLayerInterface, tableSource: TableSource): ConnectorResult[Seq[ColumnDef]]
</span>60 <span style=''>
</span>61 <span style=''>  /**
</span>62 <span style=''>   * Returns the Vertica type to use for a given Spark type.
</span>63 <span style=''>   *
</span>64 <span style=''>   * @param sparkType One of Sparks' DataTypes
</span>65 <span style=''>   * @param strlen Necessary if the type is StringType, string length to use for Vertica type.
</span>66 <span style=''>   * @return String representing Vertica type, that one could use in a create table statement
</span>67 <span style=''>   */
</span>68 <span style=''>  def getVerticaTypeFromSparkType (sparkType: org.apache.spark.sql.types.DataType, strlen: Long): SchemaResult[String]
</span>69 <span style=''>
</span>70 <span style=''>  /**
</span>71 <span style=''>   * Compares table schema and spark schema to return a list of columns to use when copying spark data to the given Vertica table.
</span>72 <span style=''>   *
</span>73 <span style=''>   * @param jdbcLayer Depedency for communicating with Vertica over JDBC
</span>74 <span style=''>   * @param tableName Name of the table we want to copy to.
</span>75 <span style=''>   * @param schema Schema of data in spark.
</span>76 <span style=''>   * @return
</span>77 <span style=''>   */
</span>78 <span style=''>  def getCopyColumnList(jdbcLayer: JdbcLayerInterface, tableName: TableName, schema: StructType): ConnectorResult[String]
</span>79 <span style=''>
</span>80 <span style=''>  /**
</span>81 <span style=''>   * Matches a list of columns against a required schema, only returning the list of matches in string form.
</span>82 <span style=''>   *
</span>83 <span style=''>   * @param columnDefs List of column definitions from the Vertica table.
</span>84 <span style=''>   * @param requiredSchema Set of columns in Spark schema format that we want to limit the column list to.
</span>85 <span style=''>   * @return List of columns in matches.
</span>86 <span style=''>   */
</span>87 <span style=''>  def makeColumnsString(columnDefs: Seq[ColumnDef], requiredSchema: StructType): String
</span>88 <span style=''>
</span>89 <span style=''>  /**
</span>90 <span style=''>   * Converts spark schema to table column defs in Vertica format
</span>91 <span style=''>   *
</span>92 <span style=''>   * @param schema Schema in spark format
</span>93 <span style=''>   * @return List of column names and types, that can be used in a Vertica CREATE TABLE.
</span>94 <span style=''>   * */
</span>95 <span style=''>  def makeTableColumnDefs(schema: StructType, strlen: Long): ConnectorResult[String]
</span>96 <span style=''>
</span>97 <span style=''>  /**
</span>98 <span style=''>   * Gets a list of column values to be inserted within a merge.
</span>99 <span style=''>   *
</span>100 <span style=''>   * @param copyColumnList String of columns passed in by user as a configuration option.
</span>101 <span style=''>   * @return String of values to append to INSERT VALUES in merge.
</span>102 <span style=''>   */
</span>103 <span style=''>  def getMergeInsertValues(jdbcLayer: JdbcLayerInterface, tableName: TableName, copyColumnList: Option[ValidColumnList]): ConnectorResult[String]
</span>104 <span style=''>
</span>105 <span style=''>  /**
</span>106 <span style=''>   * Gets a list of column values and their updates to be updated within a merge.
</span>107 <span style=''>   *
</span>108 <span style=''>   * @param copyColumnList String of columns passed in by user as a configuration option.
</span>109 <span style=''>   * @param tempTableName Temporary table created as part of merge statement
</span>110 <span style=''>   * @return String of columns and values to append to UPDATE SET in merge.
</span>111 <span style=''>   */
</span>112 <span style=''>  def getMergeUpdateValues(jdbcLayer: JdbcLayerInterface, tableName: TableName, tempTableName: TableName, copyColumnList: Option[ValidColumnList]): ConnectorResult[String]
</span>113 <span style=''>
</span>114 <span style=''>  /**
</span>115 <span style=''>  * Replaces columns of unknown type with partial schema specified with empty DF
</span>116 <span style=''>  *
</span>117 <span style=''>  * @param createExternalTableStmt Original create table statement retrieved using SELECT INFER_EXTERNAL_TABLE_DDL
</span>118 <span style=''>  * @param schema Schema passed in with empty dataframe
</span>119 <span style=''>  * @return Updated create external table statement
</span>120 <span style=''>  */
</span>121 <span style=''>  def inferExternalTableSchema(createExternalTableStmt: String, schema: StructType, tableName: String, strlen: Long): ConnectorResult[String]
</span>122 <span style=''>}
</span>123 <span style=''>
</span>124 <span style=''>class SchemaTools extends SchemaToolsInterface {
</span>125 <span style=''>  private val logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[SchemaTools])</span><span style=''>
</span>126 <span style=''>  private val unknown = </span><span style='background: #AEF1AE'>&quot;UNKNOWN&quot;</span><span style=''>
</span>127 <span style=''>  private val maxlength = </span><span style='background: #AEF1AE'>&quot;maxlength&quot;</span><span style=''>
</span>128 <span style=''>  private val longlength = </span><span style='background: #AEF1AE'>65000</span><span style=''>
</span>129 <span style=''>
</span>130 <span style=''>  private def addDoubleQuotes(str: String): String = {
</span>131 <span style=''>    </span><span style='background: #AEF1AE'>&quot;\&quot;&quot; + str + &quot;\&quot;&quot;</span><span style=''>
</span>132 <span style=''>  }
</span>133 <span style=''>
</span>134 <span style=''>  private def getCatalystType(
</span>135 <span style=''>    sqlType: Int,
</span>136 <span style=''>    precision: Int,
</span>137 <span style=''>    scale: Int,
</span>138 <span style=''>    signed: Boolean,
</span>139 <span style=''>    typename: String): Either[SchemaError, DataType] = {
</span>140 <span style=''>    val answer = sqlType match {
</span>141 <span style=''>      // scalastyle:off
</span>142 <span style=''>      case java.sql.Types.ARRAY =&gt; </span><span style='background: #F0ADAD'>null</span><span style=''>
</span>143 <span style=''>      case java.sql.Types.BIGINT =&gt;  if (signed) { </span><span style='background: #AEF1AE'>LongType</span><span style=''> } else { </span><span style='background: #AEF1AE'>DecimalType(DecimalType.MAX_PRECISION,0)</span><span style=''>} //spark 2.x
</span>144 <span style=''>      case java.sql.Types.BINARY =&gt; </span><span style='background: #AEF1AE'>BinaryType</span><span style=''>
</span>145 <span style=''>      case java.sql.Types.BIT =&gt; </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>146 <span style=''>      case java.sql.Types.BLOB =&gt; </span><span style='background: #AEF1AE'>BinaryType</span><span style=''>
</span>147 <span style=''>      case java.sql.Types.BOOLEAN =&gt; </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>148 <span style=''>      case java.sql.Types.CHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>149 <span style=''>      case java.sql.Types.CLOB =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>150 <span style=''>      case java.sql.Types.DATALINK =&gt; </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>151 <span style=''>      case java.sql.Types.DATE =&gt; </span><span style='background: #AEF1AE'>DateType</span><span style=''>
</span>152 <span style=''>      case java.sql.Types.DECIMAL =&gt; </span><span style='background: #AEF1AE'>DecimalType(precision, scale)</span><span style=''>
</span>153 <span style=''>      case java.sql.Types.DISTINCT =&gt; </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>154 <span style=''>      case java.sql.Types.DOUBLE =&gt; </span><span style='background: #AEF1AE'>DoubleType</span><span style=''>
</span>155 <span style=''>      case java.sql.Types.FLOAT =&gt; </span><span style='background: #AEF1AE'>FloatType</span><span style=''>
</span>156 <span style=''>      case java.sql.Types.INTEGER =&gt; if (signed) { </span><span style='background: #AEF1AE'>IntegerType</span><span style=''> } else { </span><span style='background: #F0ADAD'>LongType</span><span style=''> }
</span>157 <span style=''>      case java.sql.Types.JAVA_OBJECT =&gt; </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>158 <span style=''>      case java.sql.Types.LONGNVARCHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>159 <span style=''>      case java.sql.Types.LONGVARBINARY =&gt; </span><span style='background: #AEF1AE'>BinaryType</span><span style=''>
</span>160 <span style=''>      case java.sql.Types.LONGVARCHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>161 <span style=''>      case java.sql.Types.NCHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>162 <span style=''>      case java.sql.Types.NCLOB =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>163 <span style=''>      case java.sql.Types.NULL =&gt; </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>164 <span style=''>      case java.sql.Types.NUMERIC if </span><span style='background: #AEF1AE'>precision != 0 || </span><span style='background: #F0ADAD'>scale != 0</span><span style=''> =&gt; </span><span style='background: #AEF1AE'>DecimalType(precision, scale)</span><span style=''>
</span>165 <span style=''>      case java.sql.Types.NUMERIC =&gt; </span><span style='background: #F0ADAD'>DecimalType(DecimalType.USER_DEFAULT.precision,DecimalType.USER_DEFAULT.scale)</span><span style=''> //spark 2.x
</span>166 <span style=''>      case java.sql.Types.NVARCHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>167 <span style=''>      case java.sql.Types.OTHER =&gt;
</span>168 <span style=''>        val typenameNormalized = </span><span style='background: #AEF1AE'>typename.toLowerCase()</span><span style=''>
</span>169 <span style=''>        if (</span><span style='background: #AEF1AE'>typenameNormalized.startsWith(&quot;interval&quot;) || typenameNormalized.startsWith(&quot;uuid&quot;)</span><span style=''>) </span><span style='background: #AEF1AE'>StringType</span><span style=''> else </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>170 <span style=''>      case java.sql.Types.REAL =&gt; </span><span style='background: #AEF1AE'>DoubleType</span><span style=''>
</span>171 <span style=''>      case java.sql.Types.REF =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>172 <span style=''>      case java.sql.Types.ROWID =&gt; </span><span style='background: #AEF1AE'>LongType</span><span style=''>
</span>173 <span style=''>      case java.sql.Types.SMALLINT =&gt; </span><span style='background: #AEF1AE'>IntegerType</span><span style=''>
</span>174 <span style=''>      case java.sql.Types.SQLXML =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>175 <span style=''>      case java.sql.Types.STRUCT =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>176 <span style=''>      case java.sql.Types.TIME =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>177 <span style=''>      case java.sql.Types.TIMESTAMP =&gt; </span><span style='background: #AEF1AE'>TimestampType</span><span style=''>
</span>178 <span style=''>      case java.sql.Types.TINYINT =&gt; </span><span style='background: #AEF1AE'>IntegerType</span><span style=''>
</span>179 <span style=''>      case java.sql.Types.VARBINARY =&gt; </span><span style='background: #AEF1AE'>BinaryType</span><span style=''>
</span>180 <span style=''>      case java.sql.Types.VARCHAR =&gt; </span><span style='background: #AEF1AE'>StringType</span><span style=''>
</span>181 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>null</span><span style=''>
</span>182 <span style=''>    }
</span>183 <span style=''>
</span>184 <span style=''>    if (</span><span style='background: #AEF1AE'>answer == null</span><span style=''>) </span><span style='background: #AEF1AE'>Left(MissingSqlConversionError(sqlType.toString, typename))</span><span style=''>
</span>185 <span style=''>    else </span><span style='background: #AEF1AE'>Right(answer)</span><span style=''>
</span>186 <span style=''>  }
</span>187 <span style=''>
</span>188 <span style=''>  def readSchema(jdbcLayer: JdbcLayerInterface, tableSource: TableSource): ConnectorResult[StructType] = {
</span>189 <span style=''>    </span><span style='background: #AEF1AE'>this.getColumnInfo(jdbcLayer, tableSource)</span><span style=''> match {
</span>190 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>Left(err)</span><span style=''>
</span>191 <span style=''>      case Right(colInfo) =&gt;
</span>192 <span style=''>        val errorsOrFields: List[Either[SchemaError, StructField]] = </span><span style='background: #AEF1AE'>colInfo.map(info =&gt; {
</span>193 <span style=''></span><span style='background: #AEF1AE'>            this.getCatalystType(info.colType, info.size, info.scale, info.signed, info.colTypeName).map(columnType =&gt;
</span>194 <span style=''></span><span style='background: #AEF1AE'>              StructField(info.label, columnType, info.nullable, info.metadata))
</span>195 <span style=''></span><span style='background: #AEF1AE'>          }).toList</span><span style=''>
</span>196 <span style=''>        </span><span style='background: #AEF1AE'>errorsOrFields
</span>197 <span style=''></span><span style='background: #AEF1AE'>          // converts List[Either[A, B]] to Either[List[A], List[B]]
</span>198 <span style=''></span><span style='background: #AEF1AE'>          .traverse(_.leftMap(err =&gt; NonEmptyList.one(err)).toValidated).toEither
</span>199 <span style=''></span><span style='background: #AEF1AE'>          .map(field =&gt; StructType(field))
</span>200 <span style=''></span><span style='background: #AEF1AE'>          .left.map(errors =&gt; ErrorList(errors))</span><span style=''>
</span>201 <span style=''>    }
</span>202 <span style=''>  }
</span>203 <span style=''>
</span>204 <span style=''>  def getColumnInfo(jdbcLayer: JdbcLayerInterface, tableSource: TableSource): ConnectorResult[Seq[ColumnDef]] = {
</span>205 <span style=''>    // Query for an empty result set from Vertica.
</span>206 <span style=''>    // This is simply so we can load the metadata of the result set
</span>207 <span style=''>    // and use this to retrieve the name and type information of each column
</span>208 <span style=''>    val query = tableSource match {
</span>209 <span style=''>      case tablename: TableName =&gt; </span><span style='background: #AEF1AE'>&quot;SELECT * FROM &quot; + tablename.getFullTableName + &quot; WHERE 1=0&quot;</span><span style=''>
</span>210 <span style=''>      case TableQuery(query, _) =&gt; </span><span style='background: #AEF1AE'>&quot;SELECT * FROM (&quot; + query + &quot;) AS x WHERE 1=0&quot;</span><span style=''>
</span>211 <span style=''>    }
</span>212 <span style=''>
</span>213 <span style=''>    </span><span style='background: #AEF1AE'>jdbcLayer.query(query)</span><span style=''> match {
</span>214 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>Left(JdbcSchemaError(err))</span><span style=''>
</span>215 <span style=''>      case Right(rs) =&gt;
</span>216 <span style=''>        try {
</span>217 <span style=''>          </span><span style='background: #AEF1AE'>val rsmd = rs.getMetaData
</span>218 <span style=''></span><span style='background: #AEF1AE'>          Right((1 to rsmd.getColumnCount).map(idx =&gt; {
</span>219 <span style=''></span><span style='background: #AEF1AE'>            val columnLabel = rsmd.getColumnLabel(idx)
</span>220 <span style=''></span><span style='background: #AEF1AE'>            val dataType = rsmd.getColumnType(idx)
</span>221 <span style=''></span><span style='background: #AEF1AE'>            val typeName = rsmd.getColumnTypeName(idx)
</span>222 <span style=''></span><span style='background: #AEF1AE'>            val fieldSize = DecimalType.MAX_PRECISION
</span>223 <span style=''></span><span style='background: #AEF1AE'>            val fieldScale = rsmd.getScale(idx)
</span>224 <span style=''></span><span style='background: #AEF1AE'>            val isSigned = rsmd.isSigned(idx)
</span>225 <span style=''></span><span style='background: #AEF1AE'>            val nullable = rsmd.isNullable(idx) != ResultSetMetaData.columnNoNulls
</span>226 <span style=''></span><span style='background: #AEF1AE'>            val metadata = new MetadataBuilder().putString(&quot;name&quot;, columnLabel).build()
</span>227 <span style=''></span><span style='background: #AEF1AE'>            ColumnDef(columnLabel, dataType, typeName, fieldSize, fieldScale, isSigned, nullable, metadata)
</span>228 <span style=''></span><span style='background: #AEF1AE'>          }))</span><span style=''>
</span>229 <span style=''>        }
</span>230 <span style=''>        catch {
</span>231 <span style=''>          case e: Throwable =&gt;
</span>232 <span style=''>            </span><span style='background: #F0ADAD'>Left(DatabaseReadError(e).context(&quot;Could not get column info&quot;))</span><span style=''>
</span>233 <span style=''>        }
</span>234 <span style=''>        finally {
</span>235 <span style=''>          </span><span style='background: #AEF1AE'>rs.close()</span><span style=''>
</span>236 <span style=''>        }
</span>237 <span style=''>    }
</span>238 <span style=''>  }
</span>239 <span style=''>
</span>240 <span style=''>  override def getVerticaTypeFromSparkType (sparkType: org.apache.spark.sql.types.DataType, strlen: Long): SchemaResult[String] = {
</span>241 <span style=''>    sparkType match {
</span>242 <span style=''>      case org.apache.spark.sql.types.BinaryType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;VARBINARY(&quot; + longlength + &quot;)&quot;)</span><span style=''>
</span>243 <span style=''>      case org.apache.spark.sql.types.BooleanType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;BOOLEAN&quot;)</span><span style=''>
</span>244 <span style=''>      case org.apache.spark.sql.types.ByteType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;TINYINT&quot;)</span><span style=''>
</span>245 <span style=''>      case org.apache.spark.sql.types.DateType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;DATE&quot;)</span><span style=''>
</span>246 <span style=''>      case org.apache.spark.sql.types.CalendarIntervalType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;INTERVAL&quot;)</span><span style=''>
</span>247 <span style=''>      case org.apache.spark.sql.types.DecimalType() =&gt; </span><span style='background: #AEF1AE'>Right(&quot;DECIMAL&quot;)</span><span style=''>
</span>248 <span style=''>      case org.apache.spark.sql.types.DoubleType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;DOUBLE PRECISION&quot;)</span><span style=''>
</span>249 <span style=''>      case org.apache.spark.sql.types.FloatType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;FLOAT&quot;)</span><span style=''>
</span>250 <span style=''>      case org.apache.spark.sql.types.IntegerType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;INTEGER&quot;)</span><span style=''>
</span>251 <span style=''>      case org.apache.spark.sql.types.LongType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;BIGINT&quot;)</span><span style=''>
</span>252 <span style=''>      case org.apache.spark.sql.types.NullType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;null&quot;)</span><span style=''>
</span>253 <span style=''>      case org.apache.spark.sql.types.ShortType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;SMALLINT&quot;)</span><span style=''>
</span>254 <span style=''>      case org.apache.spark.sql.types.TimestampType =&gt; </span><span style='background: #AEF1AE'>Right(&quot;TIMESTAMP&quot;)</span><span style=''>
</span>255 <span style=''>      case org.apache.spark.sql.types.StringType =&gt;
</span>256 <span style=''>        // here we constrain to 32M, max long type size
</span>257 <span style=''>        // and default to VARCHAR for sizes &lt;= 65K
</span>258 <span style=''>        val vtype = if (</span><span style='background: #AEF1AE'>strlen &gt; longlength</span><span style=''>) </span><span style='background: #AEF1AE'>&quot;LONG VARCHAR&quot;</span><span style=''> else </span><span style='background: #AEF1AE'>&quot;VARCHAR&quot;</span><span style=''>
</span>259 <span style=''>        </span><span style='background: #AEF1AE'>Right(vtype + &quot;(&quot; + strlen.toString + &quot;)&quot;)</span><span style=''>
</span>260 <span style=''>
</span>261 <span style=''>      // To be reconsidered. Store as binary for now
</span>262 <span style=''>      case org.apache.spark.sql.types.ArrayType(_,_) |
</span>263 <span style=''>           org.apache.spark.sql.types.MapType(_,_,_) |
</span>264 <span style=''>           org.apache.spark.sql.types.StructType(_) =&gt; </span><span style='background: #F0ADAD'>Right(&quot;VARBINARY(&quot; + longlength + &quot;)&quot;)</span><span style=''>
</span>265 <span style=''>
</span>266 <span style=''>
</span>267 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>Left(MissingSparkConversionError(sparkType))</span><span style=''>
</span>268 <span style=''>    }
</span>269 <span style=''>  }
</span>270 <span style=''>
</span>271 <span style=''>  def getCopyColumnList(jdbcLayer: JdbcLayerInterface, tableName: TableName, schema: StructType): ConnectorResult[String] = {
</span>272 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>273 <span style=''></span><span style='background: #AEF1AE'>      columns &lt;- getColumnInfo(jdbcLayer, tableName)
</span>274 <span style=''></span><span style='background: #AEF1AE'>
</span>275 <span style=''></span><span style='background: #AEF1AE'>      columnList &lt;- {
</span>276 <span style=''></span><span style='background: #AEF1AE'>        val colCount = columns.length
</span>277 <span style=''></span><span style='background: #AEF1AE'>        var colsFound = 0
</span>278 <span style=''></span><span style='background: #AEF1AE'>        columns.foreach (column =&gt; {
</span>279 <span style=''></span><span style='background: #AEF1AE'>          logger.debug(&quot;Will check that target column: &quot; + column.label + &quot; exist in DF&quot;)
</span>280 <span style=''></span><span style='background: #AEF1AE'>          breakable {
</span>281 <span style=''></span><span style='background: #AEF1AE'>            schema.foreach(s =&gt; {
</span>282 <span style=''></span><span style='background: #AEF1AE'>              logger.debug(&quot;Comparing target table column: &quot; + column.label + &quot; with DF column: &quot; + s.name)
</span>283 <span style=''></span><span style='background: #AEF1AE'>              if (s.name.equalsIgnoreCase(column.label)) {
</span>284 <span style=''></span><span style='background: #AEF1AE'>                colsFound += 1
</span>285 <span style=''></span><span style='background: #AEF1AE'>                logger.debug(&quot;Column: &quot; + s.name + &quot; found in target table and DF&quot;)
</span>286 <span style=''></span><span style='background: #AEF1AE'>                // Data types compatibility is already verified by COPY
</span>287 <span style=''></span><span style='background: #AEF1AE'>                // Check nullability
</span>288 <span style=''></span><span style='background: #AEF1AE'>                // Log a warning if target column is not null and DF column is null
</span>289 <span style=''></span><span style='background: #AEF1AE'>                if (!column.nullable) {
</span>290 <span style=''></span><span style='background: #AEF1AE'>                  </span><span style='background: #F0ADAD'>if (s.nullable) {
</span>291 <span style=''></span><span style='background: #F0ADAD'>                    logger.warn(&quot;S2V: Column &quot; + s.name + &quot; is NOT NULL in target table &quot; + tableName.getFullTableName +
</span>292 <span style=''></span><span style='background: #F0ADAD'>                      &quot; but it's nullable in the DataFrame. Rows with NULL values in column &quot; +
</span>293 <span style=''></span><span style='background: #F0ADAD'>                      s.name + &quot; will be rejected.&quot;)
</span>294 <span style=''></span><span style='background: #F0ADAD'>                  }</span><span style='background: #AEF1AE'>
</span>295 <span style=''></span><span style='background: #AEF1AE'>                }
</span>296 <span style=''></span><span style='background: #AEF1AE'>                break
</span>297 <span style=''></span><span style='background: #AEF1AE'>              }
</span>298 <span style=''></span><span style='background: #AEF1AE'>            })
</span>299 <span style=''></span><span style='background: #AEF1AE'>          }
</span>300 <span style=''></span><span style='background: #AEF1AE'>        })
</span>301 <span style=''></span><span style='background: #AEF1AE'>        // Verify DataFrame column count &lt;= target table column count
</span>302 <span style=''></span><span style='background: #AEF1AE'>        if (!(schema.length &lt;= colCount)) {
</span>303 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>Left(TableNotEnoughRowsError().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame. &quot;
</span>304 <span style=''></span><span style='background: #F0ADAD'>            + &quot; Number of columns in DataFrame: &quot; + schema.length + &quot;. Number of columns in the target table: &quot;
</span>305 <span style=''></span><span style='background: #F0ADAD'>            + tableName.getFullTableName + &quot;: &quot; + colCount))</span><span style='background: #AEF1AE'>
</span>306 <span style=''></span><span style='background: #AEF1AE'>        }
</span>307 <span style=''></span><span style='background: #AEF1AE'>        // Load by Name:
</span>308 <span style=''></span><span style='background: #AEF1AE'>        // if all cols in DataFrame were found in target table
</span>309 <span style=''></span><span style='background: #AEF1AE'>        else if (colsFound == schema.length) {
</span>310 <span style=''></span><span style='background: #AEF1AE'>          var columnList = &quot;&quot;
</span>311 <span style=''></span><span style='background: #AEF1AE'>          var first = true
</span>312 <span style=''></span><span style='background: #AEF1AE'>          schema.foreach(s =&gt; {
</span>313 <span style=''></span><span style='background: #AEF1AE'>            if (first) {
</span>314 <span style=''></span><span style='background: #AEF1AE'>              columnList = &quot;\&quot;&quot; + s.name
</span>315 <span style=''></span><span style='background: #AEF1AE'>              first = false
</span>316 <span style=''></span><span style='background: #AEF1AE'>            }
</span>317 <span style=''></span><span style='background: #AEF1AE'>            else {
</span>318 <span style=''></span><span style='background: #AEF1AE'>              columnList += &quot;\&quot;,\&quot;&quot; + s.name
</span>319 <span style=''></span><span style='background: #AEF1AE'>            }
</span>320 <span style=''></span><span style='background: #AEF1AE'>          })
</span>321 <span style=''></span><span style='background: #AEF1AE'>          columnList = &quot;(&quot; + columnList + &quot;\&quot;)&quot;
</span>322 <span style=''></span><span style='background: #AEF1AE'>          logger.info(&quot;Load by name. Column list: &quot; + columnList)
</span>323 <span style=''></span><span style='background: #AEF1AE'>          Right(columnList)
</span>324 <span style=''></span><span style='background: #AEF1AE'>        }
</span>325 <span style=''></span><span style='background: #AEF1AE'>
</span>326 <span style=''></span><span style='background: #AEF1AE'>        else </span><span style='background: #F0ADAD'>{
</span>327 <span style=''></span><span style='background: #F0ADAD'>          // Load by position:
</span>328 <span style=''></span><span style='background: #F0ADAD'>          // If not all column names in the schema match column names in the target table
</span>329 <span style=''></span><span style='background: #F0ADAD'>          logger.info(&quot;Load by Position&quot;)
</span>330 <span style=''></span><span style='background: #F0ADAD'>          Right(&quot;&quot;)
</span>331 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style='background: #AEF1AE'>
</span>332 <span style=''></span><span style='background: #AEF1AE'>      }
</span>333 <span style=''></span><span style='background: #AEF1AE'>    } yield columnList</span><span style=''>
</span>334 <span style=''>  }
</span>335 <span style=''>
</span>336 <span style=''>  private def castToVarchar: String =&gt; String = colName =&gt; </span><span style='background: #F0ADAD'>colName + &quot;::varchar AS &quot; + addDoubleQuotes(colName)</span><span style=''>
</span>337 <span style=''>
</span>338 <span style=''>  def makeColumnsString(columnDefs: Seq[ColumnDef], requiredSchema: StructType): String = {
</span>339 <span style=''>    val requiredColumnDefs: Seq[ColumnDef] = if (</span><span style='background: #F0ADAD'>requiredSchema.nonEmpty</span><span style=''>) {
</span>340 <span style=''>      </span><span style='background: #F0ADAD'>columnDefs.filter(cd =&gt; requiredSchema.fields.exists(field =&gt; field.name == cd.label))</span><span style=''>
</span>341 <span style=''>    } else {
</span>342 <span style=''>      </span><span style='background: #F0ADAD'>columnDefs</span><span style=''>
</span>343 <span style=''>    }
</span>344 <span style=''>
</span>345 <span style=''>    </span><span style='background: #F0ADAD'>requiredColumnDefs.map(info =&gt; {
</span>346 <span style=''></span><span style='background: #F0ADAD'>      info.colType match {
</span>347 <span style=''></span><span style='background: #F0ADAD'>        case java.sql.Types.OTHER =&gt;
</span>348 <span style=''></span><span style='background: #F0ADAD'>          val typenameNormalized = info.colTypeName.toLowerCase()
</span>349 <span style=''></span><span style='background: #F0ADAD'>          if (typenameNormalized.startsWith(&quot;interval&quot;) ||
</span>350 <span style=''></span><span style='background: #F0ADAD'>            typenameNormalized.startsWith(&quot;uuid&quot;)) {
</span>351 <span style=''></span><span style='background: #F0ADAD'>            castToVarchar(info.label)
</span>352 <span style=''></span><span style='background: #F0ADAD'>          } else {
</span>353 <span style=''></span><span style='background: #F0ADAD'>            addDoubleQuotes(info.label)
</span>354 <span style=''></span><span style='background: #F0ADAD'>          }
</span>355 <span style=''></span><span style='background: #F0ADAD'>        case java.sql.Types.TIME =&gt; castToVarchar(info.label)
</span>356 <span style=''></span><span style='background: #F0ADAD'>        case _ =&gt; addDoubleQuotes(info.label)
</span>357 <span style=''></span><span style='background: #F0ADAD'>      }
</span>358 <span style=''></span><span style='background: #F0ADAD'>    }).mkString(&quot;,&quot;)</span><span style=''>
</span>359 <span style=''>  }
</span>360 <span style=''>
</span>361 <span style=''>  def makeTableColumnDefs(schema: StructType, strlen: Long): ConnectorResult[String] = {
</span>362 <span style=''>    val sb = </span><span style='background: #AEF1AE'>new StringBuilder()</span><span style=''>
</span>363 <span style=''>
</span>364 <span style=''>    </span><span style='background: #AEF1AE'>sb.append(&quot; (&quot;)</span><span style=''>
</span>365 <span style=''>    var first = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>366 <span style=''>    </span><span style='background: #AEF1AE'>schema.foreach(s =&gt; {
</span>367 <span style=''></span><span style='background: #AEF1AE'>      logger.debug(&quot;colname=&quot; + &quot;\&quot;&quot; + s.name + &quot;\&quot;&quot; + &quot;; type=&quot; + s.dataType + &quot;; nullable=&quot; + s.nullable)
</span>368 <span style=''></span><span style='background: #AEF1AE'>      if (!first) {
</span>369 <span style=''></span><span style='background: #AEF1AE'>        </span><span style='background: #F0ADAD'>sb.append(&quot;,\n&quot;)</span><span style='background: #AEF1AE'>
</span>370 <span style=''></span><span style='background: #AEF1AE'>      }
</span>371 <span style=''></span><span style='background: #AEF1AE'>      first = false
</span>372 <span style=''></span><span style='background: #AEF1AE'>      sb.append(&quot;\&quot;&quot; + s.name + &quot;\&quot; &quot;)
</span>373 <span style=''></span><span style='background: #AEF1AE'>
</span>374 <span style=''></span><span style='background: #AEF1AE'>      // remains empty unless we have a DecimalType with precision/scale
</span>375 <span style=''></span><span style='background: #AEF1AE'>      var decimal_qualifier: String = &quot;&quot;
</span>376 <span style=''></span><span style='background: #AEF1AE'>      if (s.dataType.toString.contains(&quot;DecimalType&quot;)) </span><span style='background: #F0ADAD'>{
</span>377 <span style=''></span><span style='background: #F0ADAD'>
</span>378 <span style=''></span><span style='background: #F0ADAD'>        // has precision only
</span>379 <span style=''></span><span style='background: #F0ADAD'>        val p = &quot;DecimalType\\((\\d+)\\)&quot;.r
</span>380 <span style=''></span><span style='background: #F0ADAD'>        if (s.dataType.toString.matches(p.toString)) {
</span>381 <span style=''></span><span style='background: #F0ADAD'>          val p(prec) = s.dataType.toString
</span>382 <span style=''></span><span style='background: #F0ADAD'>          decimal_qualifier = &quot;(&quot; + prec + &quot;)&quot;
</span>383 <span style=''></span><span style='background: #F0ADAD'>        }
</span>384 <span style=''></span><span style='background: #F0ADAD'>
</span>385 <span style=''></span><span style='background: #F0ADAD'>        // has precision and scale
</span>386 <span style=''></span><span style='background: #F0ADAD'>        val ps = &quot;DecimalType\\((\\d+),(\\d+)\\)&quot;.r
</span>387 <span style=''></span><span style='background: #F0ADAD'>        if (s.dataType.toString.matches(ps.toString)) {
</span>388 <span style=''></span><span style='background: #F0ADAD'>          val ps(prec, scale) = s.dataType.toString
</span>389 <span style=''></span><span style='background: #F0ADAD'>          decimal_qualifier = &quot;(&quot; + prec + &quot;,&quot; + scale + &quot;)&quot;
</span>390 <span style=''></span><span style='background: #F0ADAD'>        }
</span>391 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style='background: #AEF1AE'>
</span>392 <span style=''></span><span style='background: #AEF1AE'>
</span>393 <span style=''></span><span style='background: #AEF1AE'>      for {
</span>394 <span style=''></span><span style='background: #AEF1AE'>        col &lt;- getVerticaTypeFromSparkType(s.dataType, strlen) match {
</span>395 <span style=''></span><span style='background: #AEF1AE'>          case Left(err) =&gt;
</span>396 <span style=''></span><span style='background: #AEF1AE'>            return </span><span style='background: #F0ADAD'>Left(SchemaConversionError(err).context(&quot;Schema error when trying to create table&quot;))</span><span style='background: #AEF1AE'>
</span>397 <span style=''></span><span style='background: #AEF1AE'>          case Right(datatype) =&gt; Right(datatype + decimal_qualifier)
</span>398 <span style=''></span><span style='background: #AEF1AE'>        }
</span>399 <span style=''></span><span style='background: #AEF1AE'>        _ = sb.append(col)
</span>400 <span style=''></span><span style='background: #AEF1AE'>        _ = if (!s.nullable) {
</span>401 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>sb.append(&quot; NOT NULL&quot;)</span><span style='background: #AEF1AE'>
</span>402 <span style=''></span><span style='background: #AEF1AE'>        }
</span>403 <span style=''></span><span style='background: #AEF1AE'>      } yield ()
</span>404 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>405 <span style=''>
</span>406 <span style=''>    </span><span style='background: #AEF1AE'>sb.append(&quot;)&quot;)</span><span style=''>
</span>407 <span style=''>    </span><span style='background: #AEF1AE'>Right(sb.toString)</span><span style=''>
</span>408 <span style=''>  }
</span>409 <span style=''>
</span>410 <span style=''>  def getMergeInsertValues(jdbcLayer: JdbcLayerInterface, tableName: TableName, copyColumnList: Option[ValidColumnList]): ConnectorResult[String] = {
</span>411 <span style=''>    val valueList = </span><span style='background: #F0ADAD'>getColumnInfo(jdbcLayer, tableName)</span><span style=''> match {
</span>412 <span style=''>      case Right(info) =&gt; </span><span style='background: #F0ADAD'>Right(info.map(x =&gt; &quot;temp.&quot; + addDoubleQuotes(x.label)).mkString(&quot;,&quot;))</span><span style=''>
</span>413 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(JdbcSchemaError(err))</span><span style=''>
</span>414 <span style=''>    }
</span>415 <span style=''>    valueList
</span>416 <span style=''>  }
</span>417 <span style=''>
</span>418 <span style=''>  def getMergeUpdateValues(jdbcLayer: JdbcLayerInterface, tableName: TableName, tempTableName: TableName, copyColumnList: Option[ValidColumnList]): ConnectorResult[String] = {
</span>419 <span style=''>    val columnList = copyColumnList match {
</span>420 <span style=''>      case Some(list) =&gt; {
</span>421 <span style=''>        val customColList = </span><span style='background: #F0ADAD'>list.toString.split(&quot;,&quot;).toList.map(col =&gt; col.trim())</span><span style=''>
</span>422 <span style=''>        val colList = </span><span style='background: #F0ADAD'>getColumnInfo(jdbcLayer, tempTableName)</span><span style=''> match {
</span>423 <span style=''>          case Right(info) =&gt;
</span>424 <span style=''>            val tupleList = </span><span style='background: #F0ADAD'>customColList zip info</span><span style=''>
</span>425 <span style=''>            </span><span style='background: #F0ADAD'>Right(tupleList.map(x =&gt; addDoubleQuotes(x._1) + &quot;=temp.&quot; + addDoubleQuotes(x._2.label)).mkString(&quot;, &quot;))</span><span style=''>
</span>426 <span style=''>          case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(JdbcSchemaError(err))</span><span style=''>
</span>427 <span style=''>        }
</span>428 <span style=''>        colList
</span>429 <span style=''>      }
</span>430 <span style=''>      case None =&gt; {
</span>431 <span style=''>        val updateList = </span><span style='background: #F0ADAD'>getColumnInfo(jdbcLayer, tempTableName)</span><span style=''> match {
</span>432 <span style=''>          case Right(info) =&gt; </span><span style='background: #F0ADAD'>Right(info.map(x =&gt; addDoubleQuotes(x.label) + &quot;=temp.&quot; + addDoubleQuotes(x.label)).mkString(&quot;, &quot;))</span><span style=''>
</span>433 <span style=''>          case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(JdbcSchemaError(err))</span><span style=''>
</span>434 <span style=''>        }
</span>435 <span style=''>        updateList
</span>436 <span style=''>      }
</span>437 <span style=''>    }
</span>438 <span style=''>    columnList
</span>439 <span style=''>  }
</span>440 <span style=''>  def updateFieldDataType(col: String, colName: String, schema: StructType, strlen: Long): String = {
</span>441 <span style=''>    val fieldType = </span><span style='background: #AEF1AE'>schema.collect {
</span>442 <span style=''></span><span style='background: #AEF1AE'>      case field if(addDoubleQuotes(field.name) == colName) =&gt;
</span>443 <span style=''></span><span style='background: #AEF1AE'>        if (field.metadata.contains(maxlength) &amp;&amp; </span><span style='background: #F0ADAD'>field.dataType.simpleString == &quot;string&quot;</span><span style='background: #AEF1AE'>) {
</span>444 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>if(field.metadata.getLong(maxlength) &gt; longlength) &quot;long varchar(&quot; + field.metadata.getLong(maxlength).toString + &quot;)&quot;
</span>445 <span style=''></span><span style='background: #F0ADAD'>          else &quot;varchar(&quot; + field.metadata.getLong(maxlength).toString + &quot;)&quot;</span><span style='background: #AEF1AE'>
</span>446 <span style=''></span><span style='background: #AEF1AE'>        }
</span>447 <span style=''></span><span style='background: #AEF1AE'>        else if(field.metadata.contains(maxlength) &amp;&amp; </span><span style='background: #F0ADAD'>field.dataType.simpleString == &quot;binary&quot;</span><span style='background: #AEF1AE'>){
</span>448 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>&quot;varbinary(&quot; + field.metadata.getLong(maxlength).toString + &quot;)&quot;</span><span style='background: #AEF1AE'>
</span>449 <span style=''></span><span style='background: #AEF1AE'>        }
</span>450 <span style=''></span><span style='background: #AEF1AE'>        else  {
</span>451 <span style=''></span><span style='background: #AEF1AE'>          getVerticaTypeFromSparkType(field.dataType, strlen) match {
</span>452 <span style=''></span><span style='background: #AEF1AE'>            case Right(dataType) =&gt; dataType
</span>453 <span style=''></span><span style='background: #AEF1AE'>            case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(err)</span><span style='background: #AEF1AE'>
</span>454 <span style=''></span><span style='background: #AEF1AE'>          }
</span>455 <span style=''></span><span style='background: #AEF1AE'>        }
</span>456 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>457 <span style=''>    if(</span><span style='background: #AEF1AE'>fieldType.nonEmpty</span><span style=''>) {
</span>458 <span style=''>      </span><span style='background: #AEF1AE'>colName + &quot; &quot; + fieldType.head</span><span style=''>
</span>459 <span style=''>    }
</span>460 <span style=''>
</span>461 <span style=''>    else {
</span>462 <span style=''>      </span><span style='background: #AEF1AE'>col</span><span style=''>
</span>463 <span style=''>    }
</span>464 <span style=''>  }
</span>465 <span style=''>
</span>466 <span style=''>  def inferExternalTableSchema(createExternalTableStmt: String, schema: StructType, tableName: String, strlen: Long): ConnectorResult[String] = {
</span>467 <span style=''>    val stmt = </span><span style='background: #AEF1AE'>createExternalTableStmt.replace(&quot;\&quot;&quot; + tableName + &quot;\&quot;&quot;, tableName)</span><span style=''>
</span>468 <span style=''>    val indexOfOpeningParantheses = </span><span style='background: #AEF1AE'>stmt.indexOf(&quot;(&quot;)</span><span style=''>
</span>469 <span style=''>    val indexOfClosingParantheses = </span><span style='background: #AEF1AE'>stmt.indexOf(&quot;)&quot;)</span><span style=''>
</span>470 <span style=''>    val schemaString = </span><span style='background: #AEF1AE'>stmt.substring(indexOfOpeningParantheses + 1, indexOfClosingParantheses)</span><span style=''>
</span>471 <span style=''>    val schemaList = </span><span style='background: #AEF1AE'>schemaString.split(&quot;,&quot;).toList</span><span style=''>
</span>472 <span style=''>
</span>473 <span style=''>    val updatedSchema: String = </span><span style='background: #AEF1AE'>schemaList.map(col =&gt; {
</span>474 <span style=''></span><span style='background: #AEF1AE'>      val indexOfFirstDoubleQuote = col.indexOf(&quot;\&quot;&quot;)
</span>475 <span style=''></span><span style='background: #AEF1AE'>      val indexOfSpace = col.indexOf(&quot; &quot;, indexOfFirstDoubleQuote)
</span>476 <span style=''></span><span style='background: #AEF1AE'>      val colName = col.substring(indexOfFirstDoubleQuote, indexOfSpace)
</span>477 <span style=''></span><span style='background: #AEF1AE'>
</span>478 <span style=''></span><span style='background: #AEF1AE'>      if(schema.nonEmpty){
</span>479 <span style=''></span><span style='background: #AEF1AE'>        updateFieldDataType(col, colName, schema, strlen)
</span>480 <span style=''></span><span style='background: #AEF1AE'>      }
</span>481 <span style=''></span><span style='background: #AEF1AE'>      else if(col.toLowerCase.contains(&quot;varchar&quot;)) colName + &quot; varchar(&quot; + strlen + &quot;)&quot;
</span>482 <span style=''></span><span style='background: #AEF1AE'>      else if(col.toLowerCase.contains(&quot;varbinary&quot;)) colName + &quot; varbinary(&quot; + longlength + &quot;)&quot;
</span>483 <span style=''></span><span style='background: #AEF1AE'>      else col
</span>484 <span style=''></span><span style='background: #AEF1AE'>    }).mkString(&quot;,&quot;)</span><span style=''>
</span>485 <span style=''>
</span>486 <span style=''>    if(</span><span style='background: #AEF1AE'>updatedSchema.contains(unknown)</span><span style=''>) {
</span>487 <span style=''>      </span><span style='background: #AEF1AE'>Left(UnknownColumnTypesError().context(unknown + &quot; partitioned column data type.&quot;))</span><span style=''>
</span>488 <span style=''>    }
</span>489 <span style=''>    else </span><span style='background: #AEF1AE'>{
</span>490 <span style=''></span><span style='background: #AEF1AE'>      val updatedCreateTableStmt = stmt.replace(schemaString, updatedSchema)
</span>491 <span style=''></span><span style='background: #AEF1AE'>      logger.info(&quot;Updated create external table statement: &quot; + updatedCreateTableStmt)
</span>492 <span style=''></span><span style='background: #AEF1AE'>      Right(updatedCreateTableStmt)
</span>493 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>494 <span style=''>  }
</span>495 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          2339
        </td>
        <td>
          5465
          -
          5508
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.util.schema.SchemaTools])
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          2340
        </td>
        <td>
          5533
          -
          5542
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;UNKNOWN&quot;
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          2341
        </td>
        <td>
          5569
          -
          5580
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;maxlength&quot;
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          2342
        </td>
        <td>
          5608
          -
          5613
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          65000
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2343
        </td>
        <td>
          5674
          -
          5691
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(str).+(&quot;\&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          2344
        </td>
        <td>
          5952
          -
          5956
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          null
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2347
        </td>
        <td>
          6038
          -
          6063
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2350
        </td>
        <td>
          6026
          -
          6066
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(org.apache.spark.sql.types.DecimalType.MAX_PRECISION, 0)
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2349
        </td>
        <td>
          6026
          -
          6066
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(org.apache.spark.sql.types.DecimalType.MAX_PRECISION, 0)
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2346
        </td>
        <td>
          6008
          -
          6016
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2348
        </td>
        <td>
          6064
          -
          6065
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2345
        </td>
        <td>
          6008
          -
          6016
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          2351
        </td>
        <td>
          6116
          -
          6126
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BinaryType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BinaryType
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          2352
        </td>
        <td>
          6160
          -
          6171
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          2353
        </td>
        <td>
          6206
          -
          6216
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BinaryType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BinaryType
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          2354
        </td>
        <td>
          6254
          -
          6265
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          2355
        </td>
        <td>
          6300
          -
          6310
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          2356
        </td>
        <td>
          6345
          -
          6355
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          2357
        </td>
        <td>
          6394
          -
          6398
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          2358
        </td>
        <td>
          6433
          -
          6441
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DateType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DateType
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          2359
        </td>
        <td>
          6479
          -
          6508
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(precision, scale)
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          2360
        </td>
        <td>
          6547
          -
          6551
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          154
        </td>
        <td>
          2361
        </td>
        <td>
          6588
          -
          6598
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DoubleType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DoubleType
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          2362
        </td>
        <td>
          6634
          -
          6643
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.FloatType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.FloatType
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          2365
        </td>
        <td>
          6716
          -
          6724
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          2364
        </td>
        <td>
          6695
          -
          6706
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.IntegerType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.IntegerType
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          2363
        </td>
        <td>
          6695
          -
          6706
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.IntegerType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.IntegerType
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          2366
        </td>
        <td>
          6716
          -
          6724
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          2367
        </td>
        <td>
          6768
          -
          6772
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          2368
        </td>
        <td>
          6815
          -
          6825
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          2369
        </td>
        <td>
          6869
          -
          6879
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BinaryType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BinaryType
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2370
        </td>
        <td>
          6921
          -
          6931
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          2371
        </td>
        <td>
          6967
          -
          6977
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          2372
        </td>
        <td>
          7013
          -
          7023
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          2373
        </td>
        <td>
          7058
          -
          7062
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2374
        </td>
        <td>
          7113
          -
          7114
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2376
        </td>
        <td>
          7100
          -
          7128
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          precision.!=(0).||(scale.!=(0))
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2375
        </td>
        <td>
          7118
          -
          7128
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.!=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scale.!=(0)
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2377
        </td>
        <td>
          7132
          -
          7161
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(precision, scale)
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          2380
        </td>
        <td>
          7199
          -
          7277
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.DecimalType.apply(org.apache.spark.sql.types.DecimalType.USER_DEFAULT.precision, org.apache.spark.sql.types.DecimalType.USER_DEFAULT.scale)
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          2379
        </td>
        <td>
          7246
          -
          7276
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.scale
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.DecimalType.USER_DEFAULT.scale
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          2378
        </td>
        <td>
          7211
          -
          7245
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.precision
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.DecimalType.USER_DEFAULT.precision
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          2381
        </td>
        <td>
          7328
          -
          7338
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          2382
        </td>
        <td>
          7407
          -
          7429
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.toLowerCase
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          typename.toLowerCase()
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2389
        </td>
        <td>
          7542
          -
          7546
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2383
        </td>
        <td>
          7472
          -
          7482
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;interval&quot;
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2385
        </td>
        <td>
          7442
          -
          7524
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          typenameNormalized.startsWith(&quot;interval&quot;).||(typenameNormalized.startsWith(&quot;uuid&quot;))
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2388
        </td>
        <td>
          7542
          -
          7546
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2384
        </td>
        <td>
          7487
          -
          7524
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.startsWith
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          typenameNormalized.startsWith(&quot;uuid&quot;)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2387
        </td>
        <td>
          7526
          -
          7536
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          2386
        </td>
        <td>
          7526
          -
          7536
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          2390
        </td>
        <td>
          7581
          -
          7591
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DoubleType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DoubleType
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          2391
        </td>
        <td>
          7625
          -
          7635
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          172
        </td>
        <td>
          2392
        </td>
        <td>
          7671
          -
          7679
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          2393
        </td>
        <td>
          7718
          -
          7729
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.IntegerType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.IntegerType
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          2394
        </td>
        <td>
          7766
          -
          7776
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          2395
        </td>
        <td>
          7813
          -
          7823
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          2396
        </td>
        <td>
          7858
          -
          7868
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          2397
        </td>
        <td>
          7908
          -
          7921
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.TimestampType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.TimestampType
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          2398
        </td>
        <td>
          7959
          -
          7970
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.IntegerType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.IntegerType
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          2399
        </td>
        <td>
          8010
          -
          8020
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BinaryType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BinaryType
        </td>
      </tr><tr>
        <td>
          180
        </td>
        <td>
          2400
        </td>
        <td>
          8058
          -
          8068
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          2401
        </td>
        <td>
          8085
          -
          8089
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          2403
        </td>
        <td>
          8152
          -
          8168
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sqlType.toString()
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          2406
        </td>
        <td>
          8121
          -
          8180
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.MissingSqlConversionError, Nothing](com.vertica.spark.util.error.MissingSqlConversionError.apply(sqlType.toString(), typename))
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          2402
        </td>
        <td>
          8105
          -
          8119
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          answer.==(null)
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          2405
        </td>
        <td>
          8121
          -
          8180
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.MissingSqlConversionError, Nothing](com.vertica.spark.util.error.MissingSqlConversionError.apply(sqlType.toString(), typename))
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          2404
        </td>
        <td>
          8126
          -
          8179
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingSqlConversionError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.MissingSqlConversionError.apply(sqlType.toString(), typename)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2407
        </td>
        <td>
          8190
          -
          8203
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, org.apache.spark.sql.types.AtomicType with Product with Serializable](answer)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2408
        </td>
        <td>
          8190
          -
          8203
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, org.apache.spark.sql.types.AtomicType with Product with Serializable](answer)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2409
        </td>
        <td>
          8320
          -
          8362
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getColumnInfo
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.getColumnInfo(jdbcLayer, tableSource)
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          2410
        </td>
        <td>
          8395
          -
          8404
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2421
        </td>
        <td>
          8514
          -
          8514
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField]]
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2412
        </td>
        <td>
          8572
          -
          8581
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.size
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2415
        </td>
        <td>
          8608
          -
          8624
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.colTypeName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.colTypeName
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2414
        </td>
        <td>
          8595
          -
          8606
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.signed
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.signed
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2420
        </td>
        <td>
          8537
          -
          8724
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.getCatalystType(info.colType, info.size, info.scale, info.signed, info.colTypeName).map[org.apache.spark.sql.types.StructField](((columnType: org.apache.spark.sql.types.DataType) =&gt; org.apache.spark.sql.types.StructField.apply(info.label, columnType, info.nullable, info.metadata)))
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2411
        </td>
        <td>
          8558
          -
          8570
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.colType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.colType
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2413
        </td>
        <td>
          8583
          -
          8593
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.scale
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.scale
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2416
        </td>
        <td>
          8670
          -
          8680
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.label
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.label
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2419
        </td>
        <td>
          8658
          -
          8723
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(info.label, columnType, info.nullable, info.metadata)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2418
        </td>
        <td>
          8709
          -
          8722
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.metadata
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.metadata
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2417
        </td>
        <td>
          8694
          -
          8707
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          info.nullable
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2422
        </td>
        <td>
          8503
          -
          8744
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.toList
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          colInfo.map[scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField], Seq[scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField]]](((info: com.vertica.spark.util.schema.ColumnDef) =&gt; this.getCatalystType(info.colType, info.size, info.scale, info.signed, info.colTypeName).map[org.apache.spark.sql.types.StructField](((columnType: org.apache.spark.sql.types.DataType) =&gt; org.apache.spark.sql.types.StructField.apply(info.label, columnType, info.nullable, info.metadata)))))(collection.this.Seq.canBuildFrom[scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField]]).toList
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2423
        </td>
        <td>
          8753
          -
          8753
        </td>
        <td>
          Select
        </td>
        <td>
          cats.instances.ListInstances.catsStdInstancesForList
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsStdInstancesForList
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2425
        </td>
        <td>
          8857
          -
          8896
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.syntax.EitherOps.leftMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.SchemaError, org.apache.spark.sql.types.StructField](x$1).leftMap[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]](((err: com.vertica.spark.util.error.SchemaError) =&gt; cats.data.NonEmptyList.one[com.vertica.spark.util.error.SchemaError](err)))
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2428
        </td>
        <td>
          8856
          -
          8856
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          data.this.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]](data.this.NonEmptyList.catsDataSemigroupForNonEmptyList[com.vertica.spark.util.error.SchemaError])
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2427
        </td>
        <td>
          8856
          -
          8856
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyListInstances.catsDataSemigroupForNonEmptyList
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyList.catsDataSemigroupForNonEmptyList[com.vertica.spark.util.error.SchemaError]
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2424
        </td>
        <td>
          8874
          -
          8895
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.NonEmptyList.one
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cats.data.NonEmptyList.one[com.vertica.spark.util.error.SchemaError](err)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2426
        </td>
        <td>
          8857
          -
          8908
        </td>
        <td>
          Select
        </td>
        <td>
          cats.syntax.EitherOps.toValidated
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxEither[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError], org.apache.spark.sql.types.StructField](cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.SchemaError, org.apache.spark.sql.types.StructField](x$1).leftMap[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]](((err: com.vertica.spark.util.error.SchemaError) =&gt; cats.data.NonEmptyList.one[com.vertica.spark.util.error.SchemaError](err)))).toValidated
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2429
        </td>
        <td>
          8943
          -
          8960
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(field)
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2430
        </td>
        <td>
          8992
          -
          9009
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorList.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorList.apply(errors)
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2431
        </td>
        <td>
          8753
          -
          9010
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.toTraverseOps[List, scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField]](errorsOrFields)(cats.implicits.catsStdInstancesForList).traverse[[+A]cats.data.Validated[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError],A], org.apache.spark.sql.types.StructField](((x$1: scala.util.Either[com.vertica.spark.util.error.SchemaError,org.apache.spark.sql.types.StructField]) =&gt; cats.implicits.catsSyntaxEither[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError], org.apache.spark.sql.types.StructField](cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.SchemaError, org.apache.spark.sql.types.StructField](x$1).leftMap[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]](((err: com.vertica.spark.util.error.SchemaError) =&gt; cats.data.NonEmptyList.one[com.vertica.spark.util.error.SchemaError](err)))).toValidated))(data.this.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]](data.this.NonEmptyList.catsDataSemigroupForNonEmptyList[com.vertica.spark.util.error.SchemaError])).toEither.map[org.apache.spark.sql.types.StructType](((field: List[org.apache.spark.sql.types.StructField]) =&gt; org.apache.spark.sql.types.StructType.apply(field))).left.map[com.vertica.spark.util.error.ErrorList](((errors: cats.data.NonEmptyList[com.vertica.spark.util.error.SchemaError]) =&gt; com.vertica.spark.util.error.ErrorList.apply(errors)))
        </td>
      </tr><tr>
        <td>
          209
        </td>
        <td>
          2432
        </td>
        <td>
          9403
          -
          9463
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;SELECT * FROM &quot;.+(tablename.getFullTableName).+(&quot; WHERE 1=0&quot;)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2433
        </td>
        <td>
          9499
          -
          9545
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;SELECT * FROM (&quot;.+(query).+(&quot;) AS x WHERE 1=0&quot;)
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          2434
        </td>
        <td>
          9567
          -
          9567
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          2435
        </td>
        <td>
          9557
          -
          9579
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          jdbcLayer.query(query, jdbcLayer.query$default$2)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2436
        </td>
        <td>
          9617
          -
          9637
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.JdbcSchemaError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.JdbcSchemaError.apply(err)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2437
        </td>
        <td>
          9612
          -
          9638
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.JdbcSchemaError, Nothing](com.vertica.spark.util.error.JdbcSchemaError.apply(err))
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          2453
        </td>
        <td>
          9687
          -
          10370
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
  scala.`package`.Right.apply[Nothing, Seq[com.vertica.spark.util.schema.ColumnDef]](scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[com.vertica.spark.util.schema.ColumnDef, Seq[com.vertica.spark.util.schema.ColumnDef]](((idx: Int) =&gt; {
    val columnLabel: String = rsmd.getColumnLabel(idx);
    val dataType: Int = rsmd.getColumnType(idx);
    val typeName: String = rsmd.getColumnTypeName(idx);
    val fieldSize: Int = org.apache.spark.sql.types.DecimalType.MAX_PRECISION;
    val fieldScale: Int = rsmd.getScale(idx);
    val isSigned: Boolean = rsmd.isSigned(idx);
    val nullable: Boolean = rsmd.isNullable(idx).!=(0);
    val metadata: org.apache.spark.sql.types.Metadata = new org.apache.spark.sql.types.MetadataBuilder().putString(&quot;name&quot;, columnLabel).build();
    ColumnDef.apply(columnLabel, dataType, typeName, fieldSize, fieldScale, isSigned, nullable, metadata)
  }))(immutable.this.IndexedSeq.canBuildFrom[com.vertica.spark.util.schema.ColumnDef]))
}
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          2438
        </td>
        <td>
          9698
          -
          9712
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.getMetaData
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rs.getMetaData()
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          2452
        </td>
        <td>
          9723
          -
          10370
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Seq[com.vertica.spark.util.schema.ColumnDef]](scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[com.vertica.spark.util.schema.ColumnDef, Seq[com.vertica.spark.util.schema.ColumnDef]](((idx: Int) =&gt; {
  val columnLabel: String = rsmd.getColumnLabel(idx);
  val dataType: Int = rsmd.getColumnType(idx);
  val typeName: String = rsmd.getColumnTypeName(idx);
  val fieldSize: Int = org.apache.spark.sql.types.DecimalType.MAX_PRECISION;
  val fieldScale: Int = rsmd.getScale(idx);
  val isSigned: Boolean = rsmd.isSigned(idx);
  val nullable: Boolean = rsmd.isNullable(idx).!=(0);
  val metadata: org.apache.spark.sql.types.Metadata = new org.apache.spark.sql.types.MetadataBuilder().putString(&quot;name&quot;, columnLabel).build();
  ColumnDef.apply(columnLabel, dataType, typeName, fieldSize, fieldScale, isSigned, nullable, metadata)
}))(immutable.this.IndexedSeq.canBuildFrom[com.vertica.spark.util.schema.ColumnDef]))
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          2451
        </td>
        <td>
          9730
          -
          10369
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[com.vertica.spark.util.schema.ColumnDef, Seq[com.vertica.spark.util.schema.ColumnDef]](((idx: Int) =&gt; {
  val columnLabel: String = rsmd.getColumnLabel(idx);
  val dataType: Int = rsmd.getColumnType(idx);
  val typeName: String = rsmd.getColumnTypeName(idx);
  val fieldSize: Int = org.apache.spark.sql.types.DecimalType.MAX_PRECISION;
  val fieldScale: Int = rsmd.getScale(idx);
  val isSigned: Boolean = rsmd.isSigned(idx);
  val nullable: Boolean = rsmd.isNullable(idx).!=(0);
  val metadata: org.apache.spark.sql.types.Metadata = new org.apache.spark.sql.types.MetadataBuilder().putString(&quot;name&quot;, columnLabel).build();
  ColumnDef.apply(columnLabel, dataType, typeName, fieldSize, fieldScale, isSigned, nullable, metadata)
}))(immutable.this.IndexedSeq.canBuildFrom[com.vertica.spark.util.schema.ColumnDef])
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          2439
        </td>
        <td>
          9730
          -
          9731
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          2450
        </td>
        <td>
          9759
          -
          9759
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.IndexedSeq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.IndexedSeq.canBuildFrom[com.vertica.spark.util.schema.ColumnDef]
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          2440
        </td>
        <td>
          9735
          -
          9754
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.getColumnCount
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.getColumnCount()
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          2441
        </td>
        <td>
          9799
          -
          9823
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.getColumnLabel
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.getColumnLabel(idx)
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          2442
        </td>
        <td>
          9851
          -
          9874
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.getColumnType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.getColumnType(idx)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          2443
        </td>
        <td>
          9902
          -
          9929
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.getColumnTypeName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.getColumnTypeName(idx)
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2444
        </td>
        <td>
          9958
          -
          9983
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          2445
        </td>
        <td>
          10013
          -
          10031
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.getScale
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.getScale(idx)
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          2446
        </td>
        <td>
          10059
          -
          10077
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSetMetaData.isSigned
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.isSigned(idx)
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          2447
        </td>
        <td>
          10105
          -
          10160
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rsmd.isNullable(idx).!=(0)
        </td>
      </tr><tr>
        <td>
          226
        </td>
        <td>
          2448
        </td>
        <td>
          10188
          -
          10248
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.MetadataBuilder.build
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.types.MetadataBuilder().putString(&quot;name&quot;, columnLabel).build()
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          2449
        </td>
        <td>
          10261
          -
          10356
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ColumnDef.apply(columnLabel, dataType, typeName, fieldSize, fieldScale, isSigned, nullable, metadata)
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          2454
        </td>
        <td>
          10445
          -
          10502
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.DatabaseReadError.apply(e).context(&quot;Could not get column info&quot;)
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          2455
        </td>
        <td>
          10440
          -
          10503
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.DatabaseReadError.apply(e).context(&quot;Could not get column info&quot;))
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          2457
        </td>
        <td>
          10542
          -
          10552
        </td>
        <td>
          Block
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          2456
        </td>
        <td>
          10542
          -
          10552
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          2459
        </td>
        <td>
          10780
          -
          10818
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;VARBINARY(&quot;.+(SchemaTools.this.longlength).+(&quot;)&quot;))
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          2458
        </td>
        <td>
          10786
          -
          10817
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;VARBINARY(&quot;.+(SchemaTools.this.longlength).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2460
        </td>
        <td>
          10872
          -
          10888
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;BOOLEAN&quot;)
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          2461
        </td>
        <td>
          10939
          -
          10955
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;TINYINT&quot;)
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          2462
        </td>
        <td>
          11006
          -
          11019
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;DATE&quot;)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          2463
        </td>
        <td>
          11082
          -
          11099
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;INTERVAL&quot;)
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          2464
        </td>
        <td>
          11155
          -
          11171
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;DECIMAL&quot;)
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2465
        </td>
        <td>
          11224
          -
          11249
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;DOUBLE PRECISION&quot;)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2466
        </td>
        <td>
          11301
          -
          11315
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;FLOAT&quot;)
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          2467
        </td>
        <td>
          11369
          -
          11385
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;INTEGER&quot;)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          2468
        </td>
        <td>
          11436
          -
          11451
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;BIGINT&quot;)
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          2469
        </td>
        <td>
          11502
          -
          11515
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;null&quot;)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          2470
        </td>
        <td>
          11567
          -
          11584
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;SMALLINT&quot;)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          2471
        </td>
        <td>
          11640
          -
          11658
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;TIMESTAMP&quot;)
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2472
        </td>
        <td>
          11851
          -
          11861
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.longlength
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.longlength
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2475
        </td>
        <td>
          11863
          -
          11877
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;LONG VARCHAR&quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2474
        </td>
        <td>
          11863
          -
          11877
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;LONG VARCHAR&quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2477
        </td>
        <td>
          11883
          -
          11892
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;VARCHAR&quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2473
        </td>
        <td>
          11842
          -
          11861
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          strlen.&gt;(SchemaTools.this.longlength)
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          2476
        </td>
        <td>
          11883
          -
          11892
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;VARCHAR&quot;
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          2479
        </td>
        <td>
          11901
          -
          11943
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](vtype.+(&quot;(&quot;).+(strlen.toString()).+(&quot;)&quot;))
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          2478
        </td>
        <td>
          11907
          -
          11942
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          vtype.+(&quot;(&quot;).+(strlen.toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          2481
        </td>
        <td>
          12163
          -
          12201
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](&quot;VARBINARY(&quot;.+(SchemaTools.this.longlength).+(&quot;)&quot;))
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          2480
        </td>
        <td>
          12169
          -
          12200
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;VARBINARY(&quot;.+(SchemaTools.this.longlength).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          2483
        </td>
        <td>
          12220
          -
          12264
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.MissingSparkConversionError, Nothing](com.vertica.spark.util.error.MissingSparkConversionError.apply(sparkType))
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          2482
        </td>
        <td>
          12225
          -
          12263
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingSparkConversionError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.MissingSparkConversionError.apply(sparkType)
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          2531
        </td>
        <td>
          12406
          -
          14944
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.getColumnInfo(jdbcLayer, tableName).flatMap[com.vertica.spark.util.error.ConnectorError, String](((columns: Seq[com.vertica.spark.util.schema.ColumnDef]) =&gt; {
  val colCount: Int = columns.length;
  var colsFound: Int = 0;
  columns.foreach[Unit](((column: com.vertica.spark.util.schema.ColumnDef) =&gt; {
    (if (SchemaTools.this.logger.underlying.isDebugEnabled())
      SchemaTools.this.logger.underlying.debug(&quot;Will check that target column: &quot;.+(column.label).+(&quot; exist in DF&quot;))
    else
      (): Unit);
    scala.util.control.Breaks.breakable(schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; {
      (if (SchemaTools.this.logger.underlying.isDebugEnabled())
        SchemaTools.this.logger.underlying.debug(&quot;Comparing target table column: &quot;.+(column.label).+(&quot; with DF column: &quot;).+(s.name))
      else
        (): Unit);
      if (s.name.equalsIgnoreCase(column.label))
        {
          colsFound = colsFound.+(1);
          (if (SchemaTools.this.logger.underlying.isDebugEnabled())
            SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
          else
            (): Unit);
          if (column.nullable.unary_!)
            if (s.nullable)
              (if (SchemaTools.this.logger.underlying.isWarnEnabled())
                SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
              else
                (): Unit)
            else
              ()
          else
            ();
          scala.util.control.Breaks.break()
        }
      else
        ()
    })))
  }));
  if (schema.length.&lt;=(colCount).unary_!)
    scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.TableNotEnoughRowsError.apply().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount)))
  else
    if (colsFound.==(schema.length))
      {
        var columnList: String = &quot;&quot;;
        var first: Boolean = true;
        schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; if (first)
          {
            columnList = &quot;\&quot;&quot;.+(s.name);
            first = false
          }
        else
          columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))));
        columnList = &quot;(&quot;.+(columnList).+(&quot;\&quot;)&quot;);
        (if (SchemaTools.this.logger.underlying.isInfoEnabled())
          SchemaTools.this.logger.underlying.info(&quot;Load by name. Column list: &quot;.+(columnList))
        else
          (): Unit);
        scala.`package`.Right.apply[Nothing, String](columnList)
      }
    else
      {
        (if (SchemaTools.this.logger.underlying.isInfoEnabled())
          SchemaTools.this.logger.underlying.info(&quot;Load by Position&quot;)
        else
          (): Unit);
        scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
      }
}.map[String](((columnList: String) =&gt; columnList))))
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          2530
        </td>
        <td>
          12472
          -
          14944
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val colCount: Int = columns.length;
  var colsFound: Int = 0;
  columns.foreach[Unit](((column: com.vertica.spark.util.schema.ColumnDef) =&gt; {
    (if (SchemaTools.this.logger.underlying.isDebugEnabled())
      SchemaTools.this.logger.underlying.debug(&quot;Will check that target column: &quot;.+(column.label).+(&quot; exist in DF&quot;))
    else
      (): Unit);
    scala.util.control.Breaks.breakable(schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; {
      (if (SchemaTools.this.logger.underlying.isDebugEnabled())
        SchemaTools.this.logger.underlying.debug(&quot;Comparing target table column: &quot;.+(column.label).+(&quot; with DF column: &quot;).+(s.name))
      else
        (): Unit);
      if (s.name.equalsIgnoreCase(column.label))
        {
          colsFound = colsFound.+(1);
          (if (SchemaTools.this.logger.underlying.isDebugEnabled())
            SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
          else
            (): Unit);
          if (column.nullable.unary_!)
            if (s.nullable)
              (if (SchemaTools.this.logger.underlying.isWarnEnabled())
                SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
              else
                (): Unit)
            else
              ()
          else
            ();
          scala.util.control.Breaks.break()
        }
      else
        ()
    })))
  }));
  if (schema.length.&lt;=(colCount).unary_!)
    scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.TableNotEnoughRowsError.apply().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount)))
  else
    if (colsFound.==(schema.length))
      {
        var columnList: String = &quot;&quot;;
        var first: Boolean = true;
        schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; if (first)
          {
            columnList = &quot;\&quot;&quot;.+(s.name);
            first = false
          }
        else
          columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))));
        columnList = &quot;(&quot;.+(columnList).+(&quot;\&quot;)&quot;);
        (if (SchemaTools.this.logger.underlying.isInfoEnabled())
          SchemaTools.this.logger.underlying.info(&quot;Load by name. Column list: &quot;.+(columnList))
        else
          (): Unit);
        scala.`package`.Right.apply[Nothing, String](columnList)
      }
    else
      {
        (if (SchemaTools.this.logger.underlying.isInfoEnabled())
          SchemaTools.this.logger.underlying.info(&quot;Load by Position&quot;)
        else
          (): Unit);
        scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
      }
}.map[String](((columnList: String) =&gt; columnList))
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2484
        </td>
        <td>
          12511
          -
          12525
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.length
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          columns.length
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          2485
        </td>
        <td>
          12550
          -
          12551
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          278
        </td>
        <td>
          2503
        </td>
        <td>
          12560
          -
          13669
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          columns.foreach[Unit](((column: com.vertica.spark.util.schema.ColumnDef) =&gt; {
  (if (SchemaTools.this.logger.underlying.isDebugEnabled())
    SchemaTools.this.logger.underlying.debug(&quot;Will check that target column: &quot;.+(column.label).+(&quot; exist in DF&quot;))
  else
    (): Unit);
  scala.util.control.Breaks.breakable(schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; {
    (if (SchemaTools.this.logger.underlying.isDebugEnabled())
      SchemaTools.this.logger.underlying.debug(&quot;Comparing target table column: &quot;.+(column.label).+(&quot; with DF column: &quot;).+(s.name))
    else
      (): Unit);
    if (s.name.equalsIgnoreCase(column.label))
      {
        colsFound = colsFound.+(1);
        (if (SchemaTools.this.logger.underlying.isDebugEnabled())
          SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
        else
          (): Unit);
        if (column.nullable.unary_!)
          if (s.nullable)
            (if (SchemaTools.this.logger.underlying.isWarnEnabled())
              SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
            else
              (): Unit)
          else
            ()
        else
          ();
        scala.util.control.Breaks.break()
      }
    else
      ()
  })))
}))
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          2502
        </td>
        <td>
          12689
          -
          13658
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.control.Breaks.breakable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.util.control.Breaks.breakable(schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; {
  (if (SchemaTools.this.logger.underlying.isDebugEnabled())
    SchemaTools.this.logger.underlying.debug(&quot;Comparing target table column: &quot;.+(column.label).+(&quot; with DF column: &quot;).+(s.name))
  else
    (): Unit);
  if (s.name.equalsIgnoreCase(column.label))
    {
      colsFound = colsFound.+(1);
      (if (SchemaTools.this.logger.underlying.isDebugEnabled())
        SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
      else
        (): Unit);
      if (column.nullable.unary_!)
        if (s.nullable)
          (if (SchemaTools.this.logger.underlying.isWarnEnabled())
            SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
          else
            (): Unit)
        else
          ()
      else
        ();
      scala.util.control.Breaks.break()
    }
  else
    ()
})))
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          2501
        </td>
        <td>
          12713
          -
          13646
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; {
  (if (SchemaTools.this.logger.underlying.isDebugEnabled())
    SchemaTools.this.logger.underlying.debug(&quot;Comparing target table column: &quot;.+(column.label).+(&quot; with DF column: &quot;).+(s.name))
  else
    (): Unit);
  if (s.name.equalsIgnoreCase(column.label))
    {
      colsFound = colsFound.+(1);
      (if (SchemaTools.this.logger.underlying.isDebugEnabled())
        SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
      else
        (): Unit);
      if (column.nullable.unary_!)
        if (s.nullable)
          (if (SchemaTools.this.logger.underlying.isWarnEnabled())
            SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
          else
            (): Unit)
        else
          ()
      else
        ();
      scala.util.control.Breaks.break()
    }
  else
    ()
}))
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          2487
        </td>
        <td>
          12861
          -
          12898
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.equalsIgnoreCase
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.name.equalsIgnoreCase(column.label)
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          2499
        </td>
        <td>
          12857
          -
          12857
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          2486
        </td>
        <td>
          12885
          -
          12897
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.label
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          column.label
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          2498
        </td>
        <td>
          12900
          -
          13631
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  colsFound = colsFound.+(1);
  (if (SchemaTools.this.logger.underlying.isDebugEnabled())
    SchemaTools.this.logger.underlying.debug(&quot;Column: &quot;.+(s.name).+(&quot; found in target table and DF&quot;))
  else
    (): Unit);
  if (column.nullable.unary_!)
    if (s.nullable)
      (if (SchemaTools.this.logger.underlying.isWarnEnabled())
        SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
      else
        (): Unit)
    else
      ()
  else
    ();
  scala.util.control.Breaks.break()
}
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          2500
        </td>
        <td>
          12857
          -
          12857
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          284
        </td>
        <td>
          2488
        </td>
        <td>
          12918
          -
          12932
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          colsFound.+(1)
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2496
        </td>
        <td>
          13226
          -
          13226
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2495
        </td>
        <td>
          13226
          -
          13226
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2489
        </td>
        <td>
          13230
          -
          13246
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          column.nullable.unary_!
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          2490
        </td>
        <td>
          13272
          -
          13282
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.nullable
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          2493
        </td>
        <td>
          13268
          -
          13268
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          2492
        </td>
        <td>
          13268
          -
          13268
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          2494
        </td>
        <td>
          13268
          -
          13575
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (s.nullable)
  (if (SchemaTools.this.logger.underlying.isWarnEnabled())
    SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
  else
    (): Unit)
else
  ()
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          2491
        </td>
        <td>
          13306
          -
          13555
        </td>
        <td>
          Typed
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          (if (SchemaTools.this.logger.underlying.isWarnEnabled())
  SchemaTools.this.logger.underlying.warn(&quot;S2V: Column &quot;.+(s.name).+(&quot; is NOT NULL in target table &quot;).+(tableName.getFullTableName).+(&quot; but it\'s nullable in the DataFrame. Rows with NULL values in column &quot;).+(s.name).+(&quot; will be rejected.&quot;))
else
  (): Unit)
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          2497
        </td>
        <td>
          13610
          -
          13615
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.control.Breaks.break
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.util.control.Breaks.break()
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2504
        </td>
        <td>
          13752
          -
          13780
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.length.&lt;=(colCount).unary_!
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2508
        </td>
        <td>
          13794
          -
          14119
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.TableNotEnoughRowsError.apply().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount)))
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2507
        </td>
        <td>
          13794
          -
          14119
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.TableNotEnoughRowsError.apply().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount)))
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2506
        </td>
        <td>
          13799
          -
          14118
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.TableNotEnoughRowsError.apply().context(&quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount))
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2505
        </td>
        <td>
          13833
          -
          14117
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Error: Number of columns in the target table should be greater or equal to number of columns in the DataFrame.  Number of columns in DataFrame: &quot;.+(schema.length).+(&quot;. Number of columns in the target table: &quot;).+(tableName.getFullTableName).+(&quot;: &quot;).+(colCount)
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          2529
        </td>
        <td>
          14231
          -
          14913
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (colsFound.==(schema.length))
  {
    var columnList: String = &quot;&quot;;
    var first: Boolean = true;
    schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; if (first)
      {
        columnList = &quot;\&quot;&quot;.+(s.name);
        first = false
      }
    else
      columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))));
    columnList = &quot;(&quot;.+(columnList).+(&quot;\&quot;)&quot;);
    (if (SchemaTools.this.logger.underlying.isInfoEnabled())
      SchemaTools.this.logger.underlying.info(&quot;Load by name. Column list: &quot;.+(columnList))
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, String](columnList)
  }
else
  {
    (if (SchemaTools.this.logger.underlying.isInfoEnabled())
      SchemaTools.this.logger.underlying.info(&quot;Load by Position&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
  }
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          2526
        </td>
        <td>
          14263
          -
          14704
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  var columnList: String = &quot;&quot;;
  var first: Boolean = true;
  schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; if (first)
    {
      columnList = &quot;\&quot;&quot;.+(s.name);
      first = false
    }
  else
    columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))));
  columnList = &quot;(&quot;.+(columnList).+(&quot;\&quot;)&quot;);
  (if (SchemaTools.this.logger.underlying.isInfoEnabled())
    SchemaTools.this.logger.underlying.info(&quot;Load by name. Column list: &quot;.+(columnList))
  else
    (): Unit);
  scala.`package`.Right.apply[Nothing, String](columnList)
}
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          2510
        </td>
        <td>
          14235
          -
          14261
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          colsFound.==(schema.length)
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          2509
        </td>
        <td>
          14248
          -
          14261
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.length
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.length
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          2511
        </td>
        <td>
          14292
          -
          14294
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          2512
        </td>
        <td>
          14317
          -
          14321
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          2523
        </td>
        <td>
          14332
          -
          14552
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.foreach[Unit](((s: org.apache.spark.sql.types.StructField) =&gt; if (first)
  {
    columnList = &quot;\&quot;&quot;.+(s.name);
    first = false
  }
else
  columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))))
        </td>
      </tr><tr>
        <td>
          313
        </td>
        <td>
          2517
        </td>
        <td>
          14377
          -
          14461
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  columnList = &quot;\&quot;&quot;.+(s.name);
  first = false
}
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          2514
        </td>
        <td>
          14413
          -
          14419
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.name
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          2513
        </td>
        <td>
          14406
          -
          14410
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          2515
        </td>
        <td>
          14406
          -
          14419
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(s.name)
        </td>
      </tr><tr>
        <td>
          315
        </td>
        <td>
          2516
        </td>
        <td>
          14442
          -
          14447
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2520
        </td>
        <td>
          14509
          -
          14525
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;,\&quot;&quot;.+(s.name)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2519
        </td>
        <td>
          14519
          -
          14525
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.name
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2522
        </td>
        <td>
          14495
          -
          14525
        </td>
        <td>
          Assign
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          columnList = columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2518
        </td>
        <td>
          14509
          -
          14516
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;,\&quot;&quot;
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2521
        </td>
        <td>
          14495
          -
          14525
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          columnList.+(&quot;\&quot;,\&quot;&quot;.+(s.name))
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          2524
        </td>
        <td>
          14576
          -
          14600
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;(&quot;.+(columnList).+(&quot;\&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2525
        </td>
        <td>
          14677
          -
          14694
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](columnList)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2528
        </td>
        <td>
          14719
          -
          14913
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  (if (SchemaTools.this.logger.underlying.isInfoEnabled())
    SchemaTools.this.logger.underlying.info(&quot;Load by Position&quot;)
  else
    (): Unit);
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
}
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          2527
        </td>
        <td>
          14894
          -
          14903
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2532
        </td>
        <td>
          15019
          -
          15034
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;::varchar AS &quot;
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2534
        </td>
        <td>
          15009
          -
          15061
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          colName.+(&quot;::varchar AS &quot;).+(SchemaTools.this.addDoubleQuotes(colName))
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2533
        </td>
        <td>
          15037
          -
          15061
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.addDoubleQuotes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.addDoubleQuotes(colName)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2535
        </td>
        <td>
          15204
          -
          15227
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          requiredSchema.nonEmpty
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2538
        </td>
        <td>
          15299
          -
          15321
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.name.==(cd.label)
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2541
        </td>
        <td>
          15237
          -
          15323
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          columnDefs.filter(((cd: com.vertica.spark.util.schema.ColumnDef) =&gt; scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](requiredSchema.fields).exists(((field: org.apache.spark.sql.types.StructField) =&gt; field.name.==(cd.label)))))
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2537
        </td>
        <td>
          15313
          -
          15321
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.ColumnDef.label
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          cd.label
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2540
        </td>
        <td>
          15237
          -
          15323
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          columnDefs.filter(((cd: com.vertica.spark.util.schema.ColumnDef) =&gt; scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](requiredSchema.fields).exists(((field: org.apache.spark.sql.types.StructField) =&gt; field.name.==(cd.label)))))
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2536
        </td>
        <td>
          15261
          -
          15282
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fields
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          requiredSchema.fields
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          2539
        </td>
        <td>
          15261
          -
          15322
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.exists
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](requiredSchema.fields).exists(((field: org.apache.spark.sql.types.StructField) =&gt; field.name.==(cd.label)))
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          2542
        </td>
        <td>
          15343
          -
          15353
        </td>
        <td>
          Ident
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.columnDefs
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          columnDefs
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          2543
        </td>
        <td>
          15365
          -
          15885
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          requiredColumnDefs.map[String, Seq[String]](((info: com.vertica.spark.util.schema.ColumnDef) =&gt; info.colType match {
  case 1111 =&gt; {
    val typenameNormalized: String = info.colTypeName.toLowerCase();
    if (typenameNormalized.startsWith(&quot;interval&quot;).||(typenameNormalized.startsWith(&quot;uuid&quot;)))
      SchemaTools.this.castToVarchar.apply(info.label)
    else
      SchemaTools.this.addDoubleQuotes(info.label)
  }
  case 92 =&gt; SchemaTools.this.castToVarchar.apply(info.label)
  case _ =&gt; SchemaTools.this.addDoubleQuotes(info.label)
}))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          2544
        </td>
        <td>
          15993
          -
          16012
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new scala.`package`.StringBuilder()
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          2545
        </td>
        <td>
          16018
          -
          16033
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sb.append(&quot; (&quot;)
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          2546
        </td>
        <td>
          16050
          -
          16054
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          2594
        </td>
        <td>
          16059
          -
          17399
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.foreach[scala.util.Either[Nothing,Unit]](((s: org.apache.spark.sql.types.StructField) =&gt; {
  (if (SchemaTools.this.logger.underlying.isDebugEnabled())
    SchemaTools.this.logger.underlying.debug(&quot;colname=\&quot;&quot;.+(s.name).+(&quot;\&quot;&quot;).+(&quot;; type=&quot;).+(s.dataType).+(&quot;; nullable=&quot;).+(s.nullable))
  else
    (): Unit);
  if (first.unary_!)
    sb.append(&quot;,\n&quot;)
  else
    ();
  first = false;
  sb.append(&quot;\&quot;&quot;.+(s.name).+(&quot;\&quot; &quot;));
  var decimal_qualifier: String = &quot;&quot;;
  if (s.dataType.toString().contains(&quot;DecimalType&quot;))
    {
      val p: scala.util.matching.Regex = scala.Predef.augmentString(&quot;DecimalType\\((\\d+)\\)&quot;).r;
      if (s.dataType.toString().matches(p.toString()))
        {
          val prec: String = (s.dataType.toString(): String @unchecked) match {
            case p.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _)) =&gt; prec
          };
          decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;)&quot;)
        }
      else
        ();
      val ps: scala.util.matching.Regex = scala.Predef.augmentString(&quot;DecimalType\\((\\d+),(\\d+)\\)&quot;).r;
      if (s.dataType.toString().matches(ps.toString()))
        {
          &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$2: (String, String) = (s.dataType.toString(): String @unchecked) match {
            case ps.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _), (scale @ _)) =&gt; scala.Tuple2.apply[String, String](prec, scale)
          };
          val prec: String = x$2._1;
          val scale: String = x$2._2;
          decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;,&quot;).+(scale).+(&quot;)&quot;)
        }
      else
        ()
    }
  else
    ();
  SchemaTools.this.getVerticaTypeFromSparkType(s.dataType, strlen) match {
  case (value: com.vertica.spark.util.error.SchemaError)scala.util.Left[com.vertica.spark.util.error.SchemaError,String]((err @ _)) =&gt; return scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.SchemaConversionError.apply(err).context(&quot;Schema error when trying to create table&quot;))
  case (value: String)scala.util.Right[com.vertica.spark.util.error.SchemaError,String]((datatype @ _)) =&gt; scala.`package`.Right.apply[Nothing, String](datatype.+(decimal_qualifier))
}.map[(String, StringBuilder, Any)](((col: String) =&gt; {
  val x$3: StringBuilder = sb.append(col);
  val x$4: Any = if (s.nullable.unary_!)
    sb.append(&quot; NOT NULL&quot;)
  else
    ();
  scala.Tuple3.apply[String, StringBuilder, Any](col, x$3, x$4)
})).map[Unit](((x$5: (String, StringBuilder, Any)) =&gt; (x$5: (String, StringBuilder, Any) @unchecked) match {
    case (_1: String, _2: StringBuilder, _3: Any)(String, StringBuilder, Any)((col @ _), _, _) =&gt; ()
  }))
}))
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          2547
        </td>
        <td>
          16199
          -
          16205
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          first.unary_!
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          2550
        </td>
        <td>
          16195
          -
          16195
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          2551
        </td>
        <td>
          16195
          -
          16195
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          369
        </td>
        <td>
          2549
        </td>
        <td>
          16217
          -
          16233
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          sb.append(&quot;,\n&quot;)
        </td>
      </tr><tr>
        <td>
          369
        </td>
        <td>
          2548
        </td>
        <td>
          16217
          -
          16233
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          sb.append(&quot;,\n&quot;)
        </td>
      </tr><tr>
        <td>
          371
        </td>
        <td>
          2552
        </td>
        <td>
          16256
          -
          16261
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2553
        </td>
        <td>
          16278
          -
          16299
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(s.name).+(&quot;\&quot; &quot;)
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2554
        </td>
        <td>
          16268
          -
          16300
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sb.append(&quot;\&quot;&quot;.+(s.name).+(&quot;\&quot; &quot;))
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          2555
        </td>
        <td>
          16413
          -
          16415
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          2556
        </td>
        <td>
          16426
          -
          16469
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.dataType.toString().contains(&quot;DecimalType&quot;)
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          2577
        </td>
        <td>
          16422
          -
          16422
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          2576
        </td>
        <td>
          16471
          -
          16978
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val p: scala.util.matching.Regex = scala.Predef.augmentString(&quot;DecimalType\\((\\d+)\\)&quot;).r;
  if (s.dataType.toString().matches(p.toString()))
    {
      val prec: String = (s.dataType.toString(): String @unchecked) match {
        case p.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _)) =&gt; prec
      };
      decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;)&quot;)
    }
  else
    ();
  val ps: scala.util.matching.Regex = scala.Predef.augmentString(&quot;DecimalType\\((\\d+),(\\d+)\\)&quot;).r;
  if (s.dataType.toString().matches(ps.toString()))
    {
      &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$2: (String, String) = (s.dataType.toString(): String @unchecked) match {
        case ps.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _), (scale @ _)) =&gt; scala.Tuple2.apply[String, String](prec, scale)
      };
      val prec: String = x$2._1;
      val scale: String = x$2._2;
      decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;,&quot;).+(scale).+(&quot;)&quot;)
    }
  else
    ()
}
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          2578
        </td>
        <td>
          16422
          -
          16422
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2558
        </td>
        <td>
          16520
          -
          16547
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.r
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(&quot;DecimalType\\((\\d+)\\)&quot;).r
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2557
        </td>
        <td>
          16520
          -
          16545
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;DecimalType\\((\\d+)\\)&quot;
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          2565
        </td>
        <td>
          16556
          -
          16556
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          2559
        </td>
        <td>
          16588
          -
          16598
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.matching.Regex.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          p.toString()
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          2564
        </td>
        <td>
          16556
          -
          16556
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          2560
        </td>
        <td>
          16560
          -
          16599
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.matches
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.dataType.toString().matches(p.toString())
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          2563
        </td>
        <td>
          16601
          -
          16703
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val prec: String = (s.dataType.toString(): String @unchecked) match {
    case p.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _)) =&gt; prec
  };
  decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;)&quot;)
}
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2561
        </td>
        <td>
          16627
          -
          16646
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.dataType.toString()
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2562
        </td>
        <td>
          16677
          -
          16693
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;(&quot;.+(prec).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          2567
        </td>
        <td>
          16757
          -
          16791
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.r
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(&quot;DecimalType\\((\\d+),(\\d+)\\)&quot;).r
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          2566
        </td>
        <td>
          16757
          -
          16789
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;DecimalType\\((\\d+),(\\d+)\\)&quot;
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2574
        </td>
        <td>
          16800
          -
          16800
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2568
        </td>
        <td>
          16832
          -
          16843
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.matching.Regex.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ps.toString()
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2573
        </td>
        <td>
          16846
          -
          16970
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$2: (String, String) = (s.dataType.toString(): String @unchecked) match {
    case ps.unapplySeq(&lt;unapply-selector&gt;) &lt;unapply&gt; ((prec @ _), (scale @ _)) =&gt; scala.Tuple2.apply[String, String](prec, scale)
  };
  val prec: String = x$2._1;
  val scale: String = x$2._2;
  decimal_qualifier = &quot;(&quot;.+(prec).+(&quot;,&quot;).+(scale).+(&quot;)&quot;)
}
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2575
        </td>
        <td>
          16800
          -
          16800
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2569
        </td>
        <td>
          16804
          -
          16844
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.matches
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.dataType.toString().matches(ps.toString())
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2571
        </td>
        <td>
          16871
          -
          16871
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$2._2
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2570
        </td>
        <td>
          16865
          -
          16865
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$2._1
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          2572
        </td>
        <td>
          16930
          -
          16960
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;(&quot;.+(prec).+(&quot;,&quot;).+(scale).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          2580
        </td>
        <td>
          17007
          -
          17054
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getVerticaTypeFromSparkType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.getVerticaTypeFromSparkType(s.dataType, strlen)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          2579
        </td>
        <td>
          17035
          -
          17045
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.dataType
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          2591
        </td>
        <td>
          17000
          -
          17000
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[String, StringBuilder, Any](col, x$3, x$4)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          2593
        </td>
        <td>
          16986
          -
          17392
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.getVerticaTypeFromSparkType(s.dataType, strlen) match {
  case (value: com.vertica.spark.util.error.SchemaError)scala.util.Left[com.vertica.spark.util.error.SchemaError,String]((err @ _)) =&gt; return scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.SchemaConversionError.apply(err).context(&quot;Schema error when trying to create table&quot;))
  case (value: String)scala.util.Right[com.vertica.spark.util.error.SchemaError,String]((datatype @ _)) =&gt; scala.`package`.Right.apply[Nothing, String](datatype.+(decimal_qualifier))
}.map[(String, StringBuilder, Any)](((col: String) =&gt; {
  val x$3: StringBuilder = sb.append(col);
  val x$4: Any = if (s.nullable.unary_!)
    sb.append(&quot; NOT NULL&quot;)
  else
    ();
  scala.Tuple3.apply[String, StringBuilder, Any](col, x$3, x$4)
})).map[Unit](((x$5: (String, StringBuilder, Any)) =&gt; (x$5: (String, StringBuilder, Any) @unchecked) match {
  case (_1: String, _2: StringBuilder, _3: Any)(String, StringBuilder, Any)((col @ _), _, _) =&gt; ()
}))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2582
        </td>
        <td>
          17110
          -
          17194
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.SchemaConversionError.apply(err).context(&quot;Schema error when trying to create table&quot;))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2581
        </td>
        <td>
          17115
          -
          17193
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.SchemaConversionError.apply(err).context(&quot;Schema error when trying to create table&quot;)
        </td>
      </tr><tr>
        <td>
          397
        </td>
        <td>
          2583
        </td>
        <td>
          17235
          -
          17263
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          datatype.+(decimal_qualifier)
        </td>
      </tr><tr>
        <td>
          397
        </td>
        <td>
          2584
        </td>
        <td>
          17229
          -
          17264
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](datatype.+(decimal_qualifier))
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          2585
        </td>
        <td>
          17287
          -
          17301
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sb.append(col)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2589
        </td>
        <td>
          17314
          -
          17314
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2586
        </td>
        <td>
          17318
          -
          17329
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.nullable.unary_!
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2590
        </td>
        <td>
          17314
          -
          17314
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          2588
        </td>
        <td>
          17343
          -
          17365
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          sb.append(&quot; NOT NULL&quot;)
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          2587
        </td>
        <td>
          17343
          -
          17365
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          sb.append(&quot; NOT NULL&quot;)
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          2592
        </td>
        <td>
          17390
          -
          17392
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2595
        </td>
        <td>
          17405
          -
          17419
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.append
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sb.append(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          2597
        </td>
        <td>
          17424
          -
          17442
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](sb.toString())
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          2596
        </td>
        <td>
          17430
          -
          17441
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.StringBuilder.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sb.toString()
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2598
        </td>
        <td>
          17618
          -
          17653
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getColumnInfo
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.getColumnInfo(jdbcLayer, tableName)
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          2600
        </td>
        <td>
          17688
          -
          17758
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](info.map[String, Seq[String]](((x: com.vertica.spark.util.schema.ColumnDef) =&gt; &quot;temp.&quot;.+(SchemaTools.this.addDoubleQuotes(x.label))))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;,&quot;))
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          2599
        </td>
        <td>
          17694
          -
          17757
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          info.map[String, Seq[String]](((x: com.vertica.spark.util.schema.ColumnDef) =&gt; &quot;temp.&quot;.+(SchemaTools.this.addDoubleQuotes(x.label))))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          2601
        </td>
        <td>
          17788
          -
          17808
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.JdbcSchemaError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.JdbcSchemaError.apply(err)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          2602
        </td>
        <td>
          17783
          -
          17809
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.JdbcSchemaError, Nothing](com.vertica.spark.util.error.JdbcSchemaError.apply(err))
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          2604
        </td>
        <td>
          18153
          -
          18163
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.trim
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          col.trim()
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          2603
        </td>
        <td>
          18110
          -
          18134
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.split
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          list.toString().split(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          2606
        </td>
        <td>
          18110
          -
          18164
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[String](list.toString().split(&quot;,&quot;)).toList.map[String, List[String]](((col: String) =&gt; col.trim()))(immutable.this.List.canBuildFrom[String])
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          2605
        </td>
        <td>
          18145
          -
          18145
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          immutable.this.List.canBuildFrom[String]
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          2607
        </td>
        <td>
          18187
          -
          18226
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getColumnInfo
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.getColumnInfo(jdbcLayer, tempTableName)
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2609
        </td>
        <td>
          18293
          -
          18315
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IterableLike.zip
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          customColList.zip[String, com.vertica.spark.util.schema.ColumnDef, List[(String, com.vertica.spark.util.schema.ColumnDef)]](info)(immutable.this.List.canBuildFrom[(String, com.vertica.spark.util.schema.ColumnDef)])
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2608
        </td>
        <td>
          18307
          -
          18307
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          immutable.this.List.canBuildFrom[(String, com.vertica.spark.util.schema.ColumnDef)]
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2610
        </td>
        <td>
          18334
          -
          18431
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          tupleList.map[String, List[String]](((x: (String, com.vertica.spark.util.schema.ColumnDef)) =&gt; SchemaTools.this.addDoubleQuotes(x._1).+(&quot;=temp.&quot;).+(SchemaTools.this.addDoubleQuotes(x._2.label))))(immutable.this.List.canBuildFrom[String]).mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2611
        </td>
        <td>
          18328
          -
          18432
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](tupleList.map[String, List[String]](((x: (String, com.vertica.spark.util.schema.ColumnDef)) =&gt; SchemaTools.this.addDoubleQuotes(x._1).+(&quot;=temp.&quot;).+(SchemaTools.this.addDoubleQuotes(x._2.label))))(immutable.this.List.canBuildFrom[String]).mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2613
        </td>
        <td>
          18461
          -
          18487
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.JdbcSchemaError, Nothing](com.vertica.spark.util.error.JdbcSchemaError.apply(err))
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2612
        </td>
        <td>
          18466
          -
          18486
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.JdbcSchemaError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.JdbcSchemaError.apply(err)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          2614
        </td>
        <td>
          18568
          -
          18607
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getColumnInfo
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.getColumnInfo(jdbcLayer, tempTableName)
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2616
        </td>
        <td>
          18646
          -
          18745
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](info.map[String, Seq[String]](((x: com.vertica.spark.util.schema.ColumnDef) =&gt; SchemaTools.this.addDoubleQuotes(x.label).+(&quot;=temp.&quot;).+(SchemaTools.this.addDoubleQuotes(x.label))))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2615
        </td>
        <td>
          18652
          -
          18744
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          info.map[String, Seq[String]](((x: com.vertica.spark.util.schema.ColumnDef) =&gt; SchemaTools.this.addDoubleQuotes(x.label).+(&quot;=temp.&quot;).+(SchemaTools.this.addDoubleQuotes(x.label))))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          2618
        </td>
        <td>
          18774
          -
          18800
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.JdbcSchemaError, Nothing](com.vertica.spark.util.error.JdbcSchemaError.apply(err))
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          2617
        </td>
        <td>
          18779
          -
          18799
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.JdbcSchemaError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.JdbcSchemaError.apply(err)
        </td>
      </tr><tr>
        <td>
          441
        </td>
        <td>
          2643
        </td>
        <td>
          18985
          -
          19751
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.collect
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.collect[java.io.Serializable, Seq[java.io.Serializable]](({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.types.StructField,java.io.Serializable] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.types.StructField =&gt; java.io.Serializable&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.types.StructField, B1 &gt;: java.io.Serializable](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.types.StructField]: org.apache.spark.sql.types.StructField): org.apache.spark.sql.types.StructField @unchecked) match {
      case (field @ _) if SchemaTools.this.addDoubleQuotes(field.name).==(colName) =&gt; if (field.metadata.contains(SchemaTools.this.maxlength).&amp;&amp;(field.dataType.simpleString.==(&quot;string&quot;)))
        if (field.metadata.getLong(SchemaTools.this.maxlength).&gt;(SchemaTools.this.longlength))
          &quot;long varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        else
          &quot;varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
      else
        if (field.metadata.contains(SchemaTools.this.maxlength).&amp;&amp;(field.dataType.simpleString.==(&quot;binary&quot;)))
          &quot;varbinary(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        else
          SchemaTools.this.getVerticaTypeFromSparkType(field.dataType, strlen) match {
            case (value: String)scala.util.Right[com.vertica.spark.util.error.SchemaError,String]((dataType @ _)) =&gt; dataType
            case (value: com.vertica.spark.util.error.SchemaError)scala.util.Left[com.vertica.spark.util.error.SchemaError,String]((err @ _)) =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaError, Nothing](err)
          }
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.types.StructField): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.types.StructField]: org.apache.spark.sql.types.StructField): org.apache.spark.sql.types.StructField @unchecked) match {
      case (field @ _) if SchemaTools.this.addDoubleQuotes(field.name).==(colName) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.types.StructField,java.io.Serializable]))(collection.this.Seq.canBuildFrom[java.io.Serializable])
        </td>
      </tr><tr>
        <td>
          441
        </td>
        <td>
          2642
        </td>
        <td>
          19000
          -
          19000
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[java.io.Serializable]
        </td>
      </tr><tr>
        <td>
          441
        </td>
        <td>
          2641
        </td>
        <td>
          19000
          -
          19000
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2619
        </td>
        <td>
          19022
          -
          19060
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.addDoubleQuotes(field.name).==(colName)
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          2622
        </td>
        <td>
          19077
          -
          19154
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.metadata.contains(SchemaTools.this.maxlength).&amp;&amp;(field.dataType.simpleString.==(&quot;string&quot;))
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          2621
        </td>
        <td>
          19115
          -
          19154
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.dataType.simpleString.==(&quot;string&quot;)
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          2620
        </td>
        <td>
          19101
          -
          19110
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.maxlength
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.maxlength
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2625
        </td>
        <td>
          19171
          -
          19217
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.metadata.getLong(SchemaTools.this.maxlength).&gt;(SchemaTools.this.longlength)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2630
        </td>
        <td>
          19168
          -
          19362
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (field.metadata.getLong(SchemaTools.this.maxlength).&gt;(SchemaTools.this.longlength))
  &quot;long varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
else
  &quot;varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2624
        </td>
        <td>
          19207
          -
          19217
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.longlength
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.longlength
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2627
        </td>
        <td>
          19219
          -
          19285
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;long varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2626
        </td>
        <td>
          19219
          -
          19285
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;long varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2623
        </td>
        <td>
          19194
          -
          19203
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.maxlength
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          SchemaTools.this.maxlength
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          2628
        </td>
        <td>
          19301
          -
          19362
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          2629
        </td>
        <td>
          19301
          -
          19362
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;varchar(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2640
        </td>
        <td>
          19386
          -
          19745
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (field.metadata.contains(SchemaTools.this.maxlength).&amp;&amp;(field.dataType.simpleString.==(&quot;binary&quot;)))
  &quot;varbinary(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
else
  SchemaTools.this.getVerticaTypeFromSparkType(field.dataType, strlen) match {
    case (value: String)scala.util.Right[com.vertica.spark.util.error.SchemaError,String]((dataType @ _)) =&gt; dataType
    case (value: com.vertica.spark.util.error.SchemaError)scala.util.Left[com.vertica.spark.util.error.SchemaError,String]((err @ _)) =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaError, Nothing](err)
  }
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2631
        </td>
        <td>
          19413
          -
          19422
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.maxlength
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.maxlength
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2633
        </td>
        <td>
          19389
          -
          19466
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.metadata.contains(SchemaTools.this.maxlength).&amp;&amp;(field.dataType.simpleString.==(&quot;binary&quot;))
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2632
        </td>
        <td>
          19427
          -
          19466
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.dataType.simpleString.==(&quot;binary&quot;)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          2634
        </td>
        <td>
          19479
          -
          19542
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;varbinary(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          2635
        </td>
        <td>
          19479
          -
          19542
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;varbinary(&quot;.+(field.metadata.getLong(SchemaTools.this.maxlength).toString()).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          2637
        </td>
        <td>
          19579
          -
          19630
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.getVerticaTypeFromSparkType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.getVerticaTypeFromSparkType(field.dataType, strlen)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          2639
        </td>
        <td>
          19579
          -
          19735
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.getVerticaTypeFromSparkType(field.dataType, strlen) match {
  case (value: String)scala.util.Right[com.vertica.spark.util.error.SchemaError,String]((dataType @ _)) =&gt; dataType
  case (value: com.vertica.spark.util.error.SchemaError)scala.util.Left[com.vertica.spark.util.error.SchemaError,String]((err @ _)) =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaError, Nothing](err)
}
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          2636
        </td>
        <td>
          19607
          -
          19621
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.dataType
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          2638
        </td>
        <td>
          19714
          -
          19723
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2644
        </td>
        <td>
          19759
          -
          19777
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldType.nonEmpty
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2646
        </td>
        <td>
          19803
          -
          19817
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldType.head
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2648
        </td>
        <td>
          19787
          -
          19817
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          colName.+(&quot; &quot;).+(fieldType.head)
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2645
        </td>
        <td>
          19797
          -
          19800
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; &quot;
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2647
        </td>
        <td>
          19787
          -
          19817
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          colName.+(&quot; &quot;).+(fieldType.head)
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2649
        </td>
        <td>
          19842
          -
          19845
        </td>
        <td>
          Ident
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.col
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          col
        </td>
      </tr><tr>
        <td>
          467
        </td>
        <td>
          2651
        </td>
        <td>
          20018
          -
          20085
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replace
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          createExternalTableStmt.replace(&quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;), tableName)
        </td>
      </tr><tr>
        <td>
          467
        </td>
        <td>
          2650
        </td>
        <td>
          20050
          -
          20073
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          468
        </td>
        <td>
          2652
        </td>
        <td>
          20122
          -
          20139
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.indexOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stmt.indexOf(&quot;(&quot;)
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          2653
        </td>
        <td>
          20176
          -
          20193
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.indexOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stmt.indexOf(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          470
        </td>
        <td>
          2655
        </td>
        <td>
          20217
          -
          20289
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.substring
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stmt.substring(indexOfOpeningParantheses.+(1), indexOfClosingParantheses)
        </td>
      </tr><tr>
        <td>
          470
        </td>
        <td>
          2654
        </td>
        <td>
          20232
          -
          20261
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          indexOfOpeningParantheses.+(1)
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          2657
        </td>
        <td>
          20311
          -
          20341
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.toList
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[String](schemaString.split(&quot;,&quot;)).toList
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          2656
        </td>
        <td>
          20311
          -
          20334
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.split
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schemaString.split(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          2658
        </td>
        <td>
          20375
          -
          20906
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schemaList.map[String, List[String]](((col: String) =&gt; {
  val indexOfFirstDoubleQuote: Int = col.indexOf(&quot;\&quot;&quot;);
  val indexOfSpace: Int = col.indexOf(&quot; &quot;, indexOfFirstDoubleQuote);
  val colName: String = col.substring(indexOfFirstDoubleQuote, indexOfSpace);
  if (schema.nonEmpty)
    SchemaTools.this.updateFieldDataType(col, colName, schema, strlen)
  else
    if (col.toLowerCase().contains(&quot;varchar&quot;))
      colName.+(&quot; varchar(&quot;).+(strlen).+(&quot;)&quot;)
    else
      if (col.toLowerCase().contains(&quot;varbinary&quot;))
        colName.+(&quot; varbinary(&quot;).+(SchemaTools.this.longlength).+(&quot;)&quot;)
      else
        col
}))(immutable.this.List.canBuildFrom[String]).mkString(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          2660
        </td>
        <td>
          20915
          -
          20946
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          updatedSchema.contains(SchemaTools.this.unknown)
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          2659
        </td>
        <td>
          20938
          -
          20945
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaTools.unknown
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.unknown
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2661
        </td>
        <td>
          20995
          -
          21037
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SchemaTools.this.unknown.+(&quot; partitioned column data type.&quot;)
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2664
        </td>
        <td>
          20956
          -
          21039
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.UnknownColumnTypesError.apply().context(SchemaTools.this.unknown.+(&quot; partitioned column data type.&quot;)))
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2663
        </td>
        <td>
          20956
          -
          21039
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](com.vertica.spark.util.error.UnknownColumnTypesError.apply().context(SchemaTools.this.unknown.+(&quot; partitioned column data type.&quot;)))
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2662
        </td>
        <td>
          20961
          -
          21038
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.UnknownColumnTypesError.apply().context(SchemaTools.this.unknown.+(&quot; partitioned column data type.&quot;))
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          2667
        </td>
        <td>
          21055
          -
          21263
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val updatedCreateTableStmt: String = stmt.replace(schemaString, updatedSchema);
  (if (SchemaTools.this.logger.underlying.isInfoEnabled())
    SchemaTools.this.logger.underlying.info(&quot;Updated create external table statement: &quot;.+(updatedCreateTableStmt))
  else
    (): Unit);
  scala.`package`.Right.apply[Nothing, String](updatedCreateTableStmt)
}
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          2665
        </td>
        <td>
          21092
          -
          21133
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replace
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stmt.replace(schemaString, updatedSchema)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          2666
        </td>
        <td>
          21228
          -
          21257
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](updatedCreateTableStmt)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>