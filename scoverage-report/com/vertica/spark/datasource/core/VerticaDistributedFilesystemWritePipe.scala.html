<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/vertica/spark/datasource/core/VerticaDistributedFilesystemWritePipe.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>// (c) Copyright [2020-2021] Micro Focus or one of its affiliates.
</span>2 <span style=''>// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
</span>3 <span style=''>// You may not use this file except in compliance with the License.
</span>4 <span style=''>// You may obtain a copy of the License at
</span>5 <span style=''>//
</span>6 <span style=''>// http://www.apache.org/licenses/LICENSE-2.0
</span>7 <span style=''>//
</span>8 <span style=''>// Unless required by applicable law or agreed to in writing, software
</span>9 <span style=''>// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
</span>10 <span style=''>// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span>11 <span style=''>// See the License for the specific language governing permissions and
</span>12 <span style=''>// limitations under the License.
</span>13 <span style=''>
</span>14 <span style=''>package com.vertica.spark.datasource.core
</span>15 <span style=''>
</span>16 <span style=''>import com.vertica.spark.config._
</span>17 <span style=''>import com.vertica.spark.datasource.fs.FileStoreLayerInterface
</span>18 <span style=''>import com.vertica.spark.datasource.jdbc.{JdbcLayerInterface, JdbcUtils}
</span>19 <span style=''>import com.vertica.spark.util.Timer
</span>20 <span style=''>import com.vertica.spark.util.error.CreateExternalTableMergeKey
</span>21 <span style=''>import com.vertica.spark.util.error.ErrorHandling.ConnectorResult
</span>22 <span style=''>import com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError
</span>23 <span style=''>import com.vertica.spark.util.error._
</span>24 <span style=''>import com.vertica.spark.util.schema.SchemaToolsInterface
</span>25 <span style=''>import com.vertica.spark.util.table.TableUtilsInterface
</span>26 <span style=''>import org.apache.spark.sql.SparkSession
</span>27 <span style=''>import org.apache.spark.sql.internal.SQLConf
</span>28 <span style=''>import org.apache.spark.sql.types.StructType
</span>29 <span style=''>
</span>30 <span style=''>import scala.util.Try
</span>31 <span style=''>
</span>32 <span style=''>/**
</span>33 <span style=''> * Pipe for writing data to Vertica using an intermediary filesystem.
</span>34 <span style=''> *
</span>35 <span style=''> * @param config Configuration data for writing to Vertica
</span>36 <span style=''> * @param fileStoreLayer Dependency for communication with the intermediary filesystem
</span>37 <span style=''> * @param jdbcLayer Dependency for communication with the database over JDBC
</span>38 <span style=''> * @param schemaTools Dependency for schema conversion between Vertica and Spark
</span>39 <span style=''> * @param tableUtils Depedency on top of JDBC layer for interacting with tables
</span>40 <span style=''> * @param dataSize Number of rows per data block
</span>41 <span style=''> */
</span>42 <span style=''>class VerticaDistributedFilesystemWritePipe(val config: DistributedFilesystemWriteConfig,
</span>43 <span style=''>                                            val fileStoreLayer: FileStoreLayerInterface,
</span>44 <span style=''>                                            val jdbcLayer: JdbcLayerInterface,
</span>45 <span style=''>                                            val schemaTools: SchemaToolsInterface,
</span>46 <span style=''>                                            val tableUtils: TableUtilsInterface,
</span>47 <span style=''>                                            val dataSize: Int = 1)
</span>48 <span style=''>          extends VerticaPipeInterface with VerticaPipeWriteInterface {
</span>49 <span style=''>
</span>50 <span style=''>  private val logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[VerticaDistributedFilesystemWritePipe])</span><span style=''>
</span>51 <span style=''>  private val tempTableName = </span><span style='background: #AEF1AE'>TableName(config.tablename.name + &quot;_&quot; + config.sessionId, None)</span><span style=''>
</span>52 <span style=''>  private val LEGACY_PARQUET_REBASE_MODE_IN_WRITE = </span><span style='background: #AEF1AE'>&quot;spark.sql.legacy.parquet.datetimeRebaseModeInWrite&quot;</span><span style=''>
</span>53 <span style=''>  private val LEGACY_PARQUET_INT96_REBASE_MODE_IN_WRITE = </span><span style='background: #AEF1AE'>&quot;spark.sql.legacy.parquet.int96RebaseModeInWrite&quot;</span><span style=''>
</span>54 <span style=''>
</span>55 <span style=''>  /**
</span>56 <span style=''>   * No write metadata required for configuration as of yet.
</span>57 <span style=''>   */
</span>58 <span style=''>  def getMetadata: ConnectorResult[VerticaMetadata] = </span><span style='background: #F0ADAD'>Right(VerticaWriteMetadata())</span><span style=''>
</span>59 <span style=''>
</span>60 <span style=''>  /**
</span>61 <span style=''>   * Returns the number of rows used per data block.
</span>62 <span style=''>   */
</span>63 <span style=''>  def getDataBlockSize: ConnectorResult[Long] = </span><span style='background: #F0ADAD'>Right(dataSize)</span><span style=''>
</span>64 <span style=''>
</span>65 <span style=''>
</span>66 <span style=''>  def checkSchemaForDuplicates(schema: StructType): ConnectorResult[Unit] = {
</span>67 <span style=''>    val names = </span><span style='background: #AEF1AE'>schema.fields.map(f =&gt; f.name)</span><span style=''>
</span>68 <span style=''>    if(</span><span style='background: #AEF1AE'>names.distinct.length != names.length</span><span style=''>) {
</span>69 <span style=''>      </span><span style='background: #F0ADAD'>Left(DuplicateColumnsError())</span><span style=''>
</span>70 <span style=''>    } else {
</span>71 <span style=''>      </span><span style='background: #AEF1AE'>Right(())</span><span style=''>
</span>72 <span style=''>    }
</span>73 <span style=''>  }
</span>74 <span style=''>
</span>75 <span style=''>  /**
</span>76 <span style=''>   * Set spark conf to handle old dates if unset
</span>77 <span style=''>   * This deals with SPARK-31404 -- issue with legacy calendar format
</span>78 <span style=''>   */
</span>79 <span style=''>  private def setSparkCalendarConf(): Unit = {
</span>80 <span style=''>    </span><span style='background: #AEF1AE'>SparkSession.getActiveSession</span><span style=''> match {
</span>81 <span style=''>      case Some(session) =&gt;
</span>82 <span style=''>        </span><span style='background: #AEF1AE'>session.sparkContext.setLocalProperty(LEGACY_PARQUET_REBASE_MODE_IN_WRITE, &quot;CORRECTED&quot;)</span><span style=''>
</span>83 <span style=''>        </span><span style='background: #AEF1AE'>session.sparkContext.setLocalProperty(LEGACY_PARQUET_INT96_REBASE_MODE_IN_WRITE, &quot;CORRECTED&quot;)</span><span style=''>
</span>84 <span style=''>      case None =&gt; logger.warn(&quot;No spark session found to set config&quot;)
</span>85 <span style=''>    }
</span>86 <span style=''>  }
</span>87 <span style=''>
</span>88 <span style=''>  /**
</span>89 <span style=''>   * Determine the address where the data will be written.  This varies depending on if it is a new external table or not.
</span>90 <span style=''>   */
</span>91 <span style=''>  private def getAddress(): String = {
</span>92 <span style=''>    if (</span><span style='background: #AEF1AE'>config.createExternalTable.isDefined</span><span style=''>) </span><span style='background: #AEF1AE'>config.fileStoreConfig.externalTableAddress</span><span style=''> else </span><span style='background: #AEF1AE'>config.fileStoreConfig.address</span><span style=''>
</span>93 <span style=''>  }
</span>94 <span style=''>
</span>95 <span style=''>  /**
</span>96 <span style=''>   * Initial setup for the intermediate-based write operation.
</span>97 <span style=''>   *
</span>98 <span style=''>   * - Checks if the table exists
</span>99 <span style=''>   * - If not, creates the table (based on user supplied statement or the one we build)
</span>100 <span style=''>   * - Creates the directory that files will be exported to
</span>101 <span style=''>   */
</span>102 <span style=''>  def doPreWriteSteps(): ConnectorResult[Unit] = {
</span>103 <span style=''>    // Log a warning if the user wants to perform a merge, but also wants to overwrite data
</span>104 <span style=''>    if(</span><span style='background: #AEF1AE'>config.mergeKey.isDefined &amp;&amp; </span><span style='background: #F0ADAD'>config.isOverwrite</span><span style=''>) </span><span style='background: #F0ADAD'>logger.warn(&quot;Save mode is specified as Overwrite during a merge.&quot;)</span><span style=''>
</span>105 <span style=''>    logger.info(&quot;Writing data to Parquet file.&quot;)
</span>106 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>107 <span style=''></span><span style='background: #AEF1AE'>      // Check if schema is valid
</span>108 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- checkSchemaForDuplicates(config.schema)
</span>109 <span style=''></span><span style='background: #AEF1AE'>
</span>110 <span style=''></span><span style='background: #AEF1AE'>      // Set spark configuration
</span>111 <span style=''></span><span style='background: #AEF1AE'>      _ = setSparkCalendarConf()
</span>112 <span style=''></span><span style='background: #AEF1AE'>
</span>113 <span style=''></span><span style='background: #AEF1AE'>      // If overwrite mode, remove table and force creation of new one before writing
</span>114 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.isOverwrite &amp;&amp; config.mergeKey.isEmpty) tableUtils.dropTable(config.tablename) else Right(())
</span>115 <span style=''></span><span style='background: #AEF1AE'>
</span>116 <span style=''></span><span style='background: #AEF1AE'>      // Creating external table and merging at the same time not supported
</span>117 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.createExternalTable.isDefined &amp;&amp; config.mergeKey.isDefined) </span><span style='background: #F0ADAD'>Left(CreateExternalTableMergeKey())</span><span style='background: #AEF1AE'> else Right(())
</span>118 <span style=''></span><span style='background: #AEF1AE'>
</span>119 <span style=''></span><span style='background: #AEF1AE'>      // Create the table if it doesn't exist
</span>120 <span style=''></span><span style='background: #AEF1AE'>      tableExistsPre &lt;- tableUtils.tableExists(config.tablename)
</span>121 <span style=''></span><span style='background: #AEF1AE'>
</span>122 <span style=''></span><span style='background: #AEF1AE'>      // Overwrite safety check
</span>123 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.isOverwrite &amp;&amp; config.mergeKey.isEmpty &amp;&amp; tableExistsPre) </span><span style='background: #F0ADAD'>Left(DropTableError())</span><span style='background: #AEF1AE'> else Right(())
</span>124 <span style=''></span><span style='background: #AEF1AE'>
</span>125 <span style=''></span><span style='background: #AEF1AE'>      // External table mode doesn't work if table exists
</span>126 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.createExternalTable.isDefined &amp;&amp; tableExistsPre) </span><span style='background: #F0ADAD'>Left(CreateExternalTableAlreadyExistsError())</span><span style='background: #AEF1AE'> else Right()
</span>127 <span style=''></span><span style='background: #AEF1AE'>
</span>128 <span style=''></span><span style='background: #AEF1AE'>      // Check if a view exists or temp table exists by this name
</span>129 <span style=''></span><span style='background: #AEF1AE'>      viewExists &lt;- tableUtils.viewExists(config.tablename)
</span>130 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (viewExists) Left(ViewExistsError()) else Right(())
</span>131 <span style=''></span><span style='background: #AEF1AE'>      tempTableExists &lt;- tableUtils.tempTableExists(config.tablename)
</span>132 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (tempTableExists) </span><span style='background: #F0ADAD'>Left(TempTableExistsError())</span><span style='background: #AEF1AE'> else Right(())
</span>133 <span style=''></span><span style='background: #AEF1AE'>
</span>134 <span style=''></span><span style='background: #AEF1AE'>      // Create table unless we're appending, or we're in external table mode (that gets created later)
</span>135 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (!tableExistsPre &amp;&amp; config.createExternalTable.isEmpty) tableUtils.createTable(config.tablename, config.targetTableSql, config.schema, config.strlen) else Right(())
</span>136 <span style=''></span><span style='background: #AEF1AE'>
</span>137 <span style=''></span><span style='background: #AEF1AE'>      // Confirm table was created. This should only be false if the user specified an invalid target_table_sql
</span>138 <span style=''></span><span style='background: #AEF1AE'>      tableExistsPost &lt;- tableUtils.tableExists(config.tablename)
</span>139 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (tableExistsPost || </span><span style='background: #F0ADAD'>config.createExternalTable.isDefined</span><span style='background: #AEF1AE'>) Right(()) else </span><span style='background: #F0ADAD'>Left(CreateTableError(None))</span><span style='background: #AEF1AE'>
</span>140 <span style=''></span><span style='background: #AEF1AE'>
</span>141 <span style=''></span><span style='background: #AEF1AE'>      // Create the directory to export files to
</span>142 <span style=''></span><span style='background: #AEF1AE'>      perm = config.filePermissions
</span>143 <span style=''></span><span style='background: #AEF1AE'>      existingData = config.createExternalTable match {
</span>144 <span style=''></span><span style='background: #AEF1AE'>        case Some(value) =&gt;
</span>145 <span style=''></span><span style='background: #AEF1AE'>          value match {
</span>146 <span style=''></span><span style='background: #AEF1AE'>            case ExistingData =&gt; </span><span style='background: #F0ADAD'>true</span><span style='background: #AEF1AE'>
</span>147 <span style=''></span><span style='background: #AEF1AE'>            case NewData =&gt; false
</span>148 <span style=''></span><span style='background: #AEF1AE'>          }
</span>149 <span style=''></span><span style='background: #AEF1AE'>        case None =&gt; false
</span>150 <span style=''></span><span style='background: #AEF1AE'>      }
</span>151 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if(existingData) </span><span style='background: #F0ADAD'>Right(())</span><span style='background: #AEF1AE'> else fileStoreLayer.createDir(getAddress(), perm.toString)
</span>152 <span style=''></span><span style='background: #AEF1AE'>
</span>153 <span style=''></span><span style='background: #AEF1AE'>      // Create job status table / entry
</span>154 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if(config.saveJobStatusTable) {
</span>155 <span style=''></span><span style='background: #AEF1AE'>        tableUtils.createAndInitJobStatusTable(config.tablename, config.jdbcConfig.auth.user, config.sessionId, if(config.isOverwrite) </span><span style='background: #F0ADAD'>&quot;OVERWRITE&quot;</span><span style='background: #AEF1AE'> else &quot;APPEND&quot;)
</span>156 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>157 <span style=''></span><span style='background: #AEF1AE'>        Right(())
</span>158 <span style=''></span><span style='background: #AEF1AE'>      }
</span>159 <span style=''></span><span style='background: #AEF1AE'>    } yield ()</span><span style=''>
</span>160 <span style=''>  }
</span>161 <span style=''>
</span>162 <span style=''>  val timer = </span><span style='background: #AEF1AE'>new Timer(config.timeOperations, logger, &quot;Writing Partition.&quot;)</span><span style=''>
</span>163 <span style=''>
</span>164 <span style=''>  def startPartitionWrite(uniqueId: String): ConnectorResult[Unit] = {
</span>165 <span style=''>    val address = </span><span style='background: #AEF1AE'>getAddress()</span><span style=''>
</span>166 <span style=''>    val delimiter = if(</span><span style='background: #AEF1AE'>address.takeRight(1) == &quot;/&quot; || address.takeRight(1) == &quot;\\&quot;</span><span style=''>) </span><span style='background: #F0ADAD'>&quot;&quot;</span><span style=''> else </span><span style='background: #AEF1AE'>&quot;/&quot;</span><span style=''>
</span>167 <span style=''>    val filename = </span><span style='background: #AEF1AE'>address + delimiter + uniqueId + &quot;.snappy.parquet&quot;</span><span style=''>
</span>168 <span style=''>
</span>169 <span style=''>    </span><span style='background: #AEF1AE'>timer.startTime()</span><span style=''>
</span>170 <span style=''>
</span>171 <span style=''>    </span><span style='background: #AEF1AE'>fileStoreLayer.openWriteParquetFile(filename)</span><span style=''> match {
</span>172 <span style=''>      case Left(err) =&gt;
</span>173 <span style=''>        if(</span><span style='background: #AEF1AE'>!config.fileStoreConfig.preventCleanup</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>174 <span style=''></span><span style='background: #AEF1AE'>          logger.info(&quot;Cleaning up all files in path: &quot; + address)
</span>175 <span style=''></span><span style='background: #AEF1AE'>          fileStoreLayer.removeDir(address)
</span>176 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>177 <span style=''>        </span><span style='background: #AEF1AE'>Left(err)</span><span style=''>
</span>178 <span style=''>      case Right(()) =&gt; </span><span style='background: #AEF1AE'>Right()</span><span style=''>
</span>179 <span style=''>    }
</span>180 <span style=''>  }
</span>181 <span style=''>
</span>182 <span style=''>  def writeData(data: DataBlock): ConnectorResult[Unit] = {
</span>183 <span style=''>    </span><span style='background: #AEF1AE'>config.createExternalTable</span><span style=''> match {
</span>184 <span style=''>      case Some(value) =&gt;
</span>185 <span style=''>        value match {
</span>186 <span style=''>          case ExistingData =&gt; </span><span style='background: #F0ADAD'>Left(NonEmptyDataFrameError())</span><span style=''>
</span>187 <span style=''>          case NewData =&gt; </span><span style='background: #F0ADAD'>fileStoreLayer.writeDataToParquetFile(data)</span><span style=''>
</span>188 <span style=''>        }
</span>189 <span style=''>
</span>190 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>fileStoreLayer.writeDataToParquetFile(data)</span><span style=''>
</span>191 <span style=''>    }
</span>192 <span style=''>  }
</span>193 <span style=''>
</span>194 <span style=''>  def endPartitionWrite(): ConnectorResult[Unit] = {
</span>195 <span style=''>    </span><span style='background: #AEF1AE'>timer.endTime()</span><span style=''>
</span>196 <span style=''>    </span><span style='background: #AEF1AE'>config.createExternalTable</span><span style=''> match {
</span>197 <span style=''>      case Some(value) =&gt;
</span>198 <span style=''>        value match {
</span>199 <span style=''>          case ExistingData =&gt; </span><span style='background: #F0ADAD'>Right(())</span><span style=''>
</span>200 <span style=''>          case NewData =&gt; </span><span style='background: #F0ADAD'>fileStoreLayer.closeWriteParquetFile()</span><span style=''>
</span>201 <span style=''>        }
</span>202 <span style=''>      case None =&gt;  </span><span style='background: #AEF1AE'>fileStoreLayer.closeWriteParquetFile()</span><span style=''>
</span>203 <span style=''>    }
</span>204 <span style=''>  }
</span>205 <span style=''>
</span>206 <span style=''>  def buildCopyStatement(targetTable: String, columnList: String, url: String, rejectsTableName: String, fileFormat: String): String = {
</span>207 <span style=''>    if (</span><span style='background: #AEF1AE'>config.mergeKey.isDefined</span><span style=''>) {
</span>208 <span style=''>      </span><span style='background: #AEF1AE'>s&quot;COPY $targetTable FROM '$url' ON ANY NODE $fileFormat REJECTED DATA AS TABLE $rejectsTableName NO COMMIT&quot;</span><span style=''>
</span>209 <span style=''>    }
</span>210 <span style=''>    else {
</span>211 <span style=''>      </span><span style='background: #AEF1AE'>s&quot;COPY $targetTable $columnList FROM '$url' ON ANY NODE $fileFormat REJECTED DATA AS TABLE $rejectsTableName NO COMMIT&quot;</span><span style=''>
</span>212 <span style=''>    }
</span>213 <span style=''>  }
</span>214 <span style=''>
</span>215 <span style=''>  def buildMergeStatement(targetTableName: TableName, columnList: String, tempTable: String): String = {
</span>216 <span style=''>    val targetTable = </span><span style='background: #AEF1AE'>targetTableName.getFullTableName</span><span style=''>
</span>217 <span style=''>    val updateColValues = </span><span style='background: #AEF1AE'>schemaTools.getMergeUpdateValues(jdbcLayer, targetTableName, tempTableName, config.copyColumnList)</span><span style=''> match {
</span>218 <span style=''>      case Right(values) =&gt; values
</span>219 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(MergeColumnListError(err))</span><span style=''>
</span>220 <span style=''>    }
</span>221 <span style=''>    val insertColValues = </span><span style='background: #AEF1AE'>schemaTools.getMergeInsertValues(jdbcLayer, tempTableName, config.copyColumnList)</span><span style=''> match {
</span>222 <span style=''>      case Right(values) =&gt; values
</span>223 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(MergeColumnListError(err))</span><span style=''>
</span>224 <span style=''>    }
</span>225 <span style=''>    val mergeList = </span><span style='background: #AEF1AE'>config.mergeKey</span><span style=''> match {
</span>226 <span style=''>      case Some(key) =&gt;
</span>227 <span style=''>        val trimmedCols = </span><span style='background: #AEF1AE'>key.toString.split(&quot;,&quot;).toList.map(col =&gt; col.trim())</span><span style=''>
</span>228 <span style=''>        </span><span style='background: #AEF1AE'>trimmedCols.map(trimmedCol =&gt; s&quot;target.$trimmedCol=temp.$trimmedCol&quot;).mkString(&quot; AND &quot;)</span><span style=''>
</span>229 <span style=''>
</span>230 <span style=''>      case None =&gt; </span><span style='background: #F0ADAD'>List()</span><span style=''>
</span>231 <span style=''>    }
</span>232 <span style=''>    </span><span style='background: #AEF1AE'>s&quot;MERGE INTO $targetTable as target using $tempTable as temp ON ($mergeList) WHEN MATCHED THEN UPDATE SET $updateColValues WHEN NOT MATCHED THEN INSERT $columnList VALUES ($insertColValues)&quot;</span><span style=''>
</span>233 <span style=''>  }
</span>234 <span style=''>
</span>235 <span style=''>  def performMerge (mergeStatement: String): ConnectorResult[Unit] = {
</span>236 <span style=''>    val ret = </span><span style='background: #AEF1AE'>for {
</span>237 <span style=''></span><span style='background: #AEF1AE'>
</span>238 <span style=''></span><span style='background: #AEF1AE'>      // Explain merge first to verify it's valid.
</span>239 <span style=''></span><span style='background: #AEF1AE'>      rs &lt;- jdbcLayer.query(&quot;EXPLAIN &quot; + mergeStatement)
</span>240 <span style=''></span><span style='background: #AEF1AE'>      _ = rs.close()
</span>241 <span style=''></span><span style='background: #AEF1AE'>
</span>242 <span style=''></span><span style='background: #AEF1AE'>      // Real merge
</span>243 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- jdbcLayer.execute(mergeStatement)
</span>244 <span style=''></span><span style='background: #AEF1AE'>    } yield ()</span><span style=''>
</span>245 <span style=''>    logger.info(&quot;Executing merge&quot;)
</span>246 <span style=''>    </span><span style='background: #AEF1AE'>ret.left.map(err =&gt; </span><span style='background: #F0ADAD'>CommitError(err).context( &quot;performMerge: JDBC error when trying to merge&quot;)</span><span style='background: #AEF1AE'>)</span><span style=''>
</span>247 <span style=''>
</span>248 <span style=''>  }
</span>249 <span style=''>
</span>250 <span style=''>  def inferExternalTableSchema(): ConnectorResult[String] = {
</span>251 <span style=''>    val tableName = </span><span style='background: #AEF1AE'>config.tablename.getFullTableName.replaceAll(&quot;\&quot;&quot;,&quot;&quot;)</span><span style=''>
</span>252 <span style=''>
</span>253 <span style=''>    val inferStatement =
</span>254 <span style=''>    </span><span style='background: #AEF1AE'>fileStoreLayer.getGlobStatus(EscapeUtils.sqlEscape(s&quot;${config.fileStoreConfig.externalTableAddress.stripSuffix(&quot;/&quot;)}/*.parquet&quot;))</span><span style=''> match {
</span>255 <span style=''>      case Right(list) =&gt;
</span>256 <span style=''>        val url: String =
</span>257 <span style=''>          if(</span><span style='background: #AEF1AE'>list.nonEmpty</span><span style=''>) {
</span>258 <span style=''>            </span><span style='background: #AEF1AE'>EscapeUtils.sqlEscape(s&quot;${config.fileStoreConfig.externalTableAddress.stripSuffix(&quot;/&quot;)}/*.parquet&quot;)</span><span style=''>
</span>259 <span style=''>          }
</span>260 <span style=''>          else {
</span>261 <span style=''>            </span><span style='background: #F0ADAD'>EscapeUtils.sqlEscape(s&quot;${config.fileStoreConfig.externalTableAddress.stripSuffix(&quot;/&quot;)}/**/*.parquet&quot;)</span><span style=''>
</span>262 <span style=''>          }
</span>263 <span style=''>        </span><span style='background: #AEF1AE'>&quot;SELECT INFER_EXTERNAL_TABLE_DDL(&quot; + &quot;\'&quot; + url + &quot;\',\'&quot; + tableName + &quot;\')&quot;</span><span style=''>
</span>264 <span style=''>
</span>265 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>err.getFullContext</span><span style=''>
</span>266 <span style=''>    }
</span>267 <span style=''>
</span>268 <span style=''>    logger.debug(&quot;The infer statement is: &quot; + inferStatement)
</span>269 <span style=''>    </span><span style='background: #AEF1AE'>jdbcLayer.query(inferStatement)</span><span style=''> match {
</span>270 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(InferExternalTableSchemaError(err))</span><span style=''>
</span>271 <span style=''>      case Right(resultSet) =&gt;
</span>272 <span style=''>        try {
</span>273 <span style=''>          </span><span style='background: #AEF1AE'>val iterate = resultSet.next
</span>274 <span style=''></span><span style='background: #AEF1AE'>          val createExternalTableStatement = resultSet.getString(&quot;INFER_EXTERNAL_TABLE_DDL&quot;)
</span>275 <span style=''></span><span style='background: #AEF1AE'>          val isPartitioned = inferStatement.contains(EscapeUtils.sqlEscape(s&quot;${config.fileStoreConfig.externalTableAddress.stripSuffix(&quot;/&quot;)}/**/*.parquet&quot;))
</span>276 <span style=''></span><span style='background: #AEF1AE'>
</span>277 <span style=''></span><span style='background: #AEF1AE'>          if(!isPartitioned &amp;&amp; !createExternalTableStatement.contains(&quot;varchar&quot;) &amp;&amp; !createExternalTableStatement.contains(&quot;varbinary&quot;)) {
</span>278 <span style=''></span><span style='background: #AEF1AE'>            logger.info(&quot;Inferring schema from parquet data&quot;)
</span>279 <span style=''></span><span style='background: #AEF1AE'>            val updatedStatement = createExternalTableStatement.replace(&quot;\&quot;&quot; + tableName + &quot;\&quot;&quot;, tableName)
</span>280 <span style=''></span><span style='background: #AEF1AE'>            logger.debug(&quot;The create external table statement is: &quot; + updatedStatement)
</span>281 <span style=''></span><span style='background: #AEF1AE'>            Right(updatedStatement)
</span>282 <span style=''></span><span style='background: #AEF1AE'>          }
</span>283 <span style=''></span><span style='background: #AEF1AE'>          else </span><span style='background: #F0ADAD'>{
</span>284 <span style=''></span><span style='background: #F0ADAD'>            logger.info(&quot;Inferring partial schema from dataframe&quot;)
</span>285 <span style=''></span><span style='background: #F0ADAD'>            schemaTools.inferExternalTableSchema(createExternalTableStatement, config.schema, tableName, config.strlen)
</span>286 <span style=''></span><span style='background: #F0ADAD'>          }</span><span style=''>
</span>287 <span style=''>        }
</span>288 <span style=''>        catch {
</span>289 <span style=''>          case e: Throwable =&gt;
</span>290 <span style=''>            </span><span style='background: #F0ADAD'>Left(InferExternalSchemaError(e))</span><span style=''>
</span>291 <span style=''>        }
</span>292 <span style=''>        finally {
</span>293 <span style=''>          </span><span style='background: #AEF1AE'>resultSet.close()</span><span style=''>
</span>294 <span style=''>        }
</span>295 <span style=''>    }
</span>296 <span style=''>  }
</span>297 <span style=''>
</span>298 <span style=''>  /**
</span>299 <span style=''>   * Function to get column list to use for the operation
</span>300 <span style=''>   *
</span>301 <span style=''>   * Two options depending on configuration and specified table:
</span>302 <span style=''>   * - Custom column list provided by the user
</span>303 <span style=''>   * - Column list built for a subset of rows in the table that match our schema
</span>304 <span style=''>   */
</span>305 <span style=''>  private def getColumnList: ConnectorResult[String] = {
</span>306 <span style=''>    </span><span style='background: #AEF1AE'>config.copyColumnList</span><span style=''> match {
</span>307 <span style=''>      case Some(list) =&gt;
</span>308 <span style=''>        logger.info(s&quot;Using custom COPY column list. &quot; + &quot;Target table: &quot; + config.tablename.getFullTableName +
</span>309 <span style=''>          &quot;, &quot; + &quot;copy_column_list: &quot; + list + &quot;.&quot;)
</span>310 <span style=''>        </span><span style='background: #AEF1AE'>Right(&quot;(&quot; + list.toString.split(&quot;,&quot;).map(col =&gt; col.trim()).mkString(&quot;,&quot;) + &quot;)&quot;)</span><span style=''>
</span>311 <span style=''>      case None =&gt;
</span>312 <span style=''>        // Default COPY
</span>313 <span style=''>        logger.info(s&quot;Building default copy column list&quot;)
</span>314 <span style=''>        </span><span style='background: #AEF1AE'>schemaTools.getCopyColumnList(jdbcLayer, config.tablename, config.schema)
</span>315 <span style=''></span><span style='background: #AEF1AE'>          .left.map(err =&gt; </span><span style='background: #F0ADAD'>SchemaColumnListError(err)
</span>316 <span style=''></span><span style='background: #F0ADAD'>          .context(&quot;getColumnList: Error building default copy column list&quot;)</span><span style='background: #AEF1AE'>)</span><span style=''>
</span>317 <span style=''>    }
</span>318 <span style=''>  }
</span>319 <span style=''>
</span>320 <span style=''>  private case class FaultToleranceTestResult(success: Boolean, failedRowsPercent: Double)
</span>321 <span style=''>
</span>322 <span style=''>  private def testFaultTolerance(rowsCopied: Int, rejectsTable: String) : ConnectorResult[FaultToleranceTestResult] = {
</span>323 <span style=''>    // verify rejects to see if this falls within user tolerance.
</span>324 <span style=''>    val rejectsQuery = </span><span style='background: #AEF1AE'>&quot;SELECT COUNT(*) as count FROM &quot; + rejectsTable</span><span style=''>
</span>325 <span style=''>    logger.info(s&quot;Checking number of rejected rows via statement: &quot; + rejectsQuery)
</span>326 <span style=''>
</span>327 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>328 <span style=''></span><span style='background: #AEF1AE'>      rs &lt;- jdbcLayer.query(rejectsQuery)
</span>329 <span style=''></span><span style='background: #AEF1AE'>      res = Try {
</span>330 <span style=''></span><span style='background: #AEF1AE'>        if (rs.next) {
</span>331 <span style=''></span><span style='background: #AEF1AE'>          rs.getInt(&quot;count&quot;)
</span>332 <span style=''></span><span style='background: #AEF1AE'>        }
</span>333 <span style=''></span><span style='background: #AEF1AE'>        else </span><span style='background: #F0ADAD'>{
</span>334 <span style=''></span><span style='background: #F0ADAD'>          logger.error(&quot;Could not retrieve rejected row count.&quot;)
</span>335 <span style=''></span><span style='background: #F0ADAD'>          0
</span>336 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style='background: #AEF1AE'>
</span>337 <span style=''></span><span style='background: #AEF1AE'>      }
</span>338 <span style=''></span><span style='background: #AEF1AE'>      _ = rs.close()
</span>339 <span style=''></span><span style='background: #AEF1AE'>      rejectedCount &lt;- JdbcUtils.tryJdbcToResult(jdbcLayer, res)
</span>340 <span style=''></span><span style='background: #AEF1AE'>
</span>341 <span style=''></span><span style='background: #AEF1AE'>      failedRowsPercent = if (rowsCopied &gt; 0) {
</span>342 <span style=''></span><span style='background: #AEF1AE'>        rejectedCount.toDouble / (rowsCopied.toDouble + rejectedCount.toDouble)
</span>343 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>344 <span style=''></span><span style='background: #AEF1AE'>        </span><span style='background: #F0ADAD'>1.0</span><span style='background: #AEF1AE'>
</span>345 <span style=''></span><span style='background: #AEF1AE'>      }
</span>346 <span style=''></span><span style='background: #AEF1AE'>
</span>347 <span style=''></span><span style='background: #AEF1AE'>
</span>348 <span style=''></span><span style='background: #AEF1AE'>      passedFaultToleranceTest = failedRowsPercent &lt;= config.failedRowPercentTolerance.toDouble
</span>349 <span style=''></span><span style='background: #AEF1AE'>
</span>350 <span style=''></span><span style='background: #AEF1AE'>      // Report save status message to user either way.
</span>351 <span style=''></span><span style='background: #AEF1AE'>      tolerance_message =
</span>352 <span style=''></span><span style='background: #AEF1AE'>        &quot;Number of rows_rejected=&quot; + rejectedCount +
</span>353 <span style=''></span><span style='background: #AEF1AE'>          &quot;. rows_copied=&quot; + rowsCopied +
</span>354 <span style=''></span><span style='background: #AEF1AE'>          &quot;. failedRowsPercent=&quot; + failedRowsPercent +
</span>355 <span style=''></span><span style='background: #AEF1AE'>          &quot;. user's failed_rows_percent_tolerance=&quot; +
</span>356 <span style=''></span><span style='background: #AEF1AE'>          config.failedRowPercentTolerance +
</span>357 <span style=''></span><span style='background: #AEF1AE'>          &quot;. passedFaultToleranceTest=&quot; + passedFaultToleranceTest.toString
</span>358 <span style=''></span><span style='background: #AEF1AE'>
</span>359 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(s&quot;Verifying rows saved to Vertica is within user tolerance...&quot;)
</span>360 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(tolerance_message + {
</span>361 <span style=''></span><span style='background: #AEF1AE'>        if (passedFaultToleranceTest) {
</span>362 <span style=''></span><span style='background: #AEF1AE'>          &quot;...PASSED.  OK to commit to database.&quot;
</span>363 <span style=''></span><span style='background: #AEF1AE'>        } else {
</span>364 <span style=''></span><span style='background: #AEF1AE'>          &quot;...FAILED.  NOT OK to commit to database&quot;
</span>365 <span style=''></span><span style='background: #AEF1AE'>        }
</span>366 <span style=''></span><span style='background: #AEF1AE'>      })
</span>367 <span style=''></span><span style='background: #AEF1AE'>
</span>368 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (rejectedCount == 0) {
</span>369 <span style=''></span><span style='background: #AEF1AE'>        val dropRejectsTableStatement = &quot;DROP TABLE IF EXISTS &quot; + rejectsTable  + &quot; CASCADE&quot;
</span>370 <span style=''></span><span style='background: #AEF1AE'>        logger.info(s&quot;Dropping Vertica rejects table now: &quot; + dropRejectsTableStatement)
</span>371 <span style=''></span><span style='background: #AEF1AE'>        jdbcLayer.execute(dropRejectsTableStatement)
</span>372 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>373 <span style=''></span><span style='background: #AEF1AE'>        // Log the first few rejected rows
</span>374 <span style=''></span><span style='background: #AEF1AE'>        val rejectsDataQuery = &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot; + rejectsTable + &quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;
</span>375 <span style=''></span><span style='background: #AEF1AE'>        logger.info(s&quot;Getting summary of rejected rows via statement: &quot; + rejectsDataQuery)
</span>376 <span style=''></span><span style='background: #AEF1AE'>        for {
</span>377 <span style=''></span><span style='background: #AEF1AE'>          rs &lt;- jdbcLayer.query(rejectsDataQuery)
</span>378 <span style=''></span><span style='background: #AEF1AE'>          _ = Try {
</span>379 <span style=''></span><span style='background: #AEF1AE'>            val rsmd = rs.getMetaData
</span>380 <span style=''></span><span style='background: #AEF1AE'>            logger.error(s&quot;Found $rejectedCount rejected rows, displaying up to 10 of the most common reasons:&quot;)
</span>381 <span style=''></span><span style='background: #AEF1AE'>            logger.error((1 to rsmd.getColumnCount).map(idx =&gt; rsmd.getColumnName(idx)).toList.mkString(&quot; | &quot;))
</span>382 <span style=''></span><span style='background: #AEF1AE'>            while (</span><span style='background: #F0ADAD'>rs.next</span><span style='background: #AEF1AE'>) {
</span>383 <span style=''></span><span style='background: #AEF1AE'>              </span><span style='background: #F0ADAD'>logger.error((1 to rsmd.getColumnCount).map(idx =&gt; rs.getString(idx)).toList.mkString(&quot; | &quot;))</span><span style='background: #AEF1AE'>
</span>384 <span style=''></span><span style='background: #AEF1AE'>            }
</span>385 <span style=''></span><span style='background: #AEF1AE'>          }
</span>386 <span style=''></span><span style='background: #AEF1AE'>          _ = rs.close()
</span>387 <span style=''></span><span style='background: #AEF1AE'>        } yield ()
</span>388 <span style=''></span><span style='background: #AEF1AE'>
</span>389 <span style=''></span><span style='background: #AEF1AE'>        Right(())
</span>390 <span style=''></span><span style='background: #AEF1AE'>      }
</span>391 <span style=''></span><span style='background: #AEF1AE'>
</span>392 <span style=''></span><span style='background: #AEF1AE'>      testResult = FaultToleranceTestResult(passedFaultToleranceTest, failedRowsPercent)
</span>393 <span style=''></span><span style='background: #AEF1AE'>    } yield testResult</span><span style=''>
</span>394 <span style=''>  }
</span>395 <span style=''>
</span>396 <span style=''>  /**
</span>397 <span style=''>   * Performs copy operations
</span>398 <span style=''>   *
</span>399 <span style=''>   * @return rows copied
</span>400 <span style=''>   */
</span>401 <span style=''>  def performCopy(copyStatement: String, tablename: TableName): ConnectorResult[Int] = {
</span>402 <span style=''>    // Empty copy to make sure a projection is created if it hasn't been yet
</span>403 <span style=''>    // This will error out, but create the projection
</span>404 <span style=''>    val emptyCopy = </span><span style='background: #AEF1AE'>&quot;COPY &quot; + tablename.getFullTableName + &quot; FROM '';&quot;</span><span style=''>
</span>405 <span style=''>    </span><span style='background: #AEF1AE'>jdbcLayer.executeUpdate(emptyCopy)</span><span style=''>
</span>406 <span style=''>
</span>407 <span style=''>    val ret = </span><span style='background: #AEF1AE'>for {
</span>408 <span style=''></span><span style='background: #AEF1AE'>
</span>409 <span style=''></span><span style='background: #AEF1AE'>      // Explain copy first to verify it's valid.
</span>410 <span style=''></span><span style='background: #AEF1AE'>      rs &lt;- jdbcLayer.query(&quot;EXPLAIN &quot; + copyStatement)
</span>411 <span style=''></span><span style='background: #AEF1AE'>      _ = rs.close()
</span>412 <span style=''></span><span style='background: #AEF1AE'>
</span>413 <span style=''></span><span style='background: #AEF1AE'>      // Real copy
</span>414 <span style=''></span><span style='background: #AEF1AE'>      rowsCopied &lt;- jdbcLayer.executeUpdate(copyStatement)
</span>415 <span style=''></span><span style='background: #AEF1AE'>    } yield rowsCopied</span><span style=''>
</span>416 <span style=''>    logger.info(&quot;Performing copy from file store to Vertica&quot;)
</span>417 <span style=''>    </span><span style='background: #AEF1AE'>ret.left.map(err =&gt; CommitError(err).context(&quot;performCopy: JDBC error when trying to copy&quot;))</span><span style=''>
</span>418 <span style=''>  }
</span>419 <span style=''>
</span>420 <span style=''>  def commitDataIntoVertica(url: String): ConnectorResult[Unit] = {
</span>421 <span style=''>    val tableNameMaxLength = </span><span style='background: #AEF1AE'>30</span><span style=''>
</span>422 <span style=''>    val timer = </span><span style='background: #AEF1AE'>new Timer(config.timeOperations, logger, &quot;Copy and commit data into Vertica&quot;)</span><span style=''>
</span>423 <span style=''>    </span><span style='background: #AEF1AE'>timer.startTime()</span><span style=''>
</span>424 <span style=''>    val ret = </span><span style='background: #AEF1AE'>for {
</span>425 <span style=''></span><span style='background: #AEF1AE'>      // Set Vertica to work with kerberos and HDFS/AWS
</span>426 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- jdbcLayer.configureSession(fileStoreLayer)
</span>427 <span style=''></span><span style='background: #AEF1AE'>
</span>428 <span style=''></span><span style='background: #AEF1AE'>      // Get columnList
</span>429 <span style=''></span><span style='background: #AEF1AE'>      columnList &lt;- getColumnList.left.map(</span><span style='background: #F0ADAD'>_.context(&quot;commit: Failed to get column list&quot;)</span><span style='background: #AEF1AE'>)
</span>430 <span style=''></span><span style='background: #AEF1AE'>
</span>431 <span style=''></span><span style='background: #AEF1AE'>      // Do not check for mergeKey here, as tableName does not impact merge
</span>432 <span style=''></span><span style='background: #AEF1AE'>      tableName = config.tablename.name
</span>433 <span style=''></span><span style='background: #AEF1AE'>      sessionId = config.sessionId
</span>434 <span style=''></span><span style='background: #AEF1AE'>
</span>435 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.mergeKey.isDefined) tableUtils.createTempTable(tempTableName, config.schema, config.strlen) else Right(())
</span>436 <span style=''></span><span style='background: #AEF1AE'>
</span>437 <span style=''></span><span style='background: #AEF1AE'>      tempTableExists &lt;- tableUtils.tempTableExists(tempTableName)
</span>438 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.mergeKey.isDefined &amp;&amp; !tempTableExists) </span><span style='background: #F0ADAD'>Left(CreateTableError(None))</span><span style='background: #AEF1AE'> else Right(())
</span>439 <span style=''></span><span style='background: #AEF1AE'>
</span>440 <span style=''></span><span style='background: #AEF1AE'>      rejectsTableName = &quot;\&quot;&quot; +
</span>441 <span style=''></span><span style='background: #AEF1AE'>        EscapeUtils.sqlEscape(tableName.substring(0,Math.min(tableNameMaxLength,tableName.length))) +
</span>442 <span style=''></span><span style='background: #AEF1AE'>        &quot;_&quot; +
</span>443 <span style=''></span><span style='background: #AEF1AE'>        sessionId +
</span>444 <span style=''></span><span style='background: #AEF1AE'>        &quot;_COMMITS&quot; +
</span>445 <span style=''></span><span style='background: #AEF1AE'>        &quot;\&quot;&quot;
</span>446 <span style=''></span><span style='background: #AEF1AE'>
</span>447 <span style=''></span><span style='background: #AEF1AE'>      fullTableName &lt;- if(config.mergeKey.isDefined) Right(tempTableName.getFullTableName) else Right(config.tablename.getFullTableName)
</span>448 <span style=''></span><span style='background: #AEF1AE'>
</span>449 <span style=''></span><span style='background: #AEF1AE'>      copyStatement = buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;)
</span>450 <span style=''></span><span style='background: #AEF1AE'>
</span>451 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;The copy statement is: \n&quot; + copyStatement)
</span>452 <span style=''></span><span style='background: #AEF1AE'>
</span>453 <span style=''></span><span style='background: #AEF1AE'>      rowsCopied &lt;- if (config.mergeKey.isDefined) {
</span>454 <span style=''></span><span style='background: #AEF1AE'>        performCopy(copyStatement, tempTableName).left.map(</span><span style='background: #F0ADAD'>_.context(&quot;commit: Failed to copy rows into temp table&quot;)</span><span style='background: #AEF1AE'>)
</span>455 <span style=''></span><span style='background: #AEF1AE'>      }
</span>456 <span style=''></span><span style='background: #AEF1AE'>      else {
</span>457 <span style=''></span><span style='background: #AEF1AE'>        performCopy(copyStatement, config.tablename).left.map(_.context(&quot;commit: Failed to copy rows into target table&quot;))
</span>458 <span style=''></span><span style='background: #AEF1AE'>      }
</span>459 <span style=''></span><span style='background: #AEF1AE'>
</span>460 <span style=''></span><span style='background: #AEF1AE'>      faultToleranceResults &lt;- testFaultTolerance(rowsCopied, rejectsTableName)
</span>461 <span style=''></span><span style='background: #AEF1AE'>        .left.map(err =&gt; </span><span style='background: #F0ADAD'>CommitError(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;)</span><span style='background: #AEF1AE'>)
</span>462 <span style=''></span><span style='background: #AEF1AE'>
</span>463 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.saveJobStatusTable) {
</span>464 <span style=''></span><span style='background: #AEF1AE'>        tableUtils.updateJobStatusTable(config.tablename, config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, config.sessionId, faultToleranceResults.success)
</span>465 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>466 <span style=''></span><span style='background: #AEF1AE'>        Right(())
</span>467 <span style=''></span><span style='background: #AEF1AE'>      }
</span>468 <span style=''></span><span style='background: #AEF1AE'>
</span>469 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (faultToleranceResults.success) Right(()) else Left(FaultToleranceTestFail())
</span>470 <span style=''></span><span style='background: #AEF1AE'>
</span>471 <span style=''></span><span style='background: #AEF1AE'>      mergeStatement &lt;- if (config.mergeKey.isDefined) {
</span>472 <span style=''></span><span style='background: #AEF1AE'>                          Right(buildMergeStatement(config.tablename, columnList, tempTableName.getFullTableName))
</span>473 <span style=''></span><span style='background: #AEF1AE'>                        }
</span>474 <span style=''></span><span style='background: #AEF1AE'>                        else {
</span>475 <span style=''></span><span style='background: #AEF1AE'>                          Right(&quot;&quot;)
</span>476 <span style=''></span><span style='background: #AEF1AE'>                        }
</span>477 <span style=''></span><span style='background: #AEF1AE'>      _ = if(config.mergeKey.isDefined) logger.info(&quot;The merge statement is: \n&quot; + mergeStatement)
</span>478 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.mergeKey.isDefined) performMerge(mergeStatement) else Right(())
</span>479 <span style=''></span><span style='background: #AEF1AE'>
</span>480 <span style=''></span><span style='background: #AEF1AE'>    } yield ()</span><span style=''>
</span>481 <span style=''>    logger.info(&quot;Committing data into Vertica.&quot;)
</span>482 <span style=''>    if(</span><span style='background: #AEF1AE'>!config.fileStoreConfig.preventCleanup</span><span style=''>) </span><span style='background: #AEF1AE'>fileStoreLayer.removeDir(config.fileStoreConfig.address)</span><span style=''>
</span>483 <span style=''>    </span><span style='background: #AEF1AE'>timer.endTime()</span><span style=''>
</span>484 <span style=''>    ret
</span>485 <span style=''>  }
</span>486 <span style=''>
</span>487 <span style=''>  def commitDataAsExternalTable(url: String): ConnectorResult[Unit] = {
</span>488 <span style=''>    val timer = </span><span style='background: #AEF1AE'>new Timer(config.timeOperations, logger, &quot;Commit data as external table&quot;)</span><span style=''>
</span>489 <span style=''>    </span><span style='background: #AEF1AE'>timer.startTime()</span><span style=''>
</span>490 <span style=''>    logger.info(&quot;Committing data as external table.&quot;)
</span>491 <span style=''>    if(</span><span style='background: #AEF1AE'>config.copyColumnList.isDefined</span><span style=''>) {
</span>492 <span style=''>      </span><span style='background: #F0ADAD'>logger.warn(&quot;Custom copy column list was specified, but will be ignored when creating new external table.&quot;)</span><span style=''>
</span>493 <span style=''>    }
</span>494 <span style=''>
</span>495 <span style=''>    val ret = </span><span style='background: #AEF1AE'>for {
</span>496 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- jdbcLayer.configureSession(fileStoreLayer)
</span>497 <span style=''></span><span style='background: #AEF1AE'>
</span>498 <span style=''></span><span style='background: #AEF1AE'>      existingData = config.createExternalTable match {
</span>499 <span style=''></span><span style='background: #AEF1AE'>        case Some(value) =&gt;
</span>500 <span style=''></span><span style='background: #AEF1AE'>          value match {
</span>501 <span style=''></span><span style='background: #AEF1AE'>            case ExistingData =&gt; true
</span>502 <span style=''></span><span style='background: #AEF1AE'>            case NewData =&gt; false
</span>503 <span style=''></span><span style='background: #AEF1AE'>          }
</span>504 <span style=''></span><span style='background: #AEF1AE'>        case None =&gt; </span><span style='background: #F0ADAD'>false</span><span style='background: #AEF1AE'>
</span>505 <span style=''></span><span style='background: #AEF1AE'>      }
</span>506 <span style=''></span><span style='background: #AEF1AE'>      createExternalTableStmt &lt;- if(existingData) inferExternalTableSchema else Right(())
</span>507 <span style=''></span><span style='background: #AEF1AE'>
</span>508 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- tableUtils.createExternalTable(
</span>509 <span style=''></span><span style='background: #AEF1AE'>                tablename = config.tablename,
</span>510 <span style=''></span><span style='background: #AEF1AE'>                if(existingData) Some(createExternalTableStmt.toString) else config.targetTableSql,
</span>511 <span style=''></span><span style='background: #AEF1AE'>                schema = config.schema,
</span>512 <span style=''></span><span style='background: #AEF1AE'>                strlen = config.strlen,
</span>513 <span style=''></span><span style='background: #AEF1AE'>                urlToCopyFrom = url
</span>514 <span style=''></span><span style='background: #AEF1AE'>              )
</span>515 <span style=''></span><span style='background: #AEF1AE'>
</span>516 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- tableUtils.validateExternalTable(config.tablename)
</span>517 <span style=''></span><span style='background: #AEF1AE'>
</span>518 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (config.saveJobStatusTable) {
</span>519 <span style=''></span><span style='background: #AEF1AE'>        tableUtils.updateJobStatusTable(config.tablename, config.jdbcConfig.auth.user, 0.0, config.sessionId, success = true)
</span>520 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>521 <span style=''></span><span style='background: #AEF1AE'>        Right(())
</span>522 <span style=''></span><span style='background: #AEF1AE'>      }
</span>523 <span style=''></span><span style='background: #AEF1AE'>    } yield ()</span><span style=''>
</span>524 <span style=''>
</span>525 <span style=''>    </span><span style='background: #AEF1AE'>timer.endTime()</span><span style=''>
</span>526 <span style=''>
</span>527 <span style=''>    // External table creation always commits. So, if an error was detected, drop the table
</span>528 <span style=''>    ret match {
</span>529 <span style=''>      case Left(err) =&gt;
</span>530 <span style=''>        </span><span style='background: #AEF1AE'>tableUtils.dropTable(config.tablename)</span><span style=''>
</span>531 <span style=''>        </span><span style='background: #AEF1AE'>Left(err)</span><span style=''>
</span>532 <span style=''>      case Right(_) =&gt; </span><span style='background: #AEF1AE'>Right()</span><span style=''>
</span>533 <span style=''>    }
</span>534 <span style=''>  }
</span>535 <span style=''>
</span>536 <span style=''>  def commit(): ConnectorResult[Unit] = {
</span>537 <span style=''>    val globPattern: String = </span><span style='background: #AEF1AE'>&quot;*.parquet&quot;</span><span style=''>
</span>538 <span style=''>
</span>539 <span style=''>    // Create url string, escape any ' characters as those surround the url
</span>540 <span style=''>    val url: String = </span><span style='background: #AEF1AE'>EscapeUtils.sqlEscape(s&quot;${getAddress().stripSuffix(&quot;/&quot;)}/$globPattern&quot;)</span><span style=''>
</span>541 <span style=''>
</span>542 <span style=''>    val ret = if(</span><span style='background: #AEF1AE'>config.createExternalTable.isDefined</span><span style=''>) {
</span>543 <span style=''>      </span><span style='background: #AEF1AE'>commitDataAsExternalTable(url)</span><span style=''>
</span>544 <span style=''>    }
</span>545 <span style=''>    else {
</span>546 <span style=''>      </span><span style='background: #AEF1AE'>commitDataIntoVertica(url)</span><span style=''>
</span>547 <span style=''>    }
</span>548 <span style=''>
</span>549 <span style=''>    // Commit or rollback
</span>550 <span style=''>    val result = ret match {
</span>551 <span style=''>      case Right(_) =&gt;
</span>552 <span style=''>        </span><span style='background: #AEF1AE'>jdbcLayer.commit().left.map(err =&gt; </span><span style='background: #F0ADAD'>CommitError(err).context(&quot;JDBC Error when trying to commit&quot;)</span><span style='background: #AEF1AE'>)</span><span style=''>
</span>553 <span style=''>      case Left(retError) =&gt;
</span>554 <span style=''>        </span><span style='background: #AEF1AE'>jdbcLayer.rollback()</span><span style=''> match {
</span>555 <span style=''>          case Right(_) =&gt; </span><span style='background: #AEF1AE'>Left(retError)</span><span style=''>
</span>556 <span style=''>          case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(retError.context(&quot;JDBC Error when trying to rollback: &quot; + err.getFullContext))</span><span style=''>
</span>557 <span style=''>        }
</span>558 <span style=''>    }
</span>559 <span style=''>
</span>560 <span style=''>    </span><span style='background: #AEF1AE'>jdbcLayer.close()</span><span style=''>
</span>561 <span style=''>    result
</span>562 <span style=''>  }
</span>563 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          50
        </td>
        <td>
          1013
        </td>
        <td>
          2478
          -
          2547
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe])
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          1016
        </td>
        <td>
          2588
          -
          2634
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename.name.+(&quot;_&quot;).+(VerticaDistributedFilesystemWritePipe.this.config.sessionId)
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          1015
        </td>
        <td>
          2618
          -
          2634
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.sessionId
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.sessionId
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          1018
        </td>
        <td>
          2578
          -
          2641
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.TableName.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.TableName.apply(VerticaDistributedFilesystemWritePipe.this.config.tablename.name.+(&quot;_&quot;).+(VerticaDistributedFilesystemWritePipe.this.config.sessionId), scala.None)
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          1014
        </td>
        <td>
          2612
          -
          2615
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;_&quot;
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          1017
        </td>
        <td>
          2636
          -
          2640
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1019
        </td>
        <td>
          2694
          -
          2746
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;spark.sql.legacy.parquet.datetimeRebaseModeInWrite&quot;
        </td>
      </tr><tr>
        <td>
          53
        </td>
        <td>
          1020
        </td>
        <td>
          2805
          -
          2854
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;spark.sql.legacy.parquet.int96RebaseModeInWrite&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          1022
        </td>
        <td>
          2983
          -
          3012
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.config.VerticaWriteMetadata](com.vertica.spark.config.VerticaWriteMetadata.apply())
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          1021
        </td>
        <td>
          2989
          -
          3011
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.VerticaWriteMetadata.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.config.VerticaWriteMetadata.apply()
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          1024
        </td>
        <td>
          3127
          -
          3142
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Long](VerticaDistributedFilesystemWritePipe.this.dataSize.toLong)
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          1023
        </td>
        <td>
          3133
          -
          3141
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.dataSize.toLong
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          1028
        </td>
        <td>
          3239
          -
          3269
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](schema.fields).map[String, Array[String]](((f: org.apache.spark.sql.types.StructField) =&gt; f.name))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          1025
        </td>
        <td>
          3239
          -
          3252
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fields
        </td>
        <td style="background: #AEF1AE">
          schema.fields
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          1027
        </td>
        <td>
          3256
          -
          3256
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          1026
        </td>
        <td>
          3262
          -
          3268
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td style="background: #AEF1AE">
          f.name
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          1030
        </td>
        <td>
          3277
          -
          3314
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.!=
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[String](names).distinct.length.!=(names.length)
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          1029
        </td>
        <td>
          3302
          -
          3314
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Array.length
        </td>
        <td style="background: #AEF1AE">
          names.length
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          1031
        </td>
        <td>
          3329
          -
          3352
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.DuplicateColumnsError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.DuplicateColumnsError.apply()
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          1033
        </td>
        <td>
          3324
          -
          3353
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.DuplicateColumnsError, Nothing](com.vertica.spark.util.error.DuplicateColumnsError.apply())
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          1032
        </td>
        <td>
          3324
          -
          3353
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.DuplicateColumnsError, Nothing](com.vertica.spark.util.error.DuplicateColumnsError.apply())
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          1034
        </td>
        <td>
          3373
          -
          3382
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          1035
        </td>
        <td>
          3373
          -
          3382
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          1036
        </td>
        <td>
          3576
          -
          3605
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.SparkSession.getActiveSession
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.SparkSession.getActiveSession
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1037
        </td>
        <td>
          3688
          -
          3723
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.LEGACY_PARQUET_REBASE_MODE_IN_WRITE
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.LEGACY_PARQUET_REBASE_MODE_IN_WRITE
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1039
        </td>
        <td>
          3650
          -
          3737
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.SparkContext.setLocalProperty
        </td>
        <td style="background: #AEF1AE">
          session.sparkContext.setLocalProperty(VerticaDistributedFilesystemWritePipe.this.LEGACY_PARQUET_REBASE_MODE_IN_WRITE, &quot;CORRECTED&quot;)
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1038
        </td>
        <td>
          3725
          -
          3736
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;CORRECTED&quot;
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1040
        </td>
        <td>
          3784
          -
          3825
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.LEGACY_PARQUET_INT96_REBASE_MODE_IN_WRITE
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.LEGACY_PARQUET_INT96_REBASE_MODE_IN_WRITE
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1042
        </td>
        <td>
          3746
          -
          3839
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.SparkContext.setLocalProperty
        </td>
        <td style="background: #AEF1AE">
          session.sparkContext.setLocalProperty(VerticaDistributedFilesystemWritePipe.this.LEGACY_PARQUET_INT96_REBASE_MODE_IN_WRITE, &quot;CORRECTED&quot;)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1041
        </td>
        <td>
          3827
          -
          3838
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;CORRECTED&quot;
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          1043
        </td>
        <td>
          4104
          -
          4140
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          1046
        </td>
        <td>
          4191
          -
          4221
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          1045
        </td>
        <td>
          4142
          -
          4185
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.externalTableAddress
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          1047
        </td>
        <td>
          4191
          -
          4221
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          1044
        </td>
        <td>
          4142
          -
          4185
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.externalTableAddress
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1052
        </td>
        <td>
          4636
          -
          4636
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1049
        </td>
        <td>
          4639
          -
          4686
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1051
        </td>
        <td>
          4636
          -
          4636
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1048
        </td>
        <td>
          4668
          -
          4686
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.isOverwrite
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.config.isOverwrite
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1050
        </td>
        <td>
          4688
          -
          4754
        </td>
        <td>
          Typed
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isWarnEnabled())
  VerticaDistributedFilesystemWritePipe.this.logger.underlying.warn(&quot;Save mode is specified as Overwrite during a merge.&quot;)
else
  (): Unit)
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1055
        </td>
        <td>
          4854
          -
          4854
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Unit, Unit](x$1, x$2)
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1154
        </td>
        <td>
          4808
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.checkSchemaForDuplicates(VerticaDistributedFilesystemWritePipe.this.config.schema).map[(Unit, Unit)](((x$1: Unit) =&gt; {
  val x$2: Unit = VerticaDistributedFilesystemWritePipe.this.setSparkCalendarConf();
  scala.Tuple2.apply[Unit, Unit](x$1, x$2)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$5: (Unit, Unit)) =&gt; (x$5: (Unit, Unit) @unchecked) match {
  case (_1: Unit, _2: Unit)(Unit, Unit)(_, _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.dropTable(VerticaDistributedFilesystemWritePipe.this.config.tablename)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableMergeKey, Nothing](com.vertica.spark.util.error.CreateExternalTableMergeKey.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPre: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
    case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
  }))))))))))))))))))))))))
}))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1053
        </td>
        <td>
          4884
          -
          4897
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          1054
        </td>
        <td>
          4943
          -
          4965
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.setSparkCalendarConf
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.setSparkCalendarConf()
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1061
        </td>
        <td>
          5159
          -
          5168
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1058
        </td>
        <td>
          5136
          -
          5152
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1153
        </td>
        <td>
          5059
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.dropTable(VerticaDistributedFilesystemWritePipe.this.config.tablename)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableMergeKey, Nothing](com.vertica.spark.util.error.CreateExternalTableMergeKey.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPre: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))))))))))))
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1057
        </td>
        <td>
          5068
          -
          5113
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1060
        </td>
        <td>
          5115
          -
          5153
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.dropTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.dropTable(VerticaDistributedFilesystemWritePipe.this.config.tablename)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1056
        </td>
        <td>
          5090
          -
          5113
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isEmpty
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1059
        </td>
        <td>
          5115
          -
          5153
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.dropTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.dropTable(VerticaDistributedFilesystemWritePipe.this.config.tablename)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1062
        </td>
        <td>
          5159
          -
          5168
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1064
        </td>
        <td>
          5261
          -
          5326
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1067
        </td>
        <td>
          5328
          -
          5363
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableMergeKey, Nothing](com.vertica.spark.util.error.CreateExternalTableMergeKey.apply())
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1066
        </td>
        <td>
          5328
          -
          5363
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableMergeKey, Nothing](com.vertica.spark.util.error.CreateExternalTableMergeKey.apply())
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1069
        </td>
        <td>
          5369
          -
          5378
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1063
        </td>
        <td>
          5301
          -
          5326
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1152
        </td>
        <td>
          5252
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableMergeKey, Nothing](com.vertica.spark.util.error.CreateExternalTableMergeKey.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPre: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))))))))))
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1065
        </td>
        <td>
          5333
          -
          5362
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.CreateExternalTableMergeKey.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CreateExternalTableMergeKey.apply()
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1068
        </td>
        <td>
          5369
          -
          5378
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1070
        </td>
        <td>
          5473
          -
          5489
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1151
        </td>
        <td>
          5432
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPre: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))))))))
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1073
        </td>
        <td>
          5604
          -
          5626
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1076
        </td>
        <td>
          5632
          -
          5641
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1075
        </td>
        <td>
          5632
          -
          5641
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1150
        </td>
        <td>
          5530
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))))))
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1072
        </td>
        <td>
          5609
          -
          5625
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.DropTableError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.DropTableError.apply()
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1074
        </td>
        <td>
          5604
          -
          5626
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.DropTableError, Nothing](com.vertica.spark.util.error.DropTableError.apply())
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          1071
        </td>
        <td>
          5539
          -
          5602
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.isOverwrite.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isEmpty).&amp;&amp;(tableExistsPre)
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1079
        </td>
        <td>
          5772
          -
          5817
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1082
        </td>
        <td>
          5823
          -
          5830
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1078
        </td>
        <td>
          5777
          -
          5816
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply()
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1081
        </td>
        <td>
          5823
          -
          5830
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1149
        </td>
        <td>
          5707
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))))
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1080
        </td>
        <td>
          5772
          -
          5817
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError, Nothing](com.vertica.spark.util.error.CreateExternalTableAlreadyExistsError.apply())
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1077
        </td>
        <td>
          5716
          -
          5770
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined.&amp;&amp;(tableExistsPre)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1083
        </td>
        <td>
          5940
          -
          5956
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1148
        </td>
        <td>
          5904
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.viewExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((viewExists: Boolean) =&gt; if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))))
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1088
        </td>
        <td>
          6014
          -
          6023
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1085
        </td>
        <td>
          5985
          -
          6008
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1147
        </td>
        <td>
          5964
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (viewExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))))
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1084
        </td>
        <td>
          5990
          -
          6007
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ViewExistsError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ViewExistsError.apply()
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1087
        </td>
        <td>
          6014
          -
          6023
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1086
        </td>
        <td>
          5985
          -
          6008
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ViewExistsError, Nothing](com.vertica.spark.util.error.ViewExistsError.apply())
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1089
        </td>
        <td>
          6076
          -
          6092
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1146
        </td>
        <td>
          6030
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))))
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1091
        </td>
        <td>
          6126
          -
          6154
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1145
        </td>
        <td>
          6100
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (tempTableExists)
  scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))))
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1094
        </td>
        <td>
          6160
          -
          6169
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1093
        </td>
        <td>
          6160
          -
          6169
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1090
        </td>
        <td>
          6131
          -
          6153
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.TempTableExistsError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.TempTableExistsError.apply()
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1092
        </td>
        <td>
          6126
          -
          6154
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.TempTableExistsError, Nothing](com.vertica.spark.util.error.TempTableExistsError.apply())
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1097
        </td>
        <td>
          6368
          -
          6384
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1100
        </td>
        <td>
          6424
          -
          6437
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.strlen
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.strlen
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1144
        </td>
        <td>
          6281
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty))
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))))
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1102
        </td>
        <td>
          6345
          -
          6438
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1096
        </td>
        <td>
          6290
          -
          6343
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          tableExistsPre.unary_!.&amp;&amp;(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1099
        </td>
        <td>
          6409
          -
          6422
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1098
        </td>
        <td>
          6386
          -
          6407
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.targetTableSql
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.targetTableSql
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1101
        </td>
        <td>
          6345
          -
          6438
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1095
        </td>
        <td>
          6309
          -
          6343
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isEmpty
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isEmpty
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1104
        </td>
        <td>
          6444
          -
          6453
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1103
        </td>
        <td>
          6444
          -
          6453
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1105
        </td>
        <td>
          6615
          -
          6631
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1143
        </td>
        <td>
          6573
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.tableExists(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tableExistsPost: Boolean) =&gt; if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))))
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1142
        </td>
        <td>
          6639
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined))
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None)).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)](((x$3: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemWritePipe.this.config.filePermissions;
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)) =&gt; (x$4: (Unit, com.vertica.spark.config.ValidFilePermissions, Boolean) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Boolean)(Unit, com.vertica.spark.config.ValidFilePermissions, Boolean)(_, (perm @ _), (existingData @ _)) =&gt; if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
}))
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1106
        </td>
        <td>
          6667
          -
          6703
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1109
        </td>
        <td>
          6705
          -
          6714
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1111
        </td>
        <td>
          6725
          -
          6747
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.CreateTableError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CreateTableError.apply(scala.None)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1108
        </td>
        <td>
          6705
          -
          6714
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1107
        </td>
        <td>
          6648
          -
          6703
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td style="background: #AEF1AE">
          tableExistsPost.||(VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1110
        </td>
        <td>
          6742
          -
          6746
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1119
        </td>
        <td>
          6639
          -
          6639
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Boolean](x$3, perm, existingData)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1113
        </td>
        <td>
          6720
          -
          6748
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1112
        </td>
        <td>
          6720
          -
          6748
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          1114
        </td>
        <td>
          6812
          -
          6834
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.filePermissions
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.filePermissions
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          1115
        </td>
        <td>
          6856
          -
          6882
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.createExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          1116
        </td>
        <td>
          6976
          -
          6980
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          true
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          1117
        </td>
        <td>
          7009
          -
          7014
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          1118
        </td>
        <td>
          7048
          -
          7053
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1124
        </td>
        <td>
          7105
          -
          7158
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.createDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString())
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1120
        </td>
        <td>
          7090
          -
          7099
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1123
        </td>
        <td>
          7144
          -
          7157
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidFilePermissions.toString
        </td>
        <td style="background: #AEF1AE">
          perm.toString()
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1141
        </td>
        <td>
          7068
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (existingData)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1125
        </td>
        <td>
          7105
          -
          7158
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.createDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.createDir(VerticaDistributedFilesystemWritePipe.this.getAddress(), perm.toString())
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1122
        </td>
        <td>
          7130
          -
          7142
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.getAddress
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.getAddress()
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          1121
        </td>
        <td>
          7090
          -
          7099
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          154
        </td>
        <td>
          1126
        </td>
        <td>
          7215
          -
          7240
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.saveJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable
        </td>
      </tr><tr>
        <td>
          154
        </td>
        <td>
          1140
        </td>
        <td>
          7207
          -
          7461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
    &quot;OVERWRITE&quot;
  else
    &quot;APPEND&quot;)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1133
        </td>
        <td>
          7396
          -
          7404
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;APPEND&quot;
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1127
        </td>
        <td>
          7291
          -
          7307
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1136
        </td>
        <td>
          7252
          -
          7405
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createAndInitJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
  &quot;OVERWRITE&quot;
else
  &quot;APPEND&quot;)
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1135
        </td>
        <td>
          7252
          -
          7405
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createAndInitJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createAndInitJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, VerticaDistributedFilesystemWritePipe.this.config.sessionId, if (VerticaDistributedFilesystemWritePipe.this.config.isOverwrite)
  &quot;OVERWRITE&quot;
else
  &quot;APPEND&quot;)
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1129
        </td>
        <td>
          7338
          -
          7354
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.sessionId
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.sessionId
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1132
        </td>
        <td>
          7379
          -
          7390
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;OVERWRITE&quot;
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1134
        </td>
        <td>
          7396
          -
          7404
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;APPEND&quot;
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1128
        </td>
        <td>
          7309
          -
          7336
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.JdbcAuth.user
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1131
        </td>
        <td>
          7379
          -
          7390
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;OVERWRITE&quot;
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1130
        </td>
        <td>
          7359
          -
          7377
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.isOverwrite
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.isOverwrite
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          1138
        </td>
        <td>
          7429
          -
          7438
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          1137
        </td>
        <td>
          7429
          -
          7438
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          1139
        </td>
        <td>
          7459
          -
          7461
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          1156
        </td>
        <td>
          7514
          -
          7520
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.logger
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          1158
        </td>
        <td>
          7481
          -
          7543
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemWritePipe.this.config.timeOperations, VerticaDistributedFilesystemWritePipe.this.logger, &quot;Writing Partition.&quot;)
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          1155
        </td>
        <td>
          7491
          -
          7512
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          1157
        </td>
        <td>
          7522
          -
          7542
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Writing Partition.&quot;
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          1159
        </td>
        <td>
          7634
          -
          7646
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.getAddress
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.getAddress()
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1160
        </td>
        <td>
          7688
          -
          7689
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1162
        </td>
        <td>
          7701
          -
          7729
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(address).takeRight(1).==(&quot;\\&quot;)
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1165
        </td>
        <td>
          7731
          -
          7733
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1167
        </td>
        <td>
          7739
          -
          7742
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1161
        </td>
        <td>
          7694
          -
          7697
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1164
        </td>
        <td>
          7731
          -
          7733
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1163
        </td>
        <td>
          7670
          -
          7729
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(address).takeRight(1).==(&quot;/&quot;).||(scala.Predef.augmentString(address).takeRight(1).==(&quot;\\&quot;))
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1166
        </td>
        <td>
          7739
          -
          7742
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          1168
        </td>
        <td>
          7762
          -
          7812
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          address.+(delimiter).+(uniqueId).+(&quot;.snappy.parquet&quot;)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          1169
        </td>
        <td>
          7818
          -
          7835
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.timer.startTime()
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          1170
        </td>
        <td>
          7841
          -
          7886
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.openWriteParquetFile
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.openWriteParquetFile(filename)
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          1171
        </td>
        <td>
          7930
          -
          7968
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          1174
        </td>
        <td>
          7927
          -
          7927
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          1173
        </td>
        <td>
          7970
          -
          8092
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Cleaning up all files in path: &quot;.+(address))
  else
    (): Unit);
  VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.removeDir(address)
}
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          1175
        </td>
        <td>
          7927
          -
          7927
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1172
        </td>
        <td>
          8049
          -
          8082
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.removeDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.removeDir(address)
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1176
        </td>
        <td>
          8101
          -
          8110
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          1177
        </td>
        <td>
          8135
          -
          8142
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          1178
        </td>
        <td>
          8218
          -
          8244
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.createExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          1180
        </td>
        <td>
          8332
          -
          8362
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.NonEmptyDataFrameError, Nothing](com.vertica.spark.util.error.NonEmptyDataFrameError.apply())
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          1179
        </td>
        <td>
          8337
          -
          8361
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.NonEmptyDataFrameError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.NonEmptyDataFrameError.apply()
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          1181
        </td>
        <td>
          8389
          -
          8432
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.writeDataToParquetFile
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.writeDataToParquetFile(data)
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          1182
        </td>
        <td>
          8463
          -
          8506
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.writeDataToParquetFile
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.writeDataToParquetFile(data)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          1183
        </td>
        <td>
          8575
          -
          8590
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.timer.endTime()
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          1184
        </td>
        <td>
          8595
          -
          8621
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.createExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          1185
        </td>
        <td>
          8709
          -
          8718
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          1186
        </td>
        <td>
          8745
          -
          8783
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.closeWriteParquetFile
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.closeWriteParquetFile()
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          1187
        </td>
        <td>
          8814
          -
          8852
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.closeWriteParquetFile
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.closeWriteParquetFile()
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          1188
        </td>
        <td>
          9009
          -
          9034
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          1189
        </td>
        <td>
          9044
          -
          9151
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;COPY &quot;, &quot; FROM \'&quot;, &quot;\' ON ANY NODE &quot;, &quot; REJECTED DATA AS TABLE &quot;, &quot; NO COMMIT&quot;).s(targetTable, url, fileFormat, rejectsTableName)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          1190
        </td>
        <td>
          9044
          -
          9151
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;COPY &quot;, &quot; FROM \'&quot;, &quot;\' ON ANY NODE &quot;, &quot; REJECTED DATA AS TABLE &quot;, &quot; NO COMMIT&quot;).s(targetTable, url, fileFormat, rejectsTableName)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1192
        </td>
        <td>
          9175
          -
          9294
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;COPY &quot;, &quot; &quot;, &quot; FROM \'&quot;, &quot;\' ON ANY NODE &quot;, &quot; REJECTED DATA AS TABLE &quot;, &quot; NO COMMIT&quot;).s(targetTable, columnList, url, fileFormat, rejectsTableName)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1191
        </td>
        <td>
          9175
          -
          9294
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;COPY &quot;, &quot; &quot;, &quot; FROM \'&quot;, &quot;\' ON ANY NODE &quot;, &quot; REJECTED DATA AS TABLE &quot;, &quot; NO COMMIT&quot;).s(targetTable, columnList, url, fileFormat, rejectsTableName)
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          1193
        </td>
        <td>
          9433
          -
          9465
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.getFullTableName
        </td>
        <td style="background: #AEF1AE">
          targetTableName.getFullTableName
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          1196
        </td>
        <td>
          9568
          -
          9589
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.copyColumnList
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.copyColumnList
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          1195
        </td>
        <td>
          9553
          -
          9566
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.tempTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          1194
        </td>
        <td>
          9525
          -
          9534
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          1197
        </td>
        <td>
          9492
          -
          9590
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.getMergeUpdateValues
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.schemaTools.getMergeUpdateValues(VerticaDistributedFilesystemWritePipe.this.jdbcLayer, targetTableName, VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.copyColumnList)
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          1198
        </td>
        <td>
          9663
          -
          9688
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MergeColumnListError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MergeColumnListError.apply(err)
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          1199
        </td>
        <td>
          9658
          -
          9689
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.MergeColumnListError, Nothing](com.vertica.spark.util.error.MergeColumnListError.apply(err))
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1201
        </td>
        <td>
          9766
          -
          9779
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.tempTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1200
        </td>
        <td>
          9755
          -
          9764
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1203
        </td>
        <td>
          9722
          -
          9803
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.getMergeInsertValues
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.schemaTools.getMergeInsertValues(VerticaDistributedFilesystemWritePipe.this.jdbcLayer, VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.copyColumnList)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1202
        </td>
        <td>
          9781
          -
          9802
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.copyColumnList
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.copyColumnList
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          1205
        </td>
        <td>
          9871
          -
          9902
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.MergeColumnListError, Nothing](com.vertica.spark.util.error.MergeColumnListError.apply(err))
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          1204
        </td>
        <td>
          9876
          -
          9901
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MergeColumnListError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MergeColumnListError.apply(err)
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          1206
        </td>
        <td>
          9929
          -
          9944
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.mergeKey
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          1207
        </td>
        <td>
          10003
          -
          10026
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.split
        </td>
        <td style="background: #AEF1AE">
          key.toString().split(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          1210
        </td>
        <td>
          10003
          -
          10056
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[String](key.toString().split(&quot;,&quot;)).toList.map[String, List[String]](((col: String) =&gt; col.trim()))(immutable.this.List.canBuildFrom[String])
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          1209
        </td>
        <td>
          10037
          -
          10037
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[String]
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          1208
        </td>
        <td>
          10045
          -
          10055
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.trim
        </td>
        <td style="background: #AEF1AE">
          col.trim()
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          1211
        </td>
        <td>
          10065
          -
          10152
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td style="background: #AEF1AE">
          trimmedCols.map[String, List[String]](((trimmedCol: String) =&gt; scala.StringContext.apply(&quot;target.&quot;, &quot;=temp.&quot;, &quot;&quot;).s(trimmedCol, trimmedCol)))(immutable.this.List.canBuildFrom[String]).mkString(&quot; AND &quot;)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          1212
        </td>
        <td>
          10173
          -
          10179
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td style="background: #F0ADAD">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          1213
        </td>
        <td>
          10190
          -
          10380
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;MERGE INTO &quot;, &quot; as target using &quot;, &quot; as temp ON (&quot;, &quot;) WHEN MATCHED THEN UPDATE SET &quot;, &quot; WHEN NOT MATCHED THEN INSERT &quot;, &quot; VALUES (&quot;, &quot;)&quot;).s(targetTable, tempTable, mergeList, updateColValues, columnList, insertColValues)
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1214
        </td>
        <td>
          10557
          -
          10584
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;EXPLAIN &quot;.+(mergeStatement)
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1221
        </td>
        <td>
          10471
          -
          10687
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(&quot;EXPLAIN &quot;.+(mergeStatement), VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$6: Unit = rs.close();
  scala.Tuple2.apply[java.sql.ResultSet, Unit](rs, x$6)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$7: (java.sql.ResultSet, Unit)) =&gt; (x$7: (java.sql.ResultSet, Unit) @unchecked) match {
  case (_1: java.sql.ResultSet, _2: Unit)(java.sql.ResultSet, Unit)((rs @ _), _) =&gt; VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(mergeStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2).map[Unit](((_: Unit) =&gt; ()))
}))
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1215
        </td>
        <td>
          10551
          -
          10551
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1217
        </td>
        <td>
          10535
          -
          10535
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[java.sql.ResultSet, Unit](rs, x$6)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          1216
        </td>
        <td>
          10596
          -
          10606
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          1218
        </td>
        <td>
          10649
          -
          10649
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.execute$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          1220
        </td>
        <td>
          10634
          -
          10687
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(mergeStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2).map[Unit](((_: Unit) =&gt; ()))
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          1219
        </td>
        <td>
          10685
          -
          10687
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          1222
        </td>
        <td>
          10747
          -
          10821
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CommitError.apply(err).context(&quot;performMerge: JDBC error when trying to merge&quot;)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          1223
        </td>
        <td>
          10727
          -
          10822
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          ret.left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;performMerge: JDBC error when trying to merge&quot;)))
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          1224
        </td>
        <td>
          10911
          -
          10964
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replaceAll
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName.replaceAll(&quot;\&quot;&quot;, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1231
        </td>
        <td>
          10995
          -
          11124
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.getGlobStatus
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.getGlobStatus(com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2))
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1225
        </td>
        <td>
          11048
          -
          11049
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1228
        </td>
        <td>
          11046
          -
          11122
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;))
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1227
        </td>
        <td>
          11050
          -
          11110
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1230
        </td>
        <td>
          11024
          -
          11123
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1226
        </td>
        <td>
          11111
          -
          11122
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/*.parquet&quot;
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1229
        </td>
        <td>
          11036
          -
          11036
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          1232
        </td>
        <td>
          11198
          -
          11211
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td style="background: #AEF1AE">
          list.nonEmpty
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1237
        </td>
        <td>
          11239
          -
          11239
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1234
        </td>
        <td>
          11314
          -
          11325
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/*.parquet&quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1236
        </td>
        <td>
          11249
          -
          11325
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;))
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1239
        </td>
        <td>
          11227
          -
          11326
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1233
        </td>
        <td>
          11251
          -
          11252
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1235
        </td>
        <td>
          11253
          -
          11313
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1238
        </td>
        <td>
          11227
          -
          11326
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1246
        </td>
        <td>
          11368
          -
          11470
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1240
        </td>
        <td>
          11392
          -
          11393
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1243
        </td>
        <td>
          11390
          -
          11469
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;))
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1245
        </td>
        <td>
          11368
          -
          11470
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1242
        </td>
        <td>
          11394
          -
          11454
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1241
        </td>
        <td>
          11455
          -
          11469
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;/**/*.parquet&quot;
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          1244
        </td>
        <td>
          11380
          -
          11380
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
      </tr><tr>
        <td>
          263
        </td>
        <td>
          1247
        </td>
        <td>
          11491
          -
          11568
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;SELECT INFER_EXTERNAL_TABLE_DDL(\'&quot;.+(url).+(&quot;\',\'&quot;).+(tableName).+(&quot;\')&quot;)
        </td>
      </tr><tr>
        <td>
          265
        </td>
        <td>
          1248
        </td>
        <td>
          11594
          -
          11612
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.getFullContext
        </td>
        <td style="background: #F0ADAD">
          err.getFullContext
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1249
        </td>
        <td>
          11696
          -
          11696
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1250
        </td>
        <td>
          11686
          -
          11717
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(inferStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2)
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          1252
        </td>
        <td>
          11750
          -
          11790
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.InferExternalTableSchemaError, Nothing](com.vertica.spark.util.error.InferExternalTableSchemaError.apply(err))
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          1251
        </td>
        <td>
          11755
          -
          11789
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InferExternalTableSchemaError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InferExternalTableSchemaError.apply(err)
        </td>
      </tr><tr>
        <td>
          272
        </td>
        <td>
          1275
        </td>
        <td>
          11846
          -
          12787
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val iterate: Boolean = resultSet.next();
  val createExternalTableStatement: String = resultSet.getString(&quot;INFER_EXTERNAL_TABLE_DDL&quot;);
  val isPartitioned: Boolean = inferStatement.contains(com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2));
  if (isPartitioned.unary_!.&amp;&amp;(createExternalTableStatement.contains(&quot;varchar&quot;).unary_!).&amp;&amp;(createExternalTableStatement.contains(&quot;varbinary&quot;).unary_!))
    {
      (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Inferring schema from parquet data&quot;)
      else
        (): Unit);
      val updatedStatement: String = createExternalTableStatement.replace(&quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;), tableName);
      (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isDebugEnabled())
        VerticaDistributedFilesystemWritePipe.this.logger.underlying.debug(&quot;The create external table statement is: &quot;.+(updatedStatement))
      else
        (): Unit);
      scala.`package`.Right.apply[Nothing, String](updatedStatement)
    }
  else
    {
      (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Inferring partial schema from dataframe&quot;)
      else
        (): Unit);
      VerticaDistributedFilesystemWritePipe.this.schemaTools.inferExternalTableSchema(createExternalTableStatement, VerticaDistributedFilesystemWritePipe.this.config.schema, tableName, VerticaDistributedFilesystemWritePipe.this.config.strlen)
    }
}
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          1253
        </td>
        <td>
          11860
          -
          11874
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.next
        </td>
        <td style="background: #AEF1AE">
          resultSet.next()
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          1254
        </td>
        <td>
          11920
          -
          11967
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.getString
        </td>
        <td style="background: #AEF1AE">
          resultSet.getString(&quot;INFER_EXTERNAL_TABLE_DDL&quot;)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1255
        </td>
        <td>
          12046
          -
          12047
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1258
        </td>
        <td>
          12044
          -
          12123
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;))
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1261
        </td>
        <td>
          11998
          -
          12125
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.contains
        </td>
        <td style="background: #AEF1AE">
          inferStatement.contains(com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2))
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1260
        </td>
        <td>
          12022
          -
          12124
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/**/*.parquet&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1257
        </td>
        <td>
          12048
          -
          12108
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.externalTableAddress).stripSuffix(&quot;/&quot;)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1259
        </td>
        <td>
          12034
          -
          12034
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1256
        </td>
        <td>
          12109
          -
          12123
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/**/*.parquet&quot;
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1264
        </td>
        <td>
          12250
          -
          12261
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;varbinary&quot;
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1270
        </td>
        <td>
          12264
          -
          12571
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Inferring schema from parquet data&quot;)
  else
    (): Unit);
  val updatedStatement: String = createExternalTableStatement.replace(&quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;), tableName);
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isDebugEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.debug(&quot;The create external table statement is: &quot;.+(updatedStatement))
  else
    (): Unit);
  scala.`package`.Right.apply[Nothing, String](updatedStatement)
}
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1263
        </td>
        <td>
          12158
          -
          12207
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          createExternalTableStatement.contains(&quot;varchar&quot;).unary_!
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1266
        </td>
        <td>
          12140
          -
          12262
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          isPartitioned.unary_!.&amp;&amp;(createExternalTableStatement.contains(&quot;varchar&quot;).unary_!).&amp;&amp;(createExternalTableStatement.contains(&quot;varbinary&quot;).unary_!)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1262
        </td>
        <td>
          12197
          -
          12206
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;varchar&quot;
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1265
        </td>
        <td>
          12211
          -
          12262
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          createExternalTableStatement.contains(&quot;varbinary&quot;).unary_!
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1267
        </td>
        <td>
          12400
          -
          12423
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1268
        </td>
        <td>
          12363
          -
          12435
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replace
        </td>
        <td style="background: #AEF1AE">
          createExternalTableStatement.replace(&quot;\&quot;&quot;.+(tableName).+(&quot;\&quot;&quot;), tableName)
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          1269
        </td>
        <td>
          12536
          -
          12559
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](updatedStatement)
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          1274
        </td>
        <td>
          12587
          -
          12787
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          {
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Inferring partial schema from dataframe&quot;)
  else
    (): Unit);
  VerticaDistributedFilesystemWritePipe.this.schemaTools.inferExternalTableSchema(createExternalTableStatement, VerticaDistributedFilesystemWritePipe.this.config.schema, tableName, VerticaDistributedFilesystemWritePipe.this.config.strlen)
}
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          1273
        </td>
        <td>
          12668
          -
          12775
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.inferExternalTableSchema
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.schemaTools.inferExternalTableSchema(createExternalTableStatement, VerticaDistributedFilesystemWritePipe.this.config.schema, tableName, VerticaDistributedFilesystemWritePipe.this.config.strlen)
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          1272
        </td>
        <td>
          12761
          -
          12774
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.strlen
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.config.strlen
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          1271
        </td>
        <td>
          12735
          -
          12748
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          1276
        </td>
        <td>
          12862
          -
          12889
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InferExternalSchemaError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InferExternalSchemaError.apply(e)
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          1277
        </td>
        <td>
          12857
          -
          12890
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.InferExternalSchemaError, Nothing](com.vertica.spark.util.error.InferExternalSchemaError.apply(e))
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          1279
        </td>
        <td>
          12929
          -
          12946
        </td>
        <td>
          Block
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          resultSet.close()
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          1278
        </td>
        <td>
          12929
          -
          12946
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          resultSet.close()
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          1280
        </td>
        <td>
          13297
          -
          13318
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.copyColumnList
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.copyColumnList
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          1282
        </td>
        <td>
          13524
          -
          13604
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;(&quot;.+(scala.Predef.refArrayOps[String](scala.Predef.refArrayOps[String](list.toString().split(&quot;,&quot;)).map[String, Array[String]](((col: String) =&gt; col.trim()))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))).mkString(&quot;,&quot;)).+(&quot;)&quot;))
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          1281
        </td>
        <td>
          13530
          -
          13603
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;(&quot;.+(scala.Predef.refArrayOps[String](scala.Predef.refArrayOps[String](list.toString().split(&quot;,&quot;)).map[String, Array[String]](((col: String) =&gt; col.trim()))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))).mkString(&quot;,&quot;)).+(&quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          1285
        </td>
        <td>
          13773
          -
          13786
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          1284
        </td>
        <td>
          13755
          -
          13771
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          1283
        </td>
        <td>
          13744
          -
          13753
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          315
        </td>
        <td>
          1287
        </td>
        <td>
          13714
          -
          13919
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.schemaTools.getCopyColumnList(VerticaDistributedFilesystemWritePipe.this.jdbcLayer, VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.schema).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.SchemaColumnListError.apply(err).context(&quot;getColumnList: Error building default copy column list&quot;)))
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          1286
        </td>
        <td>
          13815
          -
          13918
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.SchemaColumnListError.apply(err).context(&quot;getColumnList: Error building default copy column list&quot;)
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          1288
        </td>
        <td>
          14232
          -
          14279
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;SELECT COUNT(*) as count FROM &quot;.+(rejectsTable)
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          1297
        </td>
        <td>
          14381
          -
          14381
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Int], Unit](rs, res, x$11)
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          1341
        </td>
        <td>
          14369
          -
          16975
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Int], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val res: scala.util.Try[Int] = scala.util.Try.apply[Int](if (rs.next())
    rs.getInt(&quot;count&quot;)
  else
    {
      (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
        VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Could not retrieve rejected row count.&quot;)
      else
        (): Unit);
      0
    });
  val x$11: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Int], Unit](rs, res, x$11)
})).flatMap[com.vertica.spark.util.error.ConnectorError, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$17: (java.sql.ResultSet, scala.util.Try[Int], Unit)) =&gt; (x$17: (java.sql.ResultSet, scala.util.Try[Int], Unit) @unchecked) match {
  case (_1: java.sql.ResultSet, _2: scala.util.Try[Int], _3: Unit)(java.sql.ResultSet, scala.util.Try[Int], Unit)((rs @ _), (res @ _), _) =&gt; com.vertica.spark.datasource.jdbc.JdbcUtils.tryJdbcToResult[Int](VerticaDistributedFilesystemWritePipe.this.jdbcLayer, res).map[(Int, Double, Boolean, String, Unit, Unit)](((rejectedCount: Int) =&gt; {
  val failedRowsPercent: Double = if (rowsCopied.&gt;(0))
    rejectedCount.toDouble./(rowsCopied.toDouble.+(rejectedCount.toDouble))
  else
    1.0;
  val passedFaultToleranceTest: Boolean = failedRowsPercent.&lt;=(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance.toDouble);
  val tolerance_message: String = &quot;Number of rows_rejected=&quot;.+(rejectedCount).+(&quot;. rows_copied=&quot;).+(rowsCopied).+(&quot;. failedRowsPercent=&quot;).+(failedRowsPercent).+(&quot;. user\'s failed_rows_percent_tolerance=&quot;).+(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance).+(&quot;. passedFaultToleranceTest=&quot;).+(passedFaultToleranceTest.toString());
  val x$12: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Verifying rows saved to Vertica is within user tolerance...&quot;)
  else
    (): Unit);
  val x$13: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(tolerance_message.+(if (passedFaultToleranceTest)
      &quot;...PASSED.  OK to commit to database.&quot;
    else
      &quot;...FAILED.  NOT OK to commit to database&quot;))
  else
    (): Unit);
  scala.Tuple6.apply[Int, Double, Boolean, String, Unit, Unit](rejectedCount, failedRowsPercent, passedFaultToleranceTest, tolerance_message, x$12, x$13)
})).flatMap[com.vertica.spark.util.error.ConnectorError, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$16: (Int, Double, Boolean, String, Unit, Unit)) =&gt; (x$16: (Int, Double, Boolean, String, Unit, Unit) @unchecked) match {
    case (_1: Int, _2: Double, _3: Boolean, _4: String, _5: Unit, _6: Unit)(Int, Double, Boolean, String, Unit, Unit)((rejectedCount @ _), (failedRowsPercent @ _), (passedFaultToleranceTest @ _), (tolerance_message @ _), _, _) =&gt; if (rejectedCount.==(0))
  {
    val dropRejectsTableStatement: String = &quot;DROP TABLE IF EXISTS &quot;.+(rejectsTable).+(&quot; CASCADE&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Dropping Vertica rejects table now: &quot;).s().+(dropRejectsTableStatement))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(dropRejectsTableStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2)
  }
else
  {
    val rejectsDataQuery: String = &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot;.+(rejectsTable).+(&quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Getting summary of rejected rows via statement: &quot;).s().+(rejectsDataQuery))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsDataQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Unit], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$8: scala.util.Try[Unit] = scala.util.Try.apply[Unit]({
    val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
    else
      (): Unit);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
    else
      (): Unit);
    while$1(){
      if (rs.next())
        {
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
            VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
          else
            (): Unit);
          while$1()
        }
      else
        ()
    }
  });
  val x$9: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
})).map[Unit](((x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit)) =&gt; (x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit) @unchecked) match {
      case (_1: java.sql.ResultSet, _2: scala.util.Try[Unit], _3: Unit)(java.sql.ResultSet, scala.util.Try[Unit], Unit)((rs @ _), _, _) =&gt; ()
    }));
    scala.`package`.Right.apply[Nothing, Unit](())
  }.map[(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)](((x$14: Unit) =&gt; {
  val testResult: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult = VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult.apply(passedFaultToleranceTest, failedRowsPercent);
  scala.Tuple2.apply[Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](x$14, testResult)
})).map[VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)) =&gt; (x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) @unchecked) match {
      case (_1: Unit, _2: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(_, (testResult @ _)) =&gt; testResult
    }))
  }))
}))
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          1289
        </td>
        <td>
          14397
          -
          14397
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          329
        </td>
        <td>
          1295
        </td>
        <td>
          14429
          -
          14606
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Int](if (rs.next())
  rs.getInt(&quot;count&quot;)
else
  {
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Could not retrieve rejected row count.&quot;)
    else
      (): Unit);
    0
  })
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          1290
        </td>
        <td>
          14447
          -
          14454
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.next
        </td>
        <td style="background: #AEF1AE">
          rs.next()
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          1291
        </td>
        <td>
          14468
          -
          14486
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.getInt
        </td>
        <td style="background: #AEF1AE">
          rs.getInt(&quot;count&quot;)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          1292
        </td>
        <td>
          14468
          -
          14486
        </td>
        <td>
          Block
        </td>
        <td>
          java.sql.ResultSet.getInt
        </td>
        <td style="background: #AEF1AE">
          rs.getInt(&quot;count&quot;)
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          1294
        </td>
        <td>
          14510
          -
          14598
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          {
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Could not retrieve rejected row count.&quot;)
  else
    (): Unit);
  0
}
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          1293
        </td>
        <td>
          14587
          -
          14588
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          1296
        </td>
        <td>
          14617
          -
          14627
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          1340
        </td>
        <td>
          14634
          -
          16975
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.datasource.jdbc.JdbcUtils.tryJdbcToResult[Int](VerticaDistributedFilesystemWritePipe.this.jdbcLayer, res).map[(Int, Double, Boolean, String, Unit, Unit)](((rejectedCount: Int) =&gt; {
  val failedRowsPercent: Double = if (rowsCopied.&gt;(0))
    rejectedCount.toDouble./(rowsCopied.toDouble.+(rejectedCount.toDouble))
  else
    1.0;
  val passedFaultToleranceTest: Boolean = failedRowsPercent.&lt;=(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance.toDouble);
  val tolerance_message: String = &quot;Number of rows_rejected=&quot;.+(rejectedCount).+(&quot;. rows_copied=&quot;).+(rowsCopied).+(&quot;. failedRowsPercent=&quot;).+(failedRowsPercent).+(&quot;. user\'s failed_rows_percent_tolerance=&quot;).+(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance).+(&quot;. passedFaultToleranceTest=&quot;).+(passedFaultToleranceTest.toString());
  val x$12: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;Verifying rows saved to Vertica is within user tolerance...&quot;)
  else
    (): Unit);
  val x$13: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(tolerance_message.+(if (passedFaultToleranceTest)
      &quot;...PASSED.  OK to commit to database.&quot;
    else
      &quot;...FAILED.  NOT OK to commit to database&quot;))
  else
    (): Unit);
  scala.Tuple6.apply[Int, Double, Boolean, String, Unit, Unit](rejectedCount, failedRowsPercent, passedFaultToleranceTest, tolerance_message, x$12, x$13)
})).flatMap[com.vertica.spark.util.error.ConnectorError, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$16: (Int, Double, Boolean, String, Unit, Unit)) =&gt; (x$16: (Int, Double, Boolean, String, Unit, Unit) @unchecked) match {
  case (_1: Int, _2: Double, _3: Boolean, _4: String, _5: Unit, _6: Unit)(Int, Double, Boolean, String, Unit, Unit)((rejectedCount @ _), (failedRowsPercent @ _), (passedFaultToleranceTest @ _), (tolerance_message @ _), _, _) =&gt; if (rejectedCount.==(0))
  {
    val dropRejectsTableStatement: String = &quot;DROP TABLE IF EXISTS &quot;.+(rejectsTable).+(&quot; CASCADE&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Dropping Vertica rejects table now: &quot;).s().+(dropRejectsTableStatement))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(dropRejectsTableStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2)
  }
else
  {
    val rejectsDataQuery: String = &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot;.+(rejectsTable).+(&quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Getting summary of rejected rows via statement: &quot;).s().+(rejectsDataQuery))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsDataQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Unit], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$8: scala.util.Try[Unit] = scala.util.Try.apply[Unit]({
    val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
    else
      (): Unit);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
    else
      (): Unit);
    while$1(){
      if (rs.next())
        {
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
            VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
          else
            (): Unit);
          while$1()
        }
      else
        ()
    }
  });
  val x$9: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
})).map[Unit](((x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit)) =&gt; (x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit) @unchecked) match {
      case (_1: java.sql.ResultSet, _2: scala.util.Try[Unit], _3: Unit)(java.sql.ResultSet, scala.util.Try[Unit], Unit)((rs @ _), _, _) =&gt; ()
    }));
    scala.`package`.Right.apply[Nothing, Unit](())
  }.map[(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)](((x$14: Unit) =&gt; {
  val testResult: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult = VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult.apply(passedFaultToleranceTest, failedRowsPercent);
  scala.Tuple2.apply[Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](x$14, testResult)
})).map[VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)) =&gt; (x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) @unchecked) match {
    case (_1: Unit, _2: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(_, (testResult @ _)) =&gt; testResult
  }))
}))
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          1316
        </td>
        <td>
          14634
          -
          14634
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple6.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple6.apply[Int, Double, Boolean, String, Unit, Unit](rejectedCount, failedRowsPercent, passedFaultToleranceTest, tolerance_message, x$12, x$13)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          1298
        </td>
        <td>
          14677
          -
          14686
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          1299
        </td>
        <td>
          14724
          -
          14738
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td style="background: #AEF1AE">
          rowsCopied.&gt;(0)
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          1300
        </td>
        <td>
          14798
          -
          14820
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toDouble
        </td>
        <td style="background: #AEF1AE">
          rejectedCount.toDouble
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          1303
        </td>
        <td>
          14750
          -
          14821
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Double./
        </td>
        <td style="background: #AEF1AE">
          rejectedCount.toDouble./(rowsCopied.toDouble.+(rejectedCount.toDouble))
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          1302
        </td>
        <td>
          14750
          -
          14821
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Double./
        </td>
        <td style="background: #AEF1AE">
          rejectedCount.toDouble./(rowsCopied.toDouble.+(rejectedCount.toDouble))
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          1301
        </td>
        <td>
          14776
          -
          14820
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Double.+
        </td>
        <td style="background: #AEF1AE">
          rowsCopied.toDouble.+(rejectedCount.toDouble)
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          1305
        </td>
        <td>
          14845
          -
          14848
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          1.0
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          1304
        </td>
        <td>
          14845
          -
          14848
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          1.0
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1306
        </td>
        <td>
          14913
          -
          14954
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Float.toDouble
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance.toDouble
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1307
        </td>
        <td>
          14892
          -
          14954
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Double.&lt;=
        </td>
        <td style="background: #AEF1AE">
          failedRowsPercent.&lt;=(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance.toDouble)
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          1308
        </td>
        <td>
          15046
          -
          15072
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Number of rows_rejected=&quot;
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          1309
        </td>
        <td>
          15101
          -
          15117
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;. rows_copied=&quot;
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          1310
        </td>
        <td>
          15143
          -
          15165
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;. failedRowsPercent=&quot;
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          1311
        </td>
        <td>
          15198
          -
          15239
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;. user\'s failed_rows_percent_tolerance=&quot;
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          1312
        </td>
        <td>
          15252
          -
          15284
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.failedRowPercentTolerance
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          1315
        </td>
        <td>
          15046
          -
          15362
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;Number of rows_rejected=&quot;.+(rejectedCount).+(&quot;. rows_copied=&quot;).+(rowsCopied).+(&quot;. failedRowsPercent=&quot;).+(failedRowsPercent).+(&quot;. user\'s failed_rows_percent_tolerance=&quot;).+(VerticaDistributedFilesystemWritePipe.this.config.failedRowPercentTolerance).+(&quot;. passedFaultToleranceTest=&quot;).+(passedFaultToleranceTest.toString())
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          1314
        </td>
        <td>
          15329
          -
          15362
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td style="background: #AEF1AE">
          passedFaultToleranceTest.toString()
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          1313
        </td>
        <td>
          15297
          -
          15326
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;. passedFaultToleranceTest=&quot;
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          1339
        </td>
        <td>
          15680
          -
          16975
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          if (rejectedCount.==(0))
  {
    val dropRejectsTableStatement: String = &quot;DROP TABLE IF EXISTS &quot;.+(rejectsTable).+(&quot; CASCADE&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Dropping Vertica rejects table now: &quot;).s().+(dropRejectsTableStatement))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(dropRejectsTableStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2)
  }
else
  {
    val rejectsDataQuery: String = &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot;.+(rejectsTable).+(&quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Getting summary of rejected rows via statement: &quot;).s().+(rejectsDataQuery))
    else
      (): Unit);
    VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsDataQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Unit], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$8: scala.util.Try[Unit] = scala.util.Try.apply[Unit]({
    val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
    else
      (): Unit);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
    else
      (): Unit);
    while$1(){
      if (rs.next())
        {
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
            VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
          else
            (): Unit);
          while$1()
        }
      else
        ()
    }
  });
  val x$9: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
})).map[Unit](((x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit)) =&gt; (x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit) @unchecked) match {
      case (_1: java.sql.ResultSet, _2: scala.util.Try[Unit], _3: Unit)(java.sql.ResultSet, scala.util.Try[Unit], Unit)((rs @ _), _, _) =&gt; ()
    }));
    scala.`package`.Right.apply[Nothing, Unit](())
  }.map[(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)](((x$14: Unit) =&gt; {
  val testResult: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult = VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult.apply(passedFaultToleranceTest, failedRowsPercent);
  scala.Tuple2.apply[Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](x$14, testResult)
})).map[VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](((x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)) =&gt; (x$15: (Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) @unchecked) match {
  case (_1: Unit, _2: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult)(_, (testResult @ _)) =&gt; testResult
}))
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          1321
        </td>
        <td>
          15709
          -
          15953
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val dropRejectsTableStatement: String = &quot;DROP TABLE IF EXISTS &quot;.+(rejectsTable).+(&quot; CASCADE&quot;);
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Dropping Vertica rejects table now: &quot;).s().+(dropRejectsTableStatement))
  else
    (): Unit);
  VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(dropRejectsTableStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2)
}
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          1338
        </td>
        <td>
          15680
          -
          15680
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Unit, VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult](x$14, testResult)
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          1317
        </td>
        <td>
          15689
          -
          15707
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          rejectedCount.==(0)
        </td>
      </tr><tr>
        <td>
          369
        </td>
        <td>
          1318
        </td>
        <td>
          15751
          -
          15803
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;DROP TABLE IF EXISTS &quot;.+(rejectsTable).+(&quot; CASCADE&quot;)
        </td>
      </tr><tr>
        <td>
          371
        </td>
        <td>
          1320
        </td>
        <td>
          15901
          -
          15945
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.execute
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute(dropRejectsTableStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2)
        </td>
      </tr><tr>
        <td>
          371
        </td>
        <td>
          1319
        </td>
        <td>
          15911
          -
          15911
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.execute$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.execute$default$2
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          1336
        </td>
        <td>
          15959
          -
          16862
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val rejectsDataQuery: String = &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot;.+(rejectsTable).+(&quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;);
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(scala.StringContext.apply(&quot;Getting summary of rejected rows via statement: &quot;).s().+(rejectsDataQuery))
  else
    (): Unit);
  VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsDataQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Unit], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$8: scala.util.Try[Unit] = scala.util.Try.apply[Unit]({
    val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
    else
      (): Unit);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
    else
      (): Unit);
    while$1(){
      if (rs.next())
        {
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
            VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
          else
            (): Unit);
          while$1()
        }
      else
        ()
    }
  });
  val x$9: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
})).map[Unit](((x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit)) =&gt; (x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit) @unchecked) match {
    case (_1: java.sql.ResultSet, _2: scala.util.Try[Unit], _3: Unit)(java.sql.ResultSet, scala.util.Try[Unit], Unit)((rs @ _), _, _) =&gt; ()
  }));
  scala.`package`.Right.apply[Nothing, Unit](())
}
        </td>
      </tr><tr>
        <td>
          374
        </td>
        <td>
          1322
        </td>
        <td>
          16035
          -
          16188
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;SELECT COUNT(*) count, MIN(rejected_data) example_data, rejected_reason FROM &quot;.+(rejectsTable).+(&quot; GROUP BY rejected_reason ORDER BY count DESC LIMIT 10&quot;)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          1323
        </td>
        <td>
          16321
          -
          16321
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          1332
        </td>
        <td>
          16305
          -
          16305
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          1334
        </td>
        <td>
          16289
          -
          16835
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(rejectsDataQuery, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, scala.util.Try[Unit], Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$8: scala.util.Try[Unit] = scala.util.Try.apply[Unit]({
    val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
    else
      (): Unit);
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
    else
      (): Unit);
    while$1(){
      if (rs.next())
        {
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
            VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
          else
            (): Unit);
          while$1()
        }
      else
        ()
    }
  });
  val x$9: Unit = rs.close();
  scala.Tuple3.apply[java.sql.ResultSet, scala.util.Try[Unit], Unit](rs, x$8, x$9)
})).map[Unit](((x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit)) =&gt; (x$10: (java.sql.ResultSet, scala.util.Try[Unit], Unit) @unchecked) match {
  case (_1: java.sql.ResultSet, _2: scala.util.Try[Unit], _3: Unit)(java.sql.ResultSet, scala.util.Try[Unit], Unit)((rs @ _), _, _) =&gt; ()
}))
        </td>
      </tr><tr>
        <td>
          378
        </td>
        <td>
          1330
        </td>
        <td>
          16359
          -
          16791
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Unit]({
  val rsmd: java.sql.ResultSetMetaData = rs.getMetaData();
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(&quot;Found {} rejected rows, displaying up to 10 of the most common reasons:&quot;, rejectedCount.asInstanceOf[AnyRef])
  else
    (): Unit);
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rsmd.getColumnName(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
  else
    (): Unit);
  while$1(){
    if (rs.next())
      {
        (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
          VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
        else
          (): Unit);
        while$1()
      }
    else
      ()
  }
})
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          1324
        </td>
        <td>
          16388
          -
          16402
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.getMetaData
        </td>
        <td style="background: #AEF1AE">
          rs.getMetaData()
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1327
        </td>
        <td>
          16672
          -
          16765
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          {
  (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isErrorEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.error(scala.Predef.intWrapper(1).to(rsmd.getColumnCount()).map[String, scala.collection.immutable.IndexedSeq[String]](((idx: Int) =&gt; rs.getString(idx)))(immutable.this.IndexedSeq.canBuildFrom[String]).toList.mkString(&quot; | &quot;))
  else
    (): Unit);
  while$1()
}
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1329
        </td>
        <td>
          16640
          -
          16640
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1328
        </td>
        <td>
          16640
          -
          16640
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1325
        </td>
        <td>
          16647
          -
          16654
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.next
        </td>
        <td style="background: #F0ADAD">
          rs.next()
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          1326
        </td>
        <td>
          16684
          -
          16684
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.while$1
        </td>
        <td style="background: #F0ADAD">
          while$1()
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          1331
        </td>
        <td>
          16806
          -
          16816
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          1333
        </td>
        <td>
          16833
          -
          16835
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          1335
        </td>
        <td>
          16845
          -
          16854
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          1337
        </td>
        <td>
          16883
          -
          16952
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.FaultToleranceTestResult.apply
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult.apply(passedFaultToleranceTest, failedRowsPercent)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          1342
        </td>
        <td>
          17293
          -
          17343
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;COPY &quot;.+(tablename.getFullTableName).+(&quot; FROM \'\';&quot;)
        </td>
      </tr><tr>
        <td>
          405
        </td>
        <td>
          1344
        </td>
        <td>
          17348
          -
          17382
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.executeUpdate
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate(emptyCopy, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate$default$2)
        </td>
      </tr><tr>
        <td>
          405
        </td>
        <td>
          1343
        </td>
        <td>
          17358
          -
          17358
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.executeUpdate$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate$default$2
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          1351
        </td>
        <td>
          17398
          -
          17633
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query(&quot;EXPLAIN &quot;.+(copyStatement), VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2).map[(java.sql.ResultSet, Unit)](((rs: java.sql.ResultSet) =&gt; {
  val x$18: Unit = rs.close();
  scala.Tuple2.apply[java.sql.ResultSet, Unit](rs, x$18)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Int](((x$19: (java.sql.ResultSet, Unit)) =&gt; (x$19: (java.sql.ResultSet, Unit) @unchecked) match {
  case (_1: java.sql.ResultSet, _2: Unit)(java.sql.ResultSet, Unit)((rs @ _), _) =&gt; VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate(copyStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate$default$2).map[Int](((rowsCopied: Int) =&gt; rowsCopied))
}))
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          1345
        </td>
        <td>
          17483
          -
          17509
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;EXPLAIN &quot;.+(copyStatement)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          1348
        </td>
        <td>
          17461
          -
          17461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[java.sql.ResultSet, Unit](rs, x$18)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          1346
        </td>
        <td>
          17477
          -
          17477
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.query$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.query$default$2
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          1347
        </td>
        <td>
          17521
          -
          17531
        </td>
        <td>
          Apply
        </td>
        <td>
          java.sql.ResultSet.close
        </td>
        <td style="background: #AEF1AE">
          rs.close()
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          1350
        </td>
        <td>
          17558
          -
          17633
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate(copyStatement, VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate$default$2).map[Int](((rowsCopied: Int) =&gt; rowsCopied))
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          1349
        </td>
        <td>
          17582
          -
          17582
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.executeUpdate$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.executeUpdate$default$2
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          1353
        </td>
        <td>
          17700
          -
          17792
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          ret.left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;performCopy: JDBC error when trying to copy&quot;)))
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          1352
        </td>
        <td>
          17720
          -
          17791
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.CommitError.apply(err).context(&quot;performCopy: JDBC error when trying to copy&quot;)
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          1354
        </td>
        <td>
          17895
          -
          17897
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          30
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          1357
        </td>
        <td>
          17955
          -
          17990
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Copy and commit data into Vertica&quot;
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          1356
        </td>
        <td>
          17947
          -
          17953
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.logger
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          1355
        </td>
        <td>
          17924
          -
          17945
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          1358
        </td>
        <td>
          17914
          -
          17991
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemWritePipe.this.config.timeOperations, VerticaDistributedFilesystemWritePipe.this.logger, &quot;Copy and commit data into Vertica&quot;)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          1359
        </td>
        <td>
          17996
          -
          18013
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          timer.startTime()
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          1360
        </td>
        <td>
          18128
          -
          18142
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          1449
        </td>
        <td>
          18028
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.configureSession(VerticaDistributedFilesystemWritePipe.this.fileStoreLayer).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.getColumnList.left.map[com.vertica.spark.util.error.ConnectorError](((x$20: com.vertica.spark.util.error.ConnectorError) =&gt; x$20.context(&quot;commit: Failed to get column list&quot;))).map[(String, String, String)](((columnList: String) =&gt; {
  val tableName: String = VerticaDistributedFilesystemWritePipe.this.config.tablename.name;
  val sessionId: String = VerticaDistributedFilesystemWritePipe.this.config.sessionId;
  scala.Tuple3.apply[String, String, String](columnList, tableName, sessionId)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$29: (String, String, String)) =&gt; (x$29: (String, String, String) @unchecked) match {
  case (_1: String, _2: String, _3: String)(String, String, String)((columnList @ _), (tableName @ _), (sessionId @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTempTable(VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.tempTableName).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[(Unit, String)](((x$23: Unit) =&gt; {
  val rejectsTableName: String = &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;);
  scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$28: (Unit, String)) =&gt; (x$28: (Unit, String) @unchecked) match {
    case (_1: Unit, _2: String)(Unit, String)(_, (rejectsTableName @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
      case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
        case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
      }))))))))))
    }))
  }))))))
}))))
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          1448
        </td>
        <td>
          18175
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.getColumnList.left.map[com.vertica.spark.util.error.ConnectorError](((x$20: com.vertica.spark.util.error.ConnectorError) =&gt; x$20.context(&quot;commit: Failed to get column list&quot;))).map[(String, String, String)](((columnList: String) =&gt; {
  val tableName: String = VerticaDistributedFilesystemWritePipe.this.config.tablename.name;
  val sessionId: String = VerticaDistributedFilesystemWritePipe.this.config.sessionId;
  scala.Tuple3.apply[String, String, String](columnList, tableName, sessionId)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$29: (String, String, String)) =&gt; (x$29: (String, String, String) @unchecked) match {
  case (_1: String, _2: String, _3: String)(String, String, String)((columnList @ _), (tableName @ _), (sessionId @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTempTable(VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.tempTableName).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[(Unit, String)](((x$23: Unit) =&gt; {
  val rejectsTableName: String = &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;);
  scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$28: (Unit, String)) =&gt; (x$28: (Unit, String) @unchecked) match {
    case (_1: Unit, _2: String)(Unit, String)(_, (rejectsTableName @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
      case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
        case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
      }))))))))))
    }))
  }))))))
}))
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          1361
        </td>
        <td>
          18212
          -
          18258
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          x$20.context(&quot;commit: Failed to get column list&quot;)
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          1364
        </td>
        <td>
          18175
          -
          18175
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[String, String, String](columnList, tableName, sessionId)
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          1362
        </td>
        <td>
          18355
          -
          18376
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.name
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename.name
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          1363
        </td>
        <td>
          18395
          -
          18411
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.sessionId
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.sessionId
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1366
        </td>
        <td>
          18482
          -
          18495
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.tempTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1447
        </td>
        <td>
          18419
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.createTempTable(VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.tempTableName).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[(Unit, String)](((x$23: Unit) =&gt; {
  val rejectsTableName: String = &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;);
  scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$28: (Unit, String)) =&gt; (x$28: (Unit, String) @unchecked) match {
  case (_1: Unit, _2: String)(Unit, String)(_, (rejectsTableName @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
    case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
      case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
    }))))))))))
  }))
}))))))
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1369
        </td>
        <td>
          18455
          -
          18526
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createTempTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createTempTable(VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1372
        </td>
        <td>
          18532
          -
          18541
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1365
        </td>
        <td>
          18428
          -
          18453
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1368
        </td>
        <td>
          18512
          -
          18525
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.strlen
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.strlen
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1371
        </td>
        <td>
          18532
          -
          18541
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1370
        </td>
        <td>
          18455
          -
          18526
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.createTempTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createTempTable(VerticaDistributedFilesystemWritePipe.this.tempTableName, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          1367
        </td>
        <td>
          18497
          -
          18510
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          1446
        </td>
        <td>
          18549
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.tempTableExists(VerticaDistributedFilesystemWritePipe.this.tempTableName).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((tempTableExists: Boolean) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[(Unit, String)](((x$23: Unit) =&gt; {
  val rejectsTableName: String = &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;);
  scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$28: (Unit, String)) =&gt; (x$28: (Unit, String) @unchecked) match {
  case (_1: Unit, _2: String)(Unit, String)(_, (rejectsTableName @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
    case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
      case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
    }))))))))))
  }))
}))))
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          1373
        </td>
        <td>
          18595
          -
          18608
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.tempTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1375
        </td>
        <td>
          18625
          -
          18670
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!)
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1378
        </td>
        <td>
          18672
          -
          18700
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1381
        </td>
        <td>
          18706
          -
          18715
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1380
        </td>
        <td>
          18706
          -
          18715
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1383
        </td>
        <td>
          18616
          -
          18616
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1374
        </td>
        <td>
          18654
          -
          18670
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          tempTableExists.unary_!
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1377
        </td>
        <td>
          18677
          -
          18699
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.CreateTableError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CreateTableError.apply(scala.None)
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1379
        </td>
        <td>
          18672
          -
          18700
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1445
        </td>
        <td>
          18616
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined.&amp;&amp;(tempTableExists.unary_!))
  scala.`package`.Left.apply[com.vertica.spark.util.error.CreateTableError, Nothing](com.vertica.spark.util.error.CreateTableError.apply(scala.None))
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[(Unit, String)](((x$23: Unit) =&gt; {
  val rejectsTableName: String = &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;);
  scala.Tuple2.apply[Unit, String](x$23, rejectsTableName)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$28: (Unit, String)) =&gt; (x$28: (Unit, String) @unchecked) match {
  case (_1: Unit, _2: String)(Unit, String)(_, (rejectsTableName @ _)) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
    case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
      case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
    }))))))))))
  }))
}))
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          1376
        </td>
        <td>
          18694
          -
          18698
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          1382
        </td>
        <td>
          18742
          -
          18918
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(com.vertica.spark.config.EscapeUtils.sqlEscape(tableName.substring(0, java.lang.Math.min(tableNameMaxLength, tableName.length())), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)).+(&quot;_&quot;).+(sessionId).+(&quot;_COMMITS&quot;).+(&quot;\&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1384
        </td>
        <td>
          18946
          -
          18971
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1387
        </td>
        <td>
          18973
          -
          19010
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1390
        </td>
        <td>
          19016
          -
          19056
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1389
        </td>
        <td>
          19016
          -
          19056
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1392
        </td>
        <td>
          18926
          -
          18926
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1386
        </td>
        <td>
          18973
          -
          19010
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1388
        </td>
        <td>
          19022
          -
          19055
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.getFullTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1385
        </td>
        <td>
          18979
          -
          19009
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.getFullTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          1444
        </td>
        <td>
          18926
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
else
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.config.tablename.getFullTableName).map[(String, String, Unit)](((fullTableName: String) =&gt; {
  val copyStatement: String = VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;);
  val x$24: Unit = (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The copy statement is: \n&quot;.+(copyStatement))
  else
    (): Unit);
  scala.Tuple3.apply[String, String, Unit](fullTableName, copyStatement, x$24)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$27: (String, String, Unit)) =&gt; (x$27: (String, String, Unit) @unchecked) match {
  case (_1: String, _2: String, _3: Unit)(String, String, Unit)((fullTableName @ _), (copyStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
    case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
  }))))))))))
}))
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          1391
        </td>
        <td>
          19080
          -
          19159
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.buildCopyStatement
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.buildCopyStatement(fullTableName, columnList, url, rejectsTableName, &quot;parquet&quot;)
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          1393
        </td>
        <td>
          19253
          -
          19278
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          1443
        </td>
        <td>
          19235
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
else
  VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((rowsCopied: Int) =&gt; VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
  case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
}))))))))))
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          1396
        </td>
        <td>
          19290
          -
          19398
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          1395
        </td>
        <td>
          19341
          -
          19397
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          1394
        </td>
        <td>
          19317
          -
          19330
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.tempTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          1397
        </td>
        <td>
          19290
          -
          19398
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.tempTableName).left.map[com.vertica.spark.util.error.ConnectorError](((x$21: com.vertica.spark.util.error.ConnectorError) =&gt; x$21.context(&quot;commit: Failed to copy rows into temp table&quot;)))
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          1399
        </td>
        <td>
          19482
          -
          19540
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #AEF1AE">
          x$22.context(&quot;commit: Failed to copy rows into target table&quot;)
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          1398
        </td>
        <td>
          19455
          -
          19471
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          1401
        </td>
        <td>
          19428
          -
          19541
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;)))
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          1400
        </td>
        <td>
          19428
          -
          19541
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performCopy(copyStatement, VerticaDistributedFilesystemWritePipe.this.config.tablename).left.map[com.vertica.spark.util.error.ConnectorError](((x$22: com.vertica.spark.util.error.ConnectorError) =&gt; x$22.context(&quot;commit: Failed to copy rows into target table&quot;)))
        </td>
      </tr><tr>
        <td>
          460
        </td>
        <td>
          1442
        </td>
        <td>
          19557
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.testFaultTolerance(rowsCopied, rejectsTableName).left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;))).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((faultToleranceResults: VerticaDistributedFilesystemWritePipe.this.FaultToleranceTestResult) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
  case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
}))))))))
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          1402
        </td>
        <td>
          19656
          -
          19743
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CommitError.apply(err).context(&quot;commit: JDBC Error when trying to determine fault tolerance&quot;)
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          1441
        </td>
        <td>
          19752
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
  case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
}))))))
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          1403
        </td>
        <td>
          19761
          -
          19786
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.saveJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1405
        </td>
        <td>
          19848
          -
          19875
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.JdbcAuth.user
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1408
        </td>
        <td>
          19936
          -
          19965
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.FaultToleranceTestResult.success
        </td>
        <td style="background: #AEF1AE">
          faultToleranceResults.success
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1407
        </td>
        <td>
          19918
          -
          19934
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.sessionId
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.sessionId
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1410
        </td>
        <td>
          19798
          -
          19966
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.updateJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1404
        </td>
        <td>
          19830
          -
          19846
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1406
        </td>
        <td>
          19877
          -
          19916
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.FaultToleranceTestResult.failedRowsPercent
        </td>
        <td style="background: #AEF1AE">
          faultToleranceResults.failedRowsPercent
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          1409
        </td>
        <td>
          19798
          -
          19966
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.updateJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, faultToleranceResults.failedRowsPercent, VerticaDistributedFilesystemWritePipe.this.config.sessionId, faultToleranceResults.success)
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          1411
        </td>
        <td>
          19990
          -
          19999
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          1412
        </td>
        <td>
          19990
          -
          19999
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1414
        </td>
        <td>
          20055
          -
          20064
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1440
        </td>
        <td>
          20015
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (faultToleranceResults.success)
  scala.`package`.Right.apply[Nothing, Unit](())
else
  scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
  case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
}))))
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1416
        </td>
        <td>
          20075
          -
          20099
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.FaultToleranceTestFail.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.FaultToleranceTestFail.apply()
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1413
        </td>
        <td>
          20024
          -
          20053
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.FaultToleranceTestResult.success
        </td>
        <td style="background: #AEF1AE">
          faultToleranceResults.success
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1415
        </td>
        <td>
          20055
          -
          20064
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1418
        </td>
        <td>
          20070
          -
          20100
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply())
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          1417
        </td>
        <td>
          20070
          -
          20100
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.FaultToleranceTestFail, Nothing](com.vertica.spark.util.error.FaultToleranceTestFail.apply())
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          1419
        </td>
        <td>
          20130
          -
          20155
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          1431
        </td>
        <td>
          20108
          -
          20108
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          1439
        </td>
        <td>
          20108
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
else
  scala.`package`.Right.apply[Nothing, String](&quot;&quot;).map[(String, Unit)](((mergeStatement: String) =&gt; {
  val x$25: Unit = if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
    (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
    else
      (): Unit)
  else
    ();
  scala.Tuple2.apply[String, Unit](mergeStatement, x$25)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$26: (String, Unit)) =&gt; (x$26: (String, Unit) @unchecked) match {
  case (_1: String, _2: Unit)(String, Unit)((mergeStatement @ _), _) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
}))
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          1420
        </td>
        <td>
          20211
          -
          20227
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          1423
        </td>
        <td>
          20185
          -
          20273
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          1422
        </td>
        <td>
          20191
          -
          20272
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.buildMergeStatement
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName)
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          1421
        </td>
        <td>
          20241
          -
          20271
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.getFullTableName
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          1424
        </td>
        <td>
          20185
          -
          20273
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemWritePipe.this.buildMergeStatement(VerticaDistributedFilesystemWritePipe.this.config.tablename, columnList, VerticaDistributedFilesystemWritePipe.this.tempTableName.getFullTableName))
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          1425
        </td>
        <td>
          20357
          -
          20366
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          1426
        </td>
        <td>
          20357
          -
          20366
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](&quot;&quot;)
        </td>
      </tr><tr>
        <td>
          477
        </td>
        <td>
          1429
        </td>
        <td>
          20403
          -
          20403
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          477
        </td>
        <td>
          1428
        </td>
        <td>
          20433
          -
          20491
        </td>
        <td>
          Typed
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isInfoEnabled())
  VerticaDistributedFilesystemWritePipe.this.logger.underlying.info(&quot;The merge statement is: \n&quot;.+(mergeStatement))
else
  (): Unit)
        </td>
      </tr><tr>
        <td>
          477
        </td>
        <td>
          1430
        </td>
        <td>
          20403
          -
          20403
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          477
        </td>
        <td>
          1427
        </td>
        <td>
          20406
          -
          20431
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1438
        </td>
        <td>
          20498
          -
          20593
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined)
  VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1432
        </td>
        <td>
          20507
          -
          20532
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.mergeKey.isDefined
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1434
        </td>
        <td>
          20534
          -
          20562
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.performMerge
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1433
        </td>
        <td>
          20534
          -
          20562
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.performMerge
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.performMerge(mergeStatement)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1436
        </td>
        <td>
          20568
          -
          20577
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1435
        </td>
        <td>
          20568
          -
          20577
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          1437
        </td>
        <td>
          20591
          -
          20593
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1450
        </td>
        <td>
          20650
          -
          20688
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1452
        </td>
        <td>
          20690
          -
          20746
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.removeDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.removeDir(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.address)
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1455
        </td>
        <td>
          20647
          -
          20647
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1454
        </td>
        <td>
          20647
          -
          20647
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1451
        </td>
        <td>
          20715
          -
          20745
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1453
        </td>
        <td>
          20690
          -
          20746
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.removeDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer.removeDir(VerticaDistributedFilesystemWritePipe.this.config.fileStoreConfig.address)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          1456
        </td>
        <td>
          20751
          -
          20766
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          timer.endTime()
        </td>
      </tr><tr>
        <td>
          488
        </td>
        <td>
          1459
        </td>
        <td>
          20909
          -
          20940
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Commit data as external table&quot;
        </td>
      </tr><tr>
        <td>
          488
        </td>
        <td>
          1458
        </td>
        <td>
          20901
          -
          20907
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.logger
        </td>
      </tr><tr>
        <td>
          488
        </td>
        <td>
          1457
        </td>
        <td>
          20878
          -
          20899
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          488
        </td>
        <td>
          1460
        </td>
        <td>
          20868
          -
          20941
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemWritePipe.this.config.timeOperations, VerticaDistributedFilesystemWritePipe.this.logger, &quot;Commit data as external table&quot;)
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          1461
        </td>
        <td>
          20946
          -
          20963
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          timer.startTime()
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          1465
        </td>
        <td>
          21022
          -
          21022
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          1464
        </td>
        <td>
          21022
          -
          21022
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          1462
        </td>
        <td>
          21025
          -
          21056
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.copyColumnList.isDefined
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          1463
        </td>
        <td>
          21066
          -
          21173
        </td>
        <td>
          Typed
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          (if (VerticaDistributedFilesystemWritePipe.this.logger.underlying.isWarnEnabled())
  VerticaDistributedFilesystemWritePipe.this.logger.underlying.warn(&quot;Custom copy column list was specified, but will be ignored when creating new external table.&quot;)
else
  (): Unit)
        </td>
      </tr><tr>
        <td>
          496
        </td>
        <td>
          1500
        </td>
        <td>
          21195
          -
          22184
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.configureSession(VerticaDistributedFilesystemWritePipe.this.fileStoreLayer).map[(Unit, Boolean)](((x$30: Unit) =&gt; {
  val existingData: Boolean = VerticaDistributedFilesystemWritePipe.this.config.createExternalTable match {
    case (value: com.vertica.spark.datasource.core.CreateExternalTableOption)Some[com.vertica.spark.datasource.core.CreateExternalTableOption]((value @ _)) =&gt; value match {
      case ExistingData =&gt; true
      case NewData =&gt; false
    }
    case scala.None =&gt; false
  };
  scala.Tuple2.apply[Unit, Boolean](x$30, existingData)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$31: (Unit, Boolean)) =&gt; (x$31: (Unit, Boolean) @unchecked) match {
  case (_1: Unit, _2: Boolean)(Unit, Boolean)(_, (existingData @ _)) =&gt; if (existingData)
  VerticaDistributedFilesystemWritePipe.this.inferExternalTableSchema()
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((createExternalTableStmt: Any) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.createExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, if (existingData)
  scala.Some.apply[String](createExternalTableStmt.toString())
else
  VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen, url).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.validateExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))))))
}))
        </td>
      </tr><tr>
        <td>
          496
        </td>
        <td>
          1466
        </td>
        <td>
          21239
          -
          21253
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          496
        </td>
        <td>
          1471
        </td>
        <td>
          21207
          -
          21207
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Unit, Boolean](x$30, existingData)
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          1467
        </td>
        <td>
          21277
          -
          21303
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.createExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          1468
        </td>
        <td>
          21397
          -
          21401
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          1469
        </td>
        <td>
          21430
          -
          21435
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          504
        </td>
        <td>
          1470
        </td>
        <td>
          21469
          -
          21474
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          1474
        </td>
        <td>
          21563
          -
          21572
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          1473
        </td>
        <td>
          21533
          -
          21557
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.inferExternalTableSchema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.inferExternalTableSchema()
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          1499
        </td>
        <td>
          21489
          -
          22184
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (existingData)
  VerticaDistributedFilesystemWritePipe.this.inferExternalTableSchema()
else
  scala.`package`.Right.apply[Nothing, Unit](()).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((createExternalTableStmt: Any) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.createExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, if (existingData)
  scala.Some.apply[String](createExternalTableStmt.toString())
else
  VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen, url).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.validateExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))))))
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          1472
        </td>
        <td>
          21533
          -
          21557
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.inferExternalTableSchema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.inferExternalTableSchema()
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          1475
        </td>
        <td>
          21563
          -
          21572
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          1498
        </td>
        <td>
          21580
          -
          22184
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.createExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, if (existingData)
  scala.Some.apply[String](createExternalTableStmt.toString())
else
  VerticaDistributedFilesystemWritePipe.this.config.targetTableSql, VerticaDistributedFilesystemWritePipe.this.config.schema, VerticaDistributedFilesystemWritePipe.this.config.strlen, url).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemWritePipe.this.tableUtils.validateExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))))
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          1476
        </td>
        <td>
          21645
          -
          21661
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          1479
        </td>
        <td>
          21696
          -
          21734
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](createExternalTableStmt.toString())
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          1481
        </td>
        <td>
          21740
          -
          21761
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.targetTableSql
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.targetTableSql
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          1478
        </td>
        <td>
          21696
          -
          21734
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](createExternalTableStmt.toString())
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          1477
        </td>
        <td>
          21701
          -
          21733
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td style="background: #AEF1AE">
          createExternalTableStmt.toString()
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          1480
        </td>
        <td>
          21740
          -
          21761
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.targetTableSql
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.targetTableSql
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          1482
        </td>
        <td>
          21788
          -
          21801
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.schema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.schema
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          1483
        </td>
        <td>
          21828
          -
          21841
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.strlen
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.strlen
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          1497
        </td>
        <td>
          21902
          -
          22184
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.validateExternalTable(VerticaDistributedFilesystemWritePipe.this.config.tablename).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))))
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          1484
        </td>
        <td>
          21940
          -
          21956
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          1485
        </td>
        <td>
          21974
          -
          21999
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.saveJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          1496
        </td>
        <td>
          21965
          -
          22184
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemWritePipe.this.config.saveJobStatusTable)
  VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
else
  scala.`package`.Right.apply[Nothing, Unit](()).map[Unit](((_: Unit) =&gt; ()))
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1492
        </td>
        <td>
          22011
          -
          22128
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.updateJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1488
        </td>
        <td>
          22090
          -
          22093
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0.0
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1491
        </td>
        <td>
          22011
          -
          22128
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.updateJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.updateJobStatusTable(VerticaDistributedFilesystemWritePipe.this.config.tablename, VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user, 0.0, VerticaDistributedFilesystemWritePipe.this.config.sessionId, true)
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1490
        </td>
        <td>
          22123
          -
          22127
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1487
        </td>
        <td>
          22061
          -
          22088
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.JdbcAuth.user
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.jdbcConfig.auth.user
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1486
        </td>
        <td>
          22043
          -
          22059
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          1489
        </td>
        <td>
          22095
          -
          22111
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.sessionId
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.sessionId
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          1494
        </td>
        <td>
          22152
          -
          22161
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          1493
        </td>
        <td>
          22152
          -
          22161
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          523
        </td>
        <td>
          1495
        </td>
        <td>
          22182
          -
          22184
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          1501
        </td>
        <td>
          22190
          -
          22205
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          timer.endTime()
        </td>
      </tr><tr>
        <td>
          530
        </td>
        <td>
          1503
        </td>
        <td>
          22347
          -
          22385
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.table.TableUtilsInterface.dropTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.tableUtils.dropTable(VerticaDistributedFilesystemWritePipe.this.config.tablename)
        </td>
      </tr><tr>
        <td>
          530
        </td>
        <td>
          1502
        </td>
        <td>
          22368
          -
          22384
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig.tablename
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.tablename
        </td>
      </tr><tr>
        <td>
          531
        </td>
        <td>
          1504
        </td>
        <td>
          22394
          -
          22403
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          532
        </td>
        <td>
          1505
        </td>
        <td>
          22427
          -
          22434
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          537
        </td>
        <td>
          1506
        </td>
        <td>
          22518
          -
          22529
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;*.parquet&quot;
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1510
        </td>
        <td>
          22655
          -
          22684
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.getAddress()).stripSuffix(&quot;/&quot;)
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1509
        </td>
        <td>
          22698
          -
          22699
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1512
        </td>
        <td>
          22641
          -
          22641
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape$default$2
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1508
        </td>
        <td>
          22685
          -
          22687
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1511
        </td>
        <td>
          22651
          -
          22699
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;/&quot;, &quot;&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.getAddress()).stripSuffix(&quot;/&quot;), globPattern)
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1513
        </td>
        <td>
          22629
          -
          22700
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.EscapeUtils.sqlEscape
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EscapeUtils.sqlEscape(scala.StringContext.apply(&quot;&quot;, &quot;/&quot;, &quot;&quot;).s(scala.Predef.augmentString(VerticaDistributedFilesystemWritePipe.this.getAddress()).stripSuffix(&quot;/&quot;), globPattern), com.vertica.spark.config.EscapeUtils.sqlEscape$default$2)
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          1507
        </td>
        <td>
          22653
          -
          22654
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          542
        </td>
        <td>
          1514
        </td>
        <td>
          22719
          -
          22755
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.config.createExternalTable.isDefined
        </td>
      </tr><tr>
        <td>
          543
        </td>
        <td>
          1515
        </td>
        <td>
          22765
          -
          22795
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.commitDataAsExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.commitDataAsExternalTable(url)
        </td>
      </tr><tr>
        <td>
          543
        </td>
        <td>
          1516
        </td>
        <td>
          22765
          -
          22795
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.commitDataAsExternalTable
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.commitDataAsExternalTable(url)
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          1518
        </td>
        <td>
          22819
          -
          22845
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.commitDataIntoVertica
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.commitDataIntoVertica(url)
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          1517
        </td>
        <td>
          22819
          -
          22845
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemWritePipe.commitDataIntoVertica
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.commitDataIntoVertica(url)
        </td>
      </tr><tr>
        <td>
          552
        </td>
        <td>
          1519
        </td>
        <td>
          22974
          -
          23034
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.CommitError.apply(err).context(&quot;JDBC Error when trying to commit&quot;)
        </td>
      </tr><tr>
        <td>
          552
        </td>
        <td>
          1520
        </td>
        <td>
          22939
          -
          23035
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.LeftProjection.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.commit().left.map[com.vertica.spark.util.error.ConnectorError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.CommitError.apply(err).context(&quot;JDBC Error when trying to commit&quot;)))
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          1521
        </td>
        <td>
          23073
          -
          23093
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.rollback
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.rollback()
        </td>
      </tr><tr>
        <td>
          555
        </td>
        <td>
          1522
        </td>
        <td>
          23129
          -
          23143
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](retError)
        </td>
      </tr><tr>
        <td>
          556
        </td>
        <td>
          1527
        </td>
        <td>
          23172
          -
          23255
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](retError.context(&quot;JDBC Error when trying to rollback: &quot;.+(err.getFullContext)))
        </td>
      </tr><tr>
        <td>
          556
        </td>
        <td>
          1524
        </td>
        <td>
          23235
          -
          23253
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.getFullContext
        </td>
        <td style="background: #F0ADAD">
          err.getFullContext
        </td>
      </tr><tr>
        <td>
          556
        </td>
        <td>
          1523
        </td>
        <td>
          23194
          -
          23232
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;JDBC Error when trying to rollback: &quot;
        </td>
      </tr><tr>
        <td>
          556
        </td>
        <td>
          1526
        </td>
        <td>
          23177
          -
          23254
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          retError.context(&quot;JDBC Error when trying to rollback: &quot;.+(err.getFullContext))
        </td>
      </tr><tr>
        <td>
          556
        </td>
        <td>
          1525
        </td>
        <td>
          23194
          -
          23253
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #F0ADAD">
          &quot;JDBC Error when trying to rollback: &quot;.+(err.getFullContext)
        </td>
      </tr><tr>
        <td>
          560
        </td>
        <td>
          1528
        </td>
        <td>
          23277
          -
          23294
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.close
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemWritePipe.this.jdbcLayer.close()
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>