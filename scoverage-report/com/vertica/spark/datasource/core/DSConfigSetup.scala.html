<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/vertica/spark/datasource/core/DSConfigSetup.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>// (c) Copyright [2020-2021] Micro Focus or one of its affiliates.
</span>2 <span style=''>// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
</span>3 <span style=''>// You may not use this file except in compliance with the License.
</span>4 <span style=''>// You may obtain a copy of the License at
</span>5 <span style=''>//
</span>6 <span style=''>// http://www.apache.org/licenses/LICENSE-2.0
</span>7 <span style=''>//
</span>8 <span style=''>// Unless required by applicable law or agreed to in writing, software
</span>9 <span style=''>// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
</span>10 <span style=''>// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span>11 <span style=''>// See the License for the specific language governing permissions and
</span>12 <span style=''>// limitations under the License.
</span>13 <span style=''>
</span>14 <span style=''>package com.vertica.spark.datasource.core
</span>15 <span style=''>
</span>16 <span style=''>import com.vertica.spark.util.error._
</span>17 <span style=''>import org.apache.spark.sql.types.StructType
</span>18 <span style=''>import com.vertica.spark.config._
</span>19 <span style=''>
</span>20 <span style=''>import scala.util.Try
</span>21 <span style=''>import scala.util.Success
</span>22 <span style=''>import scala.util.Failure
</span>23 <span style=''>import cats.data._
</span>24 <span style=''>import cats.data.Validated._
</span>25 <span style=''>import cats.implicits._
</span>26 <span style=''>import com.typesafe.scalalogging.Logger
</span>27 <span style=''>import com.vertica.spark.datasource.core.factory.{VerticaPipeFactory, VerticaPipeFactoryInterface}
</span>28 <span style=''>import com.vertica.spark.util.error.ErrorHandling.ConnectorResult
</span>29 <span style=''>import org.apache.spark.sql.SparkSession
</span>30 <span style=''>
</span>31 <span style=''>
</span>32 <span style=''>/**
</span>33 <span style=''>  * Interface for taking input of user selected options, performing any setup steps required, and returning the proper configuration structure for the operation.
</span>34 <span style=''>  */
</span>35 <span style=''>trait DSConfigSetupInterface[T] {
</span>36 <span style=''>  /**
</span>37 <span style=''>    * Validates and returns the configuration structure for the specific read/write operation.
</span>38 <span style=''>    *
</span>39 <span style=''>    * @return Will return an error if validation of the user options failed, otherwise will return the configuration structure expected by the writer/reader.
</span>40 <span style=''>    */
</span>41 <span style=''>  def validateAndGetConfig(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[T]
</span>42 <span style=''>
</span>43 <span style=''>  /**
</span>44 <span style=''>   * Performs any necessary initial steps required for the given configuration
</span>45 <span style=''>   *
</span>46 <span style=''>   * @return Optionally returns partitioning information for the operation when needed
</span>47 <span style=''>   */
</span>48 <span style=''>  def performInitialSetup(config: T): ConnectorResult[Option[PartitionInfo]]
</span>49 <span style=''>
</span>50 <span style=''>  /**
</span>51 <span style=''>    * Returns the schema for the table as required by Spark.
</span>52 <span style=''>    */
</span>53 <span style=''>  def getTableSchema(config: T): ConnectorResult[StructType]
</span>54 <span style=''>}
</span>55 <span style=''>
</span>56 <span style=''>sealed trait TLSMode
</span>57 <span style=''>case object Disable extends TLSMode {
</span>58 <span style=''>  override def toString: String = </span><span style='background: #F0ADAD'>&quot;disable&quot;</span><span style=''>
</span>59 <span style=''>}
</span>60 <span style=''>case object Require extends TLSMode {
</span>61 <span style=''>  override def toString: String = </span><span style='background: #F0ADAD'>&quot;require&quot;</span><span style=''>
</span>62 <span style=''>}
</span>63 <span style=''>case object VerifyCA extends TLSMode {
</span>64 <span style=''>  override def toString: String = </span><span style='background: #F0ADAD'>&quot;verify-ca&quot;</span><span style=''>
</span>65 <span style=''>}
</span>66 <span style=''>case object VerifyFull extends TLSMode {
</span>67 <span style=''>  override def toString: String = </span><span style='background: #F0ADAD'>&quot;verify-full&quot;</span><span style=''>
</span>68 <span style=''>}
</span>69 <span style=''>
</span>70 <span style=''>sealed trait CreateExternalTableOption
</span>71 <span style=''>case object ExistingData extends CreateExternalTableOption {
</span>72 <span style=''>  override def toString: String = </span><span style='background: #AEF1AE'>&quot;existing-data&quot;</span><span style=''>
</span>73 <span style=''>}
</span>74 <span style=''>case object NewData extends CreateExternalTableOption {
</span>75 <span style=''>  override def toString: String = </span><span style='background: #AEF1AE'>&quot;new-data&quot;</span><span style=''>
</span>76 <span style=''>}
</span>77 <span style=''>
</span>78 <span style=''>/**
</span>79 <span style=''>  * Util class for common config setup functionality.
</span>80 <span style=''>  */
</span>81 <span style=''>// scalastyle:off
</span>82 <span style=''>object DSConfigSetupUtils {
</span>83 <span style=''>  type ValidationResult[+A] = ValidatedNec[ConnectorError, A]
</span>84 <span style=''>
</span>85 <span style=''>  def checkOldConnectorOptions(config: Map[String, String]): Seq[ConnectorError] = {
</span>86 <span style=''>    val oldList = </span><span style='background: #AEF1AE'>Array(&quot;target_table_ddl&quot;, &quot;numpartitions&quot;, &quot;hdfs_url&quot;, &quot;web_hdfs_url&quot;)</span><span style=''>
</span>87 <span style=''>    val replacementsList = </span><span style='background: #AEF1AE'>Array(&quot;target_table_sql&quot;, &quot;num_partitions&quot;, &quot;staging_fs_url&quot;, &quot;staging_fs_url&quot;)</span><span style=''>
</span>88 <span style=''>
</span>89 <span style=''>    </span><span style='background: #AEF1AE'>oldList.zip(replacementsList).filter({
</span>90 <span style=''></span><span style='background: #AEF1AE'>      case (old, replacement) =&gt; config.contains(old)
</span>91 <span style=''></span><span style='background: #AEF1AE'>    }).map {
</span>92 <span style=''></span><span style='background: #AEF1AE'>      case (old, replacement) =&gt;
</span>93 <span style=''></span><span style='background: #AEF1AE'>          V1ReplacementOption(old, replacement)
</span>94 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>95 <span style=''>  }
</span>96 <span style=''>
</span>97 <span style=''>  def logOrAppendErrorsForOldConnectorOptions[T](config: Map[String, String], res: ValidationResult[T], logger: Logger): ValidationResult[T] = {
</span>98 <span style=''>    val oldConnectorMessages = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.checkOldConnectorOptions(config)</span><span style=''>
</span>99 <span style=''>    val oldConnectorChain = </span><span style='background: #AEF1AE'>NonEmptyChain.fromChain(Chain.fromSeq(oldConnectorMessages))</span><span style=''>
</span>100 <span style=''>
</span>101 <span style=''>    // These are not fatal errors, only add to errors if there is something missing from configuration.
</span>102 <span style=''>    // Otherwise, just log a warning
</span>103 <span style=''>    res match {
</span>104 <span style=''>      case Valid(a) =&gt;
</span>105 <span style=''>        </span><span style='background: #AEF1AE'>oldConnectorMessages.foreach(m =&gt; logger.warn(m.getUserMessage))</span><span style=''>
</span>106 <span style=''>        </span><span style='background: #AEF1AE'>Valid(a)</span><span style=''>
</span>107 <span style=''>      case Invalid(errList) =&gt; </span><span style='background: #AEF1AE'>Invalid(
</span>108 <span style=''></span><span style='background: #AEF1AE'>        oldConnectorChain match {
</span>109 <span style=''></span><span style='background: #AEF1AE'>          case Some(chain) =&gt; errList.concat(chain)
</span>110 <span style=''></span><span style='background: #AEF1AE'>          case None =&gt; errList
</span>111 <span style=''></span><span style='background: #AEF1AE'>        }
</span>112 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>113 <span style=''>    }
</span>114 <span style=''>  }
</span>115 <span style=''>
</span>116 <span style=''>  def getHost(config: Map[String, String]): ValidationResult[String] = {
</span>117 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;host&quot;)</span><span style=''> match {
</span>118 <span style=''>      case Some(host) =&gt; </span><span style='background: #AEF1AE'>host.validNec</span><span style=''>
</span>119 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>HostMissingError().invalidNec</span><span style=''>
</span>120 <span style=''>    }
</span>121 <span style=''>  }
</span>122 <span style=''>
</span>123 <span style=''>  def getCreateExternalTable(config: Map[String, String]): ValidationResult[Option[CreateExternalTableOption]] = {
</span>124 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;create_external_table&quot;)</span><span style=''> match {
</span>125 <span style=''>      case Some(str) =&gt;
</span>126 <span style=''>        str match {
</span>127 <span style=''>          case &quot;new-data&quot; | &quot;true&quot; =&gt; </span><span style='background: #AEF1AE'>Some(NewData).validNec</span><span style=''>
</span>128 <span style=''>          case &quot;existing-data&quot; =&gt; </span><span style='background: #AEF1AE'>Some(ExistingData).validNec</span><span style=''>
</span>129 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>InvalidCreateExternalTableOption().invalidNec</span><span style=''>
</span>130 <span style=''>        }
</span>131 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None.validNec</span><span style=''>
</span>132 <span style=''>    }
</span>133 <span style=''>  }
</span>134 <span style=''>
</span>135 <span style=''>  def getSaveJobStatusTable(config: Map[String, String]): ValidationResult[Boolean] = {
</span>136 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;save_job_status_table&quot;)</span><span style=''> match {
</span>137 <span style=''>      case Some(str) =&gt;
</span>138 <span style=''>        str match {
</span>139 <span style=''>          case &quot;true&quot; =&gt; </span><span style='background: #AEF1AE'>true.validNec</span><span style=''>
</span>140 <span style=''>          case &quot;false&quot; =&gt; </span><span style='background: #F0ADAD'>false.validNec</span><span style=''>
</span>141 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>InvalidSaveJobStatusTableOption().invalidNec</span><span style=''>
</span>142 <span style=''>        }
</span>143 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>false.validNec</span><span style=''>
</span>144 <span style=''>    }
</span>145 <span style=''>  }
</span>146 <span style=''>
</span>147 <span style=''>  def getStagingFsUrl(config: Map[String, String]): ValidationResult[String] = {
</span>148 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;staging_fs_url&quot;)</span><span style=''> match {
</span>149 <span style=''>      case Some(address) =&gt; </span><span style='background: #AEF1AE'>address.validNec</span><span style=''>
</span>150 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>StagingFsUrlMissingError().invalidNec</span><span style=''>
</span>151 <span style=''>    }
</span>152 <span style=''>  }
</span>153 <span style=''>
</span>154 <span style=''>  def getPort(config: Map[String, String]): ValidationResult[Int] = {
</span>155 <span style=''>    </span><span style='background: #AEF1AE'>Try {config.getOrElse(&quot;port&quot;,&quot;5433&quot;).toInt}</span><span style=''> match {
</span>156 <span style=''>      case Success(i) =&gt; if (</span><span style='background: #AEF1AE'>i &gt;= 1 &amp;&amp; i &lt;= 65535</span><span style=''>) </span><span style='background: #AEF1AE'>i.validNec</span><span style=''> else </span><span style='background: #F0ADAD'>InvalidPortError().invalidNec</span><span style=''>
</span>157 <span style=''>      case Failure(_) =&gt; </span><span style='background: #AEF1AE'>InvalidPortError().invalidNec</span><span style=''>
</span>158 <span style=''>    }
</span>159 <span style=''>  }
</span>160 <span style=''>
</span>161 <span style=''>  def getMaxFileSize(config: Map[String, String]): ValidationResult[Int] = {
</span>162 <span style=''>    </span><span style='background: #AEF1AE'>Try {config.getOrElse(&quot;max_file_size_export_mb&quot;,&quot;4096&quot;).toInt}</span><span style=''> match {
</span>163 <span style=''>      case Success(i) =&gt; </span><span style='background: #AEF1AE'>i.validNec</span><span style=''>
</span>164 <span style=''>      case Failure(_) =&gt; </span><span style='background: #F0ADAD'>InvalidIntegerField(&quot;max_file_size_export_mb&quot;).invalidNec</span><span style=''>
</span>165 <span style=''>    }
</span>166 <span style=''>  }
</span>167 <span style=''>
</span>168 <span style=''>  def getMaxRowGroupSize(config: Map[String, String]): ValidationResult[Int] = {
</span>169 <span style=''>    </span><span style='background: #AEF1AE'>Try {config.getOrElse(&quot;max_row_group_size_export_mb&quot;,&quot;16&quot;).toInt}</span><span style=''> match {
</span>170 <span style=''>      case Success(i) =&gt; </span><span style='background: #AEF1AE'>i.validNec</span><span style=''>
</span>171 <span style=''>      case Failure(_) =&gt; </span><span style='background: #F0ADAD'>InvalidIntegerField(&quot;max_row_group_size_export_mb&quot;).invalidNec</span><span style=''>
</span>172 <span style=''>    }
</span>173 <span style=''>  }
</span>174 <span style=''>
</span>175 <span style=''>  def getFailedRowsPercentTolerance(config: Map[String, String]): ValidationResult[Float] = {
</span>176 <span style=''>    </span><span style='background: #AEF1AE'>Try {config.getOrElse(&quot;failed_rows_percent_tolerance&quot;,&quot;0.00&quot;).toFloat}</span><span style=''> match {
</span>177 <span style=''>      case Success(f) =&gt; if (</span><span style='background: #AEF1AE'>f &gt;= 0.00 &amp;&amp; f &lt;= 1.00</span><span style=''>) </span><span style='background: #AEF1AE'>f.validNec</span><span style=''> else </span><span style='background: #AEF1AE'>InvalidFailedRowsTolerance().invalidNec</span><span style=''>
</span>178 <span style=''>      case Failure(_) =&gt; </span><span style='background: #F0ADAD'>InvalidFailedRowsTolerance().invalidNec</span><span style=''>
</span>179 <span style=''>    }
</span>180 <span style=''>  }
</span>181 <span style=''>
</span>182 <span style=''>  def getDb(config: Map[String, String]): ValidationResult[String] = {
</span>183 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;db&quot;)</span><span style=''> match {
</span>184 <span style=''>      case Some(db) =&gt; </span><span style='background: #AEF1AE'>db.validNec</span><span style=''>
</span>185 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>DbMissingError().invalidNec</span><span style=''>
</span>186 <span style=''>    }
</span>187 <span style=''>  }
</span>188 <span style=''>
</span>189 <span style=''>  def getUser(config: Map[String, String]): Option[String] = {
</span>190 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;user&quot;)</span><span style=''>
</span>191 <span style=''>  }
</span>192 <span style=''>
</span>193 <span style=''>  def getPassword(config: Map[String, String]): Option[String] = {
</span>194 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;password&quot;)</span><span style=''>
</span>195 <span style=''>  }
</span>196 <span style=''>
</span>197 <span style=''>  def getKerberosServiceName(config: Map[String, String]): Option[String] = {
</span>198 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;kerberos_service_name&quot;)</span><span style=''>
</span>199 <span style=''>  }
</span>200 <span style=''>
</span>201 <span style=''>  def getKerberosHostname(config: Map[String, String]): Option[String] = {
</span>202 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;kerberos_host_name&quot;)</span><span style=''>
</span>203 <span style=''>  }
</span>204 <span style=''>
</span>205 <span style=''>  def getJaasConfigName(config: Map[String, String]): Option[String] = {
</span>206 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;jaas_config_name&quot;)</span><span style=''>
</span>207 <span style=''>  }
</span>208 <span style=''>
</span>209 <span style=''>  def getTLS(config: Map[String, String]): ValidationResult[TLSMode] = {
</span>210 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;tls_mode&quot;)</span><span style=''> match {
</span>211 <span style=''>      case Some(value) =&gt; value match {
</span>212 <span style=''>        case &quot;disable&quot; =&gt; </span><span style='background: #F0ADAD'>Disable.validNec</span><span style=''>
</span>213 <span style=''>        case &quot;require&quot; =&gt; </span><span style='background: #AEF1AE'>Require.validNec</span><span style=''>
</span>214 <span style=''>        case &quot;verify-ca&quot; =&gt; </span><span style='background: #F0ADAD'>VerifyCA.validNec</span><span style=''>
</span>215 <span style=''>        case &quot;verify-full&quot; =&gt; </span><span style='background: #F0ADAD'>VerifyFull.validNec</span><span style=''>
</span>216 <span style=''>        case _ =&gt; </span><span style='background: #AEF1AE'>TLSModeParseError().invalidNec</span><span style=''>
</span>217 <span style=''>      }
</span>218 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>Disable.validNec</span><span style=''>
</span>219 <span style=''>    }
</span>220 <span style=''>  }
</span>221 <span style=''>
</span>222 <span style=''>  def getAWSAuth(config: Map[String, String]): ValidationResult[Option[AWSAuth]] = {
</span>223 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Secret</span><span style=''>
</span>224 <span style=''>    val accessKeyIdOpt = </span><span style='background: #AEF1AE'>getAWSArg(visibility)(
</span>225 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>226 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_access_key_id&quot;,
</span>227 <span style=''></span><span style='background: #AEF1AE'>      &quot;spark.hadoop.fs.s3a.access.key&quot;,
</span>228 <span style=''></span><span style='background: #AEF1AE'>      &quot;AWS_ACCESS_KEY_ID&quot;).sequence</span><span style=''>
</span>229 <span style=''>    val secretAccessKeyOpt = </span><span style='background: #AEF1AE'>getAWSArg(visibility)(
</span>230 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>231 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_secret_access_key&quot;,
</span>232 <span style=''></span><span style='background: #AEF1AE'>      &quot;spark.hadoop.fs.s3a.secret.key&quot;,
</span>233 <span style=''></span><span style='background: #AEF1AE'>      &quot;AWS_SECRET_ACCESS_KEY&quot;).sequence</span><span style=''>
</span>234 <span style=''>    (accessKeyIdOpt, secretAccessKeyOpt) match {
</span>235 <span style=''>      case (Some(accessKeyId), Some(secretAccessKey)) =&gt; </span><span style='background: #AEF1AE'>(accessKeyId, secretAccessKey).mapN(AWSAuth).map(Some(_))</span><span style=''>
</span>236 <span style=''>      case (None, None) =&gt; </span><span style='background: #F0ADAD'>None.validNec</span><span style=''>
</span>237 <span style=''>      case (Some(_), None) =&gt; </span><span style='background: #F0ADAD'>MissingAWSAccessKeyId().invalidNec</span><span style=''>
</span>238 <span style=''>      case (None, Some(_)) =&gt; </span><span style='background: #F0ADAD'>MissingAWSSecretAccessKey().invalidNec</span><span style=''>
</span>239 <span style=''>    }
</span>240 <span style=''>  }
</span>241 <span style=''>
</span>242 <span style=''>  def getAWSRegion(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>243 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Visible</span><span style=''>
</span>244 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>245 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>246 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_region&quot;,
</span>247 <span style=''></span><span style='background: #AEF1AE'>      _ =&gt; getAWSArgFromEnvVar(visibility)(&quot;AWS_DEFAULT_REGION&quot;))</span><span style=''>
</span>248 <span style=''>  }
</span>249 <span style=''>
</span>250 <span style=''>  def getAWSSessionToken(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>251 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArg(Secret)(
</span>252 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>253 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_session_token&quot;,
</span>254 <span style=''></span><span style='background: #AEF1AE'>      &quot;spark.hadoop.fs.s3a.session.token&quot;,
</span>255 <span style=''></span><span style='background: #AEF1AE'>      &quot;AWS_SESSION_TOKEN&quot;)</span><span style=''>
</span>256 <span style=''>  }
</span>257 <span style=''>
</span>258 <span style=''>  def getAWSCredentialsProvider(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>259 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Visible</span><span style=''>
</span>260 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>261 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>262 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_credentials_provider&quot;,
</span>263 <span style=''></span><span style='background: #AEF1AE'>      _ =&gt; getAWSArgFromSparkConfig(visibility)(
</span>264 <span style=''></span><span style='background: #AEF1AE'>        &quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;, _ =&gt; None.validNec))</span><span style=''>
</span>265 <span style=''>  }
</span>266 <span style=''>
</span>267 <span style=''>  def getAWSEndpoint(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>268 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Visible</span><span style=''>
</span>269 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>270 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>271 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_endpoint&quot;,
</span>272 <span style=''></span><span style='background: #AEF1AE'>      _ =&gt; getAWSArgFromSparkConfig(visibility)(
</span>273 <span style=''></span><span style='background: #AEF1AE'>        &quot;spark.hadoop.fs.s3a.endpoint&quot;, _ =&gt; None.validNec))</span><span style=''>
</span>274 <span style=''>  }
</span>275 <span style=''>
</span>276 <span style=''>  def getAWSSSLEnabled(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>277 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Visible</span><span style=''>
</span>278 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>279 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>280 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_enable_ssl&quot;,
</span>281 <span style=''></span><span style='background: #AEF1AE'>      _ =&gt; getAWSArgFromSparkConfig(visibility)(
</span>282 <span style=''></span><span style='background: #AEF1AE'>        &quot;fs.s3a.connection.ssl.enabled&quot;, _ =&gt; None.validNec))</span><span style=''>
</span>283 <span style=''>  }
</span>284 <span style=''>
</span>285 <span style=''>  def getAWSPathStyleEnabled(config: Map[String, String]): ValidationResult[Option[AWSArg[String]]] = {
</span>286 <span style=''>    val visibility = </span><span style='background: #AEF1AE'>Visible</span><span style=''>
</span>287 <span style=''>    </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>288 <span style=''></span><span style='background: #AEF1AE'>      config,
</span>289 <span style=''></span><span style='background: #AEF1AE'>      &quot;aws_enable_path_style&quot;,
</span>290 <span style=''></span><span style='background: #AEF1AE'>      _ =&gt; getAWSArgFromSparkConfig(visibility)(
</span>291 <span style=''></span><span style='background: #AEF1AE'>        &quot;fs.s3a.path.style.access&quot;, _ =&gt; None.validNec))</span><span style=''>
</span>292 <span style=''>  }
</span>293 <span style=''>
</span>294 <span style=''>  def getBackupServerNode(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>295 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;backup_server_node&quot;)</span><span style=''> match {
</span>296 <span style=''>      case Some(backUpServer) =&gt; </span><span style='background: #AEF1AE'>Some(backUpServer).validNec</span><span style=''>
</span>297 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None.validNec</span><span style=''>
</span>298 <span style=''>    }
</span>299 <span style=''>  }
</span>300 <span style=''>
</span>301 <span style=''>  private def getAWSArg(visibility: Visibility)(
</span>302 <span style=''>                 config: Map[String, String],
</span>303 <span style=''>                 connectorOption: String,
</span>304 <span style=''>                 sparkConfigOption: String,
</span>305 <span style=''>                 envVar: String
</span>306 <span style=''>               ): ValidationResult[Option[AWSArg[String]]] = {
</span>307 <span style=''>      </span><span style='background: #AEF1AE'>getAWSArgFromConnectorOption(visibility)(
</span>308 <span style=''></span><span style='background: #AEF1AE'>        config,
</span>309 <span style=''></span><span style='background: #AEF1AE'>        connectorOption,
</span>310 <span style=''></span><span style='background: #AEF1AE'>        _ =&gt; getAWSArgFromSparkConfig(visibility)(
</span>311 <span style=''></span><span style='background: #AEF1AE'>          sparkConfigOption,
</span>312 <span style=''></span><span style='background: #AEF1AE'>          _ =&gt; getAWSArgFromEnvVar(visibility)(envVar)))</span><span style=''>
</span>313 <span style=''>  }
</span>314 <span style=''>
</span>315 <span style=''>  private def getAWSArgFromConnectorOption(visibility: Visibility)(
</span>316 <span style=''>    config: Map[String, String],
</span>317 <span style=''>    connectorOption: String,
</span>318 <span style=''>    next: Unit =&gt; ValidationResult[Option[AWSArg[String]]]): ValidationResult[Option[AWSArg[String]]] = {
</span>319 <span style=''>    </span><span style='background: #AEF1AE'>config.get(connectorOption)</span><span style=''> match {
</span>320 <span style=''>      case Some(token) =&gt; </span><span style='background: #AEF1AE'>Some(AWSArg(visibility, ConnectorOption, token)).validNec</span><span style=''>
</span>321 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>next(())</span><span style=''>
</span>322 <span style=''>    }
</span>323 <span style=''>  }
</span>324 <span style=''>
</span>325 <span style=''>  private def getAWSArgFromSparkConfig(visibility: Visibility)(
</span>326 <span style=''>    sparkConfigOption: String,
</span>327 <span style=''>    next: Unit =&gt; ValidationResult[Option[AWSArg[String]]]): ValidationResult[Option[AWSArg[String]]] = {
</span>328 <span style=''>    </span><span style='background: #AEF1AE'>SparkSession.getActiveSession</span><span style=''> match {
</span>329 <span style=''>      case Some(session) =&gt;
</span>330 <span style=''>        val sparkConf = </span><span style='background: #AEF1AE'>session.sparkContext.getConf</span><span style=''>
</span>331 <span style=''>        </span><span style='background: #AEF1AE'>Try(sparkConf.get(sparkConfigOption)).toOption</span><span style=''> match {
</span>332 <span style=''>          case Some(token) =&gt; </span><span style='background: #AEF1AE'>Some(AWSArg(visibility, SparkConf, token)).validNec</span><span style=''>
</span>333 <span style=''>          case None =&gt; </span><span style='background: #AEF1AE'>next(())</span><span style=''>
</span>334 <span style=''>        }
</span>335 <span style=''>      case None =&gt; </span><span style='background: #F0ADAD'>LoadConfigMissingSparkSessionError().invalidNec</span><span style=''>
</span>336 <span style=''>    }
</span>337 <span style=''>  }
</span>338 <span style=''>
</span>339 <span style=''>  private def getAWSArgFromEnvVar(visibility: Visibility)(envVar: String):  ValidationResult[Option[AWSArg[String]]] = {
</span>340 <span style=''>    </span><span style='background: #AEF1AE'>sys.env.get(envVar).map(token =&gt; AWSArg(visibility, EnvVar, token)).validNec</span><span style=''>
</span>341 <span style=''>  }
</span>342 <span style=''>
</span>343 <span style=''>  def getKeyStorePath(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>344 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;key_store_path&quot;).validNec</span><span style=''>
</span>345 <span style=''>  }
</span>346 <span style=''>
</span>347 <span style=''>  def getKeyStorePassword(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>348 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;key_store_password&quot;).validNec</span><span style=''>
</span>349 <span style=''>  }
</span>350 <span style=''>
</span>351 <span style=''>  def getTrustStorePath(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>352 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;trust_store_path&quot;).validNec</span><span style=''>
</span>353 <span style=''>  }
</span>354 <span style=''>
</span>355 <span style=''>  def getTrustStorePassword(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>356 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;trust_store_password&quot;).validNec</span><span style=''>
</span>357 <span style=''>  }
</span>358 <span style=''>
</span>359 <span style=''>  def getTablename(config: Map[String, String]): Option[String] = {
</span>360 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;table&quot;)</span><span style=''>
</span>361 <span style=''>  }
</span>362 <span style=''>
</span>363 <span style=''>  def getQuery(config: Map[String, String]): Option[String] = {
</span>364 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;query&quot;)</span><span style=''> match {
</span>365 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None</span><span style=''>
</span>366 <span style=''>        // Strip the ';' from the query to allow for queries ending with this
</span>367 <span style=''>      case Some(value) =&gt; </span><span style='background: #AEF1AE'>Some(value.stripSuffix(&quot;;&quot;))</span><span style=''>
</span>368 <span style=''>    }
</span>369 <span style=''>  }
</span>370 <span style=''>
</span>371 <span style=''>  def getDbSchema(config: Map[String, String]): Option[String] = {
</span>372 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;dbschema&quot;)</span><span style=''>
</span>373 <span style=''>  }
</span>374 <span style=''>
</span>375 <span style=''>  def getTargetTableSQL(config: Map[String, String]): ValidationResult[Option[String]] = {
</span>376 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;target_table_sql&quot;).validNec</span><span style=''>
</span>377 <span style=''>  }
</span>378 <span style=''>
</span>379 <span style=''>  def getCopyColumnList(config: Map[String, String]): ValidationResult[Option[ValidColumnList]] = {
</span>380 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;copy_column_list&quot;)</span><span style=''> match {
</span>381 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None.validNec</span><span style=''>
</span>382 <span style=''>      case Some(listStr) =&gt; </span><span style='background: #AEF1AE'>ValidColumnList(listStr)</span><span style=''>
</span>383 <span style=''>    }
</span>384 <span style=''>  }
</span>385 <span style=''>
</span>386 <span style=''>  def getFilePermissions(config: Map[String, String]): ValidationResult[ValidFilePermissions] = {
</span>387 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;file_permissions&quot;)</span><span style=''> match {
</span>388 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>ValidFilePermissions(&quot;700&quot;)</span><span style=''> // Default to allowing user and group
</span>389 <span style=''>      case Some(str) =&gt; </span><span style='background: #AEF1AE'>ValidFilePermissions(str)</span><span style=''>
</span>390 <span style=''>    }
</span>391 <span style=''>  }
</span>392 <span style=''>
</span>393 <span style=''>  // Optional param, if not specified the partition count will be decided as part of the inital steps
</span>394 <span style=''>  def getPartitionCount(config: Map[String, String]): ValidationResult[Option[Int]] = {
</span>395 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;num_partitions&quot;)</span><span style=''> match {
</span>396 <span style=''>      case Some(partitionCount) =&gt; </span><span style='background: #AEF1AE'>Try{partitionCount.toInt}</span><span style=''> match {
</span>397 <span style=''>        case Success(i) =&gt;
</span>398 <span style=''>          if(</span><span style='background: #AEF1AE'>i &gt; 0</span><span style=''>) </span><span style='background: #AEF1AE'>Some(i).validNec</span><span style=''> else </span><span style='background: #F0ADAD'>InvalidPartitionCountError().invalidNec</span><span style=''>
</span>399 <span style=''>        case Failure(_) =&gt; </span><span style='background: #AEF1AE'>InvalidPartitionCountError().invalidNec</span><span style=''>
</span>400 <span style=''>      }
</span>401 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None.validNec</span><span style=''>
</span>402 <span style=''>    }
</span>403 <span style=''>  }
</span>404 <span style=''>
</span>405 <span style=''>  def getStrLen(config: Map[String, String]) : ValidationResult[Long] = {
</span>406 <span style=''>    </span><span style='background: #AEF1AE'>Try {config.getOrElse(&quot;strlen&quot;,&quot;1024&quot;).toLong}</span><span style=''> match {
</span>407 <span style=''>      case Success(i) =&gt; if (</span><span style='background: #AEF1AE'>i &gt;= 1 &amp;&amp; i &lt;= 32000000</span><span style=''>) </span><span style='background: #AEF1AE'>i.validNec</span><span style=''> else </span><span style='background: #F0ADAD'>InvalidStrlenError().invalidNec</span><span style=''>
</span>408 <span style=''>      case Failure(_) =&gt; </span><span style='background: #AEF1AE'>InvalidStrlenError().invalidNec</span><span style=''>
</span>409 <span style=''>    }
</span>410 <span style=''>  }
</span>411 <span style=''>
</span>412 <span style=''>  def getMergeKey(config: Map[String, String]) : ValidationResult[Option[ValidColumnList]] = {
</span>413 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;merge_key&quot;)</span><span style=''> match {
</span>414 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>None.validNec</span><span style=''>
</span>415 <span style=''>      case Some(listStr) =&gt; </span><span style='background: #AEF1AE'>ValidColumnList(listStr)</span><span style=''>
</span>416 <span style=''>    }
</span>417 <span style=''>  }
</span>418 <span style=''>
</span>419 <span style=''>  def getPreventCleanup(config: Map[String, String]) : ValidationResult[Boolean] = {
</span>420 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;prevent_cleanup&quot;)</span><span style=''> match {
</span>421 <span style=''>      case Some(str) =&gt;
</span>422 <span style=''>        str match {
</span>423 <span style=''>          case &quot;true&quot; =&gt; </span><span style='background: #AEF1AE'>true.validNec</span><span style=''>
</span>424 <span style=''>          case &quot;false&quot; =&gt; </span><span style='background: #F0ADAD'>false.validNec</span><span style=''>
</span>425 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>InvalidPreventCleanupOption().invalidNec</span><span style=''>
</span>426 <span style=''>        }
</span>427 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>false.validNec</span><span style=''>
</span>428 <span style=''>    }
</span>429 <span style=''>  }
</span>430 <span style=''>
</span>431 <span style=''>  def getTimeOperations(config: Map[String, String]) : ValidationResult[Boolean] = {
</span>432 <span style=''>    </span><span style='background: #AEF1AE'>config.get(&quot;time_operations&quot;)</span><span style=''> match {
</span>433 <span style=''>      case Some(str) =&gt;
</span>434 <span style=''>        str match {
</span>435 <span style=''>          case &quot;true&quot; =&gt; </span><span style='background: #F0ADAD'>true.validNec</span><span style=''>
</span>436 <span style=''>          case _ =&gt; </span><span style='background: #F0ADAD'>false.validNec</span><span style=''>
</span>437 <span style=''>        }
</span>438 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>true.validNec</span><span style=''>
</span>439 <span style=''>    }
</span>440 <span style=''>  }
</span>441 <span style=''>
</span>442 <span style=''>  def validateAndGetJDBCAuth(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[JdbcAuth] = {
</span>443 <span style=''>    val user = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getUser(config)</span><span style=''>
</span>444 <span style=''>    val password = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getPassword(config)</span><span style=''>
</span>445 <span style=''>
</span>446 <span style=''>    val serviceName = </span><span style='background: #AEF1AE'>getKerberosServiceName(config)</span><span style=''>
</span>447 <span style=''>    val hostname = </span><span style='background: #AEF1AE'>getKerberosHostname(config)</span><span style=''>
</span>448 <span style=''>    val jaasConfig = </span><span style='background: #AEF1AE'>getJaasConfigName(config)</span><span style=''>
</span>449 <span style=''>
</span>450 <span style=''>    (user, password, serviceName, hostname, jaasConfig) match {
</span>451 <span style=''>      case (Some(u), _, Some(s), Some(h), Some(j)) =&gt; </span><span style='background: #AEF1AE'>KerberosAuth(u, s, h, j).validNec</span><span style=''>
</span>452 <span style=''>      case (Some(u), Some(p), _, _, _) =&gt; </span><span style='background: #AEF1AE'>BasicJdbcAuth(u, p).validNec</span><span style=''>
</span>453 <span style=''>      case (None, _, _, _, _) =&gt; </span><span style='background: #AEF1AE'>UserMissingError().invalidNec</span><span style=''>
</span>454 <span style=''>      case (_, None, None, None, None) =&gt; </span><span style='background: #F0ADAD'>PasswordMissingError().invalidNec</span><span style=''>
</span>455 <span style=''>      case (_, _, _, _, _) =&gt; </span><span style='background: #F0ADAD'>KerberosAuthMissingError().invalidNec</span><span style=''>
</span>456 <span style=''>    }
</span>457 <span style=''>  }
</span>458 <span style=''>
</span>459 <span style=''>  def validateAndGetJDBCSSLConfig(config: Map[String, String]): ValidationResult[JDBCTLSConfig] = {
</span>460 <span style=''>    </span><span style='background: #AEF1AE'>(getTLS(config),
</span>461 <span style=''></span><span style='background: #AEF1AE'>    getKeyStorePath(config),
</span>462 <span style=''></span><span style='background: #AEF1AE'>    getKeyStorePassword(config),
</span>463 <span style=''></span><span style='background: #AEF1AE'>    getTrustStorePath(config),
</span>464 <span style=''></span><span style='background: #AEF1AE'>    getTrustStorePassword(config)).mapN(JDBCTLSConfig)</span><span style=''>
</span>465 <span style=''>  }
</span>466 <span style=''>
</span>467 <span style=''>  /**
</span>468 <span style=''>   * Parses the config map for JDBC config params, collecting any errors.
</span>469 <span style=''>   */
</span>470 <span style=''>  def validateAndGetJDBCConfig(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[JDBCConfig] = {
</span>471 <span style=''>    </span><span style='background: #AEF1AE'>(DSConfigSetupUtils.getHost(config),
</span>472 <span style=''></span><span style='background: #AEF1AE'>    DSConfigSetupUtils.getPort(config),
</span>473 <span style=''></span><span style='background: #AEF1AE'>    DSConfigSetupUtils.getDb(config),
</span>474 <span style=''></span><span style='background: #AEF1AE'>    DSConfigSetupUtils.validateAndGetJDBCAuth(config),
</span>475 <span style=''></span><span style='background: #AEF1AE'>    DSConfigSetupUtils.validateAndGetJDBCSSLConfig(config),
</span>476 <span style=''></span><span style='background: #AEF1AE'>    DSConfigSetupUtils.getBackupServerNode(config)).mapN(JDBCConfig)</span><span style=''>
</span>477 <span style=''>  }
</span>478 <span style=''>
</span>479 <span style=''>  def validateAndGetFilestoreConfig(config: Map[String, String], sessionId: String): DSConfigSetupUtils.ValidationResult[FileStoreConfig] = {
</span>480 <span style=''>    </span><span style='background: #AEF1AE'>(DSConfigSetupUtils.getStagingFsUrl(config),
</span>481 <span style=''></span><span style='background: #AEF1AE'>      sessionId.validNec,
</span>482 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getPreventCleanup(config),
</span>483 <span style=''></span><span style='background: #AEF1AE'>      (DSConfigSetupUtils.getAWSAuth(config),
</span>484 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSRegion(config),
</span>485 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSSessionToken(config),
</span>486 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSCredentialsProvider(config),
</span>487 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSEndpoint(config),
</span>488 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSSSLEnabled(config),
</span>489 <span style=''></span><span style='background: #AEF1AE'>        DSConfigSetupUtils.getAWSPathStyleEnabled(config)
</span>490 <span style=''></span><span style='background: #AEF1AE'>        ).mapN(AWSOptions)
</span>491 <span style=''></span><span style='background: #AEF1AE'>      ).mapN(FileStoreConfig)</span><span style=''>
</span>492 <span style=''>  }
</span>493 <span style=''>
</span>494 <span style=''>  def validateAndGetTableSource(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[TableSource] = {
</span>495 <span style=''>    val name = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getTablename(config)</span><span style=''>
</span>496 <span style=''>    val schema = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getDbSchema(config)</span><span style=''>
</span>497 <span style=''>    val query = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getQuery(config)</span><span style=''>
</span>498 <span style=''>
</span>499 <span style=''>    (query, name) match {
</span>500 <span style=''>      case (Some(q), _) =&gt; </span><span style='background: #AEF1AE'>TableQuery(q, SessionId.getId).validNec</span><span style=''>
</span>501 <span style=''>      case (None, Some(n)) =&gt; </span><span style='background: #AEF1AE'>TableName(n, schema).validNec</span><span style=''>
</span>502 <span style=''>      case (None, None) =&gt; </span><span style='background: #AEF1AE'>TableAndQueryMissingError().invalidNec</span><span style=''>
</span>503 <span style=''>    }
</span>504 <span style=''>  }
</span>505 <span style=''>
</span>506 <span style=''>  def validateAndGetFullTableName(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[TableName] = {
</span>507 <span style=''>    val name = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getTablename(config)</span><span style=''>
</span>508 <span style=''>    val schema = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getDbSchema(config)</span><span style=''>
</span>509 <span style=''>    val query = </span><span style='background: #AEF1AE'>DSConfigSetupUtils.getQuery(config)</span><span style=''>
</span>510 <span style=''>
</span>511 <span style=''>    (query, name) match {
</span>512 <span style=''>      case (_, Some(n)) =&gt; </span><span style='background: #AEF1AE'>TableName(n, schema).validNec</span><span style=''>
</span>513 <span style=''>      case (Some(_), None) =&gt; </span><span style='background: #AEF1AE'>QuerySpecifiedOnWriteError().invalidNec</span><span style=''>
</span>514 <span style=''>      case (None, None) =&gt; </span><span style='background: #F0ADAD'>TablenameMissingError().invalidNec</span><span style=''>
</span>515 <span style=''>    }
</span>516 <span style=''>  }
</span>517 <span style=''>
</span>518 <span style=''>}
</span>519 <span style=''>
</span>520 <span style=''>
</span>521 <span style=''>
</span>522 <span style=''>/**
</span>523 <span style=''>  * Implementation for parsing user option map and getting read config
</span>524 <span style=''>  */
</span>525 <span style=''>class DSReadConfigSetup(val pipeFactory: VerticaPipeFactoryInterface = VerticaPipeFactory, val sessionIdInterface: SessionIdInterface = SessionId) extends DSConfigSetupInterface[ReadConfig] {
</span>526 <span style=''>  private val logger: Logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[DSReadConfigSetup])</span><span style=''>
</span>527 <span style=''>
</span>528 <span style=''>  /**
</span>529 <span style=''>    * Validates the user option map and parses read config
</span>530 <span style=''>    *
</span>531 <span style=''>    * @return Either [[ReadConfig]] or sequence of [[ConnectorError]]
</span>532 <span style=''>    */
</span>533 <span style=''>  override def validateAndGetConfig(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[ReadConfig] = {
</span>534 <span style=''>    val sessionId = </span><span style='background: #AEF1AE'>sessionIdInterface.getId</span><span style=''>
</span>535 <span style=''>
</span>536 <span style=''>    val res = </span><span style='background: #AEF1AE'>(
</span>537 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.validateAndGetJDBCConfig(config),
</span>538 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId),
</span>539 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.validateAndGetTableSource(config),
</span>540 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getPartitionCount(config),
</span>541 <span style=''></span><span style='background: #AEF1AE'>      None.validNec,
</span>542 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getFilePermissions(config),
</span>543 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getMaxRowGroupSize(config),
</span>544 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getMaxFileSize(config),
</span>545 <span style=''></span><span style='background: #AEF1AE'>      DSConfigSetupUtils.getTimeOperations(config)
</span>546 <span style=''></span><span style='background: #AEF1AE'>    ).mapN(DistributedFilesystemReadConfig).andThen { initialConfig =&gt;
</span>547 <span style=''></span><span style='background: #AEF1AE'>      val pipe = pipeFactory.getReadPipe(initialConfig)
</span>548 <span style=''></span><span style='background: #AEF1AE'>
</span>549 <span style=''></span><span style='background: #AEF1AE'>      // Then, retrieve metadata
</span>550 <span style=''></span><span style='background: #AEF1AE'>      val metadata = pipe.getMetadata
</span>551 <span style=''></span><span style='background: #AEF1AE'>      metadata match {
</span>552 <span style=''></span><span style='background: #AEF1AE'>        case Left(err) =&gt; err.invalidNec
</span>553 <span style=''></span><span style='background: #AEF1AE'>        case Right(meta) =&gt; meta match {
</span>554 <span style=''></span><span style='background: #AEF1AE'>          case readMeta: VerticaReadMetadata =&gt; initialConfig.copy(metadata = Some(readMeta)).validNec
</span>555 <span style=''></span><span style='background: #AEF1AE'>          case _ =&gt; </span><span style='background: #F0ADAD'>MissingMetadata().invalidNec</span><span style='background: #AEF1AE'>
</span>556 <span style=''></span><span style='background: #AEF1AE'>        }
</span>557 <span style=''></span><span style='background: #AEF1AE'>      }
</span>558 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>559 <span style=''>
</span>560 <span style=''>    // Check for options left over from old connector
</span>561 <span style=''>    </span><span style='background: #AEF1AE'>DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions(config, res, logger)</span><span style=''>
</span>562 <span style=''>  }
</span>563 <span style=''>
</span>564 <span style=''>  /**
</span>565 <span style=''>   * Calls read pipe implementation to perform initial setup for the read operation.
</span>566 <span style=''>   *
</span>567 <span style=''>   * @param config Configuration data for the read operation. Used to construct pipe for performing initial setup.
</span>568 <span style=''>   * @return List of partitioning information for the operation to pass down to readers, or error that occured in setup.
</span>569 <span style=''>   */
</span>570 <span style=''>  override def performInitialSetup(config: ReadConfig): ConnectorResult[Option[PartitionInfo]] = {
</span>571 <span style=''>    </span><span style='background: #F0ADAD'>pipeFactory.getReadPipe(config).doPreReadSteps()</span><span style=''> match {
</span>572 <span style=''>      case Right(partitionInfo) =&gt; </span><span style='background: #F0ADAD'>Right(Some(partitionInfo))</span><span style=''>
</span>573 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(err)</span><span style=''>
</span>574 <span style=''>    }
</span>575 <span style=''>  }
</span>576 <span style=''>
</span>577 <span style=''>  /**
</span>578 <span style=''>   * Returns the schema of the table being read
</span>579 <span style=''>   *
</span>580 <span style=''>   * @param config Configuration data for the read operation. Contains the metadata required for returning the table schema.
</span>581 <span style=''>   * @return The table schema or an error that occured trying to retrieve it
</span>582 <span style=''>   */
</span>583 <span style=''>  override def getTableSchema(config: ReadConfig): ConnectorResult[StructType] =  {
</span>584 <span style=''>    config match {
</span>585 <span style=''>      case DistributedFilesystemReadConfig(_, _, _, _, verticaMetadata, _, _, _, _) =&gt;
</span>586 <span style=''>        verticaMetadata match {
</span>587 <span style=''>          case None =&gt; </span><span style='background: #F0ADAD'>Left(SchemaDiscoveryError())</span><span style=''>
</span>588 <span style=''>          case Some(metadata) =&gt; </span><span style='background: #F0ADAD'>Right(metadata.schema)</span><span style=''>
</span>589 <span style=''>        }
</span>590 <span style=''>    }
</span>591 <span style=''>  }
</span>592 <span style=''>}
</span>593 <span style=''>
</span>594 <span style=''>/**
</span>595 <span style=''>  * Implementation for parsing user option map and getting write config
</span>596 <span style=''>  */
</span>597 <span style=''>class DSWriteConfigSetup(val schema: Option[StructType], val pipeFactory: VerticaPipeFactoryInterface = VerticaPipeFactory, sessionIdInterface: SessionIdInterface = SessionId) extends DSConfigSetupInterface[WriteConfig] {
</span>598 <span style=''>  private val logger: Logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[DSWriteConfigSetup])</span><span style=''>
</span>599 <span style=''>
</span>600 <span style=''>  /**
</span>601 <span style=''>    * Validates the user option map and parses read config
</span>602 <span style=''>    *
</span>603 <span style=''>    * @return Either [[WriteConfig]] or [[ConnectorError]]
</span>604 <span style=''>    */
</span>605 <span style=''>  override def validateAndGetConfig(config: Map[String, String]): DSConfigSetupUtils.ValidationResult[WriteConfig] = {
</span>606 <span style=''>    val sessionId = </span><span style='background: #AEF1AE'>sessionIdInterface.getId</span><span style=''>
</span>607 <span style=''>
</span>608 <span style=''>    // List of configuration errors. We keep these all so that we report all issues with the given configuration to the user at once and they don't have to solve issues one by one.
</span>609 <span style=''>    val res = </span><span style='background: #AEF1AE'>schema</span><span style=''> match {
</span>610 <span style=''>      case Some(passedInSchema) =&gt;
</span>611 <span style=''>        </span><span style='background: #AEF1AE'>(
</span>612 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.validateAndGetJDBCConfig(config),
</span>613 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId),
</span>614 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.validateAndGetFullTableName(config),
</span>615 <span style=''></span><span style='background: #AEF1AE'>          passedInSchema.validNec,
</span>616 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getStrLen(config),
</span>617 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getTargetTableSQL(config),
</span>618 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getCopyColumnList(config),
</span>619 <span style=''></span><span style='background: #AEF1AE'>          sessionId.validNec,
</span>620 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getFailedRowsPercentTolerance(config),
</span>621 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getFilePermissions(config),
</span>622 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getCreateExternalTable(config),
</span>623 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getSaveJobStatusTable(config),
</span>624 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getMergeKey(config),
</span>625 <span style=''></span><span style='background: #AEF1AE'>          DSConfigSetupUtils.getTimeOperations(config)
</span>626 <span style=''></span><span style='background: #AEF1AE'>        ).mapN(DistributedFilesystemWriteConfig)</span><span style=''>
</span>627 <span style=''>      case None =&gt;
</span>628 <span style=''>        </span><span style='background: #F0ADAD'>MissingSchemaError().invalidNec</span><span style=''>
</span>629 <span style=''>    }
</span>630 <span style=''>
</span>631 <span style=''>    // Check for options left over from old connector
</span>632 <span style=''>    </span><span style='background: #AEF1AE'>DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions(config, res, logger)</span><span style=''>
</span>633 <span style=''>  }
</span>634 <span style=''>
</span>635 <span style=''>  /**
</span>636 <span style=''>   * Performs initial steps for write operation.
</span>637 <span style=''>   *
</span>638 <span style=''>   * @return None, partitioning info not needed for write operation.
</span>639 <span style=''>   */
</span>640 <span style=''>  override def performInitialSetup(config: WriteConfig): ConnectorResult[Option[PartitionInfo]] = {
</span>641 <span style=''>    val pipe = </span><span style='background: #F0ADAD'>pipeFactory.getWritePipe(config)</span><span style=''>
</span>642 <span style=''>    </span><span style='background: #F0ADAD'>pipe.doPreWriteSteps()</span><span style=''> match {
</span>643 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(err)</span><span style=''>
</span>644 <span style=''>      case Right(_) =&gt; </span><span style='background: #F0ADAD'>Right(None)</span><span style=''>
</span>645 <span style=''>    }
</span>646 <span style=''>  }
</span>647 <span style=''>
</span>648 <span style=''>  /**
</span>649 <span style=''>   * Returns the same schema that was passed in to this class.
</span>650 <span style=''>   */
</span>651 <span style=''>  override def getTableSchema(config: WriteConfig): ConnectorResult[StructType] = </span><span style='background: #F0ADAD'>this.schema</span><span style=''> match {
</span>652 <span style=''>    case Some(schema) =&gt; </span><span style='background: #F0ADAD'>Right(schema)</span><span style=''>
</span>653 <span style=''>    case None =&gt; </span><span style='background: #F0ADAD'>Left(SchemaDiscoveryError())</span><span style=''>
</span>654 <span style=''>  }
</span>655 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          164
        </td>
        <td>
          2240
          -
          2249
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;disable&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          165
        </td>
        <td>
          2324
          -
          2333
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;require&quot;
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          166
        </td>
        <td>
          2409
          -
          2420
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;verify-ca&quot;
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          167
        </td>
        <td>
          2498
          -
          2511
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;verify-full&quot;
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          168
        </td>
        <td>
          2649
          -
          2664
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;existing-data&quot;
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          169
        </td>
        <td>
          2757
          -
          2767
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;new-data&quot;
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          173
        </td>
        <td>
          3101
          -
          3115
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;web_hdfs_url&quot;
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          172
        </td>
        <td>
          3089
          -
          3099
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;hdfs_url&quot;
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          174
        </td>
        <td>
          3046
          -
          3116
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;target_table_ddl&quot;, &quot;numpartitions&quot;, &quot;hdfs_url&quot;, &quot;web_hdfs_url&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          171
        </td>
        <td>
          3072
          -
          3087
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;numpartitions&quot;
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          170
        </td>
        <td>
          3052
          -
          3070
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;target_table_ddl&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          176
        </td>
        <td>
          3170
          -
          3186
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;num_partitions&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          175
        </td>
        <td>
          3150
          -
          3168
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;target_table_sql&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          178
        </td>
        <td>
          3206
          -
          3222
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;staging_fs_url&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          177
        </td>
        <td>
          3188
          -
          3204
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;staging_fs_url&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          179
        </td>
        <td>
          3144
          -
          3223
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;target_table_sql&quot;, &quot;num_partitions&quot;, &quot;staging_fs_url&quot;, &quot;staging_fs_url&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          182
        </td>
        <td>
          3229
          -
          3258
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.zip
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[String](oldList).zip[String, String, Array[(String, String)]](scala.Predef.wrapRefArray[String](replacementsList))(scala.this.Array.canBuildFrom[(String, String)]((ClassTag.apply[(String, String)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, String)])))
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          184
        </td>
        <td>
          3229
          -
          3328
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[(String, String)](scala.Predef.refArrayOps[String](oldList).zip[String, String, Array[(String, String)]](scala.Predef.wrapRefArray[String](replacementsList))(scala.this.Array.canBuildFrom[(String, String)]((ClassTag.apply[(String, String)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, String)])))).filter(((x0$1: (String, String)) =&gt; x0$1 match {
  case (_1: String, _2: String)(String, String)((old @ _), (replacement @ _)) =&gt; config.contains(old)
}))
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          181
        </td>
        <td>
          3240
          -
          3240
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[(String, String)]((ClassTag.apply[(String, String)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, String)]))
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          180
        </td>
        <td>
          3241
          -
          3257
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.LowPriorityImplicits.wrapRefArray
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.wrapRefArray[String](replacementsList)
        </td>
      </tr><tr>
        <td>
          90
        </td>
        <td>
          183
        </td>
        <td>
          3301
          -
          3321
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td style="background: #AEF1AE">
          config.contains(old)
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          187
        </td>
        <td>
          3333
          -
          3333
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.FallbackArrayBuilding.fallbackCanBuildFrom
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.fallbackCanBuildFrom[com.vertica.spark.util.error.V1ReplacementOption](Predef.this.DummyImplicit.dummyImplicit)
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          186
        </td>
        <td>
          3333
          -
          3333
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.DummyImplicit.dummyImplicit
        </td>
        <td style="background: #AEF1AE">
          Predef.this.DummyImplicit.dummyImplicit
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          188
        </td>
        <td>
          3229
          -
          3421
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[(String, String)](scala.Predef.refArrayOps[(String, String)](scala.Predef.refArrayOps[String](oldList).zip[String, String, Array[(String, String)]](scala.Predef.wrapRefArray[String](replacementsList))(scala.this.Array.canBuildFrom[(String, String)]((ClassTag.apply[(String, String)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, String)])))).filter(((x0$1: (String, String)) =&gt; x0$1 match {
  case (_1: String, _2: String)(String, String)((old @ _), (replacement @ _)) =&gt; config.contains(old)
}))).map[com.vertica.spark.util.error.V1ReplacementOption, Seq[com.vertica.spark.util.error.ConnectorError]](((x0$2: (String, String)) =&gt; x0$2 match {
  case (_1: String, _2: String)(String, String)((old @ _), (replacement @ _)) =&gt; com.vertica.spark.util.error.V1ReplacementOption.apply(old, replacement)
}))(scala.this.Array.fallbackCanBuildFrom[com.vertica.spark.util.error.V1ReplacementOption](Predef.this.DummyImplicit.dummyImplicit))
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          185
        </td>
        <td>
          3378
          -
          3415
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.V1ReplacementOption.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.V1ReplacementOption.apply(old, replacement)
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          189
        </td>
        <td>
          3603
          -
          3654
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.checkOldConnectorOptions
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.checkOldConnectorOptions(config)
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          191
        </td>
        <td>
          3683
          -
          3743
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.NonEmptyChainImpl.fromChain
        </td>
        <td style="background: #AEF1AE">
          cats.data.`package`.NonEmptyChain.fromChain[com.vertica.spark.util.error.ConnectorError](cats.data.Chain.fromSeq[com.vertica.spark.util.error.ConnectorError](oldConnectorMessages))
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          190
        </td>
        <td>
          3707
          -
          3742
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.Chain.fromSeq
        </td>
        <td style="background: #AEF1AE">
          cats.data.Chain.fromSeq[com.vertica.spark.util.error.ConnectorError](oldConnectorMessages)
        </td>
      </tr><tr>
        <td>
          105
        </td>
        <td>
          192
        </td>
        <td>
          3933
          -
          3997
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td style="background: #AEF1AE">
          oldConnectorMessages.foreach[Unit](((m: com.vertica.spark.util.error.ConnectorError) =&gt; (if (logger.underlying.isWarnEnabled())
  logger.underlying.warn(m.getUserMessage)
else
  (): Unit)))
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          193
        </td>
        <td>
          4006
          -
          4014
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.Validated.Valid.apply
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.Valid.apply[T](a)
        </td>
      </tr><tr>
        <td>
          107
        </td>
        <td>
          195
        </td>
        <td>
          4046
          -
          4189
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.Validated.Invalid.apply
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.Invalid.apply[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](oldConnectorChain match {
  case (value: cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError])Some[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]]((chain @ _)) =&gt; data.this.NonEmptyChainImpl.catsNonEmptyChainOps[com.vertica.spark.util.error.ConnectorError](errList).concat[com.vertica.spark.util.error.ConnectorError](chain)
  case scala.None =&gt; errList
})
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          194
        </td>
        <td>
          4119
          -
          4140
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.NonEmptyChainOps.concat
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsNonEmptyChainOps[com.vertica.spark.util.error.ConnectorError](errList).concat[com.vertica.spark.util.error.ConnectorError](chain)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          196
        </td>
        <td>
          4278
          -
          4296
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;host&quot;)
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          197
        </td>
        <td>
          4330
          -
          4343
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[String](host).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          199
        </td>
        <td>
          4363
          -
          4392
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.HostMissingError](com.vertica.spark.util.error.HostMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          198
        </td>
        <td>
          4363
          -
          4381
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.HostMissingError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.HostMissingError.apply()
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          200
        </td>
        <td>
          4523
          -
          4558
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;create_external_table&quot;)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          202
        </td>
        <td>
          4649
          -
          4671
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[com.vertica.spark.datasource.core.NewData.type]](scala.Some.apply[com.vertica.spark.datasource.core.NewData.type](NewData)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          201
        </td>
        <td>
          4649
          -
          4662
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.datasource.core.NewData.type](NewData)
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          204
        </td>
        <td>
          4706
          -
          4733
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[com.vertica.spark.datasource.core.ExistingData.type]](scala.Some.apply[com.vertica.spark.datasource.core.ExistingData.type](ExistingData)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          203
        </td>
        <td>
          4706
          -
          4724
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.datasource.core.ExistingData.type](ExistingData)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          205
        </td>
        <td>
          4754
          -
          4788
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidCreateExternalTableOption.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidCreateExternalTableOption.apply()
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          206
        </td>
        <td>
          4754
          -
          4799
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidCreateExternalTableOption](com.vertica.spark.util.error.InvalidCreateExternalTableOption.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          208
        </td>
        <td>
          4829
          -
          4842
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          207
        </td>
        <td>
          4829
          -
          4833
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          209
        </td>
        <td>
          4946
          -
          4981
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;save_job_status_table&quot;)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          211
        </td>
        <td>
          5059
          -
          5072
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](true).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          210
        </td>
        <td>
          5059
          -
          5063
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          213
        </td>
        <td>
          5099
          -
          5113
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](false).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          212
        </td>
        <td>
          5099
          -
          5104
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          214
        </td>
        <td>
          5134
          -
          5167
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidSaveJobStatusTableOption.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidSaveJobStatusTableOption.apply()
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          215
        </td>
        <td>
          5134
          -
          5178
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidSaveJobStatusTableOption](com.vertica.spark.util.error.InvalidSaveJobStatusTableOption.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          217
        </td>
        <td>
          5208
          -
          5222
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](false).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          216
        </td>
        <td>
          5208
          -
          5213
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          218
        </td>
        <td>
          5319
          -
          5347
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;staging_fs_url&quot;)
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          219
        </td>
        <td>
          5384
          -
          5400
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[String](address).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          220
        </td>
        <td>
          5420
          -
          5446
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.StagingFsUrlMissingError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.StagingFsUrlMissingError.apply()
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          221
        </td>
        <td>
          5420
          -
          5457
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.StagingFsUrlMissingError](com.vertica.spark.util.error.StagingFsUrlMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          223
        </td>
        <td>
          5548
          -
          5585
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toInt
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(config.getOrElse[String](&quot;port&quot;, &quot;5433&quot;)).toInt
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          222
        </td>
        <td>
          5548
          -
          5579
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.getOrElse
        </td>
        <td style="background: #AEF1AE">
          config.getOrElse[String](&quot;port&quot;, &quot;5433&quot;)
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          224
        </td>
        <td>
          5543
          -
          5586
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Int](scala.Predef.augmentString(config.getOrElse[String](&quot;port&quot;, &quot;5433&quot;)).toInt)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          227
        </td>
        <td>
          5624
          -
          5644
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          i.&gt;=(1).&amp;&amp;(i.&lt;=(65535))
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          226
        </td>
        <td>
          5634
          -
          5644
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td style="background: #AEF1AE">
          i.&lt;=(65535)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          229
        </td>
        <td>
          5646
          -
          5656
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Int](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          232
        </td>
        <td>
          5662
          -
          5691
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPortError](com.vertica.spark.util.error.InvalidPortError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          225
        </td>
        <td>
          5629
          -
          5630
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          228
        </td>
        <td>
          5646
          -
          5656
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Int](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          231
        </td>
        <td>
          5662
          -
          5691
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPortError](com.vertica.spark.util.error.InvalidPortError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          230
        </td>
        <td>
          5662
          -
          5680
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPortError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidPortError.apply()
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          234
        </td>
        <td>
          5717
          -
          5746
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPortError](com.vertica.spark.util.error.InvalidPortError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          233
        </td>
        <td>
          5717
          -
          5735
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPortError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidPortError.apply()
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          236
        </td>
        <td>
          5844
          -
          5900
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toInt
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(config.getOrElse[String](&quot;max_file_size_export_mb&quot;, &quot;4096&quot;)).toInt
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          235
        </td>
        <td>
          5844
          -
          5894
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.getOrElse
        </td>
        <td style="background: #AEF1AE">
          config.getOrElse[String](&quot;max_file_size_export_mb&quot;, &quot;4096&quot;)
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          237
        </td>
        <td>
          5839
          -
          5901
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Int](scala.Predef.augmentString(config.getOrElse[String](&quot;max_file_size_export_mb&quot;, &quot;4096&quot;)).toInt)
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          238
        </td>
        <td>
          5935
          -
          5945
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Int](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          240
        </td>
        <td>
          5971
          -
          6028
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidIntegerField](com.vertica.spark.util.error.InvalidIntegerField.apply(&quot;max_file_size_export_mb&quot;)).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          239
        </td>
        <td>
          5971
          -
          6017
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidIntegerField.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidIntegerField.apply(&quot;max_file_size_export_mb&quot;)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          241
        </td>
        <td>
          6130
          -
          6183
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.getOrElse
        </td>
        <td style="background: #AEF1AE">
          config.getOrElse[String](&quot;max_row_group_size_export_mb&quot;, &quot;16&quot;)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          243
        </td>
        <td>
          6125
          -
          6190
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Int](scala.Predef.augmentString(config.getOrElse[String](&quot;max_row_group_size_export_mb&quot;, &quot;16&quot;)).toInt)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          242
        </td>
        <td>
          6130
          -
          6189
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toInt
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(config.getOrElse[String](&quot;max_row_group_size_export_mb&quot;, &quot;16&quot;)).toInt
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          244
        </td>
        <td>
          6224
          -
          6234
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Int](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          245
        </td>
        <td>
          6260
          -
          6311
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidIntegerField.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidIntegerField.apply(&quot;max_row_group_size_export_mb&quot;)
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          246
        </td>
        <td>
          6260
          -
          6322
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidIntegerField](com.vertica.spark.util.error.InvalidIntegerField.apply(&quot;max_row_group_size_export_mb&quot;)).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          247
        </td>
        <td>
          6437
          -
          6493
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.getOrElse
        </td>
        <td style="background: #AEF1AE">
          config.getOrElse[String](&quot;failed_rows_percent_tolerance&quot;, &quot;0.00&quot;)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          249
        </td>
        <td>
          6432
          -
          6502
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Float](scala.Predef.augmentString(config.getOrElse[String](&quot;failed_rows_percent_tolerance&quot;, &quot;0.00&quot;)).toFloat)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          248
        </td>
        <td>
          6437
          -
          6501
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toFloat
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(config.getOrElse[String](&quot;failed_rows_percent_tolerance&quot;, &quot;0.00&quot;)).toFloat
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          254
        </td>
        <td>
          6564
          -
          6574
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Float](f).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          253
        </td>
        <td>
          6564
          -
          6574
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Float](f).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          256
        </td>
        <td>
          6580
          -
          6619
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidFailedRowsTolerance](com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          250
        </td>
        <td>
          6545
          -
          6549
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0.0
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          252
        </td>
        <td>
          6540
          -
          6562
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          f.&gt;=(0.0).&amp;&amp;(f.&lt;=(1.0))
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          255
        </td>
        <td>
          6580
          -
          6608
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply()
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          257
        </td>
        <td>
          6580
          -
          6619
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidFailedRowsTolerance](com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          251
        </td>
        <td>
          6553
          -
          6562
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Float.&lt;=
        </td>
        <td style="background: #AEF1AE">
          f.&lt;=(1.0)
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          259
        </td>
        <td>
          6645
          -
          6684
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidFailedRowsTolerance](com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          258
        </td>
        <td>
          6645
          -
          6673
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidFailedRowsTolerance.apply()
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          260
        </td>
        <td>
          6771
          -
          6787
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;db&quot;)
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          261
        </td>
        <td>
          6819
          -
          6830
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[String](db).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          262
        </td>
        <td>
          6850
          -
          6866
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.DbMissingError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.DbMissingError.apply()
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          263
        </td>
        <td>
          6850
          -
          6877
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.DbMissingError](com.vertica.spark.util.error.DbMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          264
        </td>
        <td>
          6956
          -
          6974
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;user&quot;)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          265
        </td>
        <td>
          7051
          -
          7073
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;password&quot;)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          266
        </td>
        <td>
          7161
          -
          7196
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;kerberos_service_name&quot;)
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          267
        </td>
        <td>
          7281
          -
          7313
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;kerberos_host_name&quot;)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          268
        </td>
        <td>
          7396
          -
          7426
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;jaas_config_name&quot;)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          269
        </td>
        <td>
          7509
          -
          7531
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;tls_mode&quot;)
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          270
        </td>
        <td>
          7606
          -
          7622
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.datasource.core.Disable.type](Disable).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          271
        </td>
        <td>
          7649
          -
          7665
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.datasource.core.Require.type](Require).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          272
        </td>
        <td>
          7694
          -
          7711
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.datasource.core.VerifyCA.type](VerifyCA).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          273
        </td>
        <td>
          7742
          -
          7761
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.datasource.core.VerifyFull.type](VerifyFull).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          274
        </td>
        <td>
          7780
          -
          7799
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.TLSModeParseError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.TLSModeParseError.apply()
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          275
        </td>
        <td>
          7780
          -
          7810
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.TLSModeParseError](com.vertica.spark.util.error.TLSModeParseError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          218
        </td>
        <td>
          276
        </td>
        <td>
          7838
          -
          7854
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.datasource.core.Disable.type](Disable).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          277
        </td>
        <td>
          7972
          -
          7978
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Secret
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Secret
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          279
        </td>
        <td>
          8025
          -
          8025
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.ValidatedInstances2.catsDataTraverseFunctorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataTraverseFunctorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]]
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          278
        </td>
        <td>
          8004
          -
          8134
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArg
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArg(visibility)(config, &quot;aws_access_key_id&quot;, &quot;spark.hadoop.fs.s3a.access.key&quot;, &quot;AWS_ACCESS_KEY_ID&quot;)
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          280
        </td>
        <td>
          8135
          -
          8135
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[Option[com.vertica.spark.config.AWSArg[String]]]
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          282
        </td>
        <td>
          8004
          -
          8143
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.Traverse.Ops.sequence
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.toTraverseOps[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, Option[com.vertica.spark.config.AWSArg[String]]](DSConfigSetupUtils.this.getAWSArg(visibility)(config, &quot;aws_access_key_id&quot;, &quot;spark.hadoop.fs.s3a.access.key&quot;, &quot;AWS_ACCESS_KEY_ID&quot;))(cats.data.Validated.catsDataTraverseFunctorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]]).sequence[Option, com.vertica.spark.config.AWSArg[String]](scala.Predef.$conforms[Option[com.vertica.spark.config.AWSArg[String]]], cats.implicits.catsStdInstancesForOption)
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          281
        </td>
        <td>
          8135
          -
          8135
        </td>
        <td>
          Select
        </td>
        <td>
          cats.instances.OptionInstances.catsStdInstancesForOption
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsStdInstancesForOption
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          283
        </td>
        <td>
          8173
          -
          8311
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArg
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArg(visibility)(config, &quot;aws_secret_access_key&quot;, &quot;spark.hadoop.fs.s3a.secret.key&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;)
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          284
        </td>
        <td>
          8194
          -
          8194
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.ValidatedInstances2.catsDataTraverseFunctorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataTraverseFunctorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]]
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          286
        </td>
        <td>
          8312
          -
          8312
        </td>
        <td>
          Select
        </td>
        <td>
          cats.instances.OptionInstances.catsStdInstancesForOption
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsStdInstancesForOption
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          285
        </td>
        <td>
          8312
          -
          8312
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[Option[com.vertica.spark.config.AWSArg[String]]]
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          287
        </td>
        <td>
          8173
          -
          8320
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.Traverse.Ops.sequence
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.toTraverseOps[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, Option[com.vertica.spark.config.AWSArg[String]]](DSConfigSetupUtils.this.getAWSArg(visibility)(config, &quot;aws_secret_access_key&quot;, &quot;spark.hadoop.fs.s3a.secret.key&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;))(cats.data.Validated.catsDataTraverseFunctorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]]).sequence[Option, com.vertica.spark.config.AWSArg[String]](scala.Predef.$conforms[Option[com.vertica.spark.config.AWSArg[String]]], cats.implicits.catsStdInstancesForOption)
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          295
        </td>
        <td>
          8427
          -
          8484
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.Validated.map
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple2Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, com.vertica.spark.config.AWSArg[String], com.vertica.spark.config.AWSArg[String]](scala.Tuple2.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSArg[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSArg[String]]](accessKeyId, secretAccessKey)).mapN[com.vertica.spark.config.AWSAuth](com.vertica.spark.config.AWSAuth)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])).map[Some[com.vertica.spark.config.AWSAuth]](((x$1: com.vertica.spark.config.AWSAuth) =&gt; scala.Some.apply[com.vertica.spark.config.AWSAuth](x$1)))
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          289
        </td>
        <td>
          8463
          -
          8470
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.AWSAuth
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.AWSAuth
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          292
        </td>
        <td>
          8462
          -
          8462
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          294
        </td>
        <td>
          8476
          -
          8483
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.config.AWSAuth](x$1)
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          288
        </td>
        <td>
          8427
          -
          8457
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSArg[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSArg[String]]](accessKeyId, secretAccessKey)
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          291
        </td>
        <td>
          8462
          -
          8462
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          290
        </td>
        <td>
          8462
          -
          8462
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          293
        </td>
        <td>
          8462
          -
          8462
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          297
        </td>
        <td>
          8512
          -
          8525
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          296
        </td>
        <td>
          8512
          -
          8516
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          298
        </td>
        <td>
          8556
          -
          8579
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingAWSAccessKeyId.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MissingAWSAccessKeyId.apply()
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          299
        </td>
        <td>
          8556
          -
          8590
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.MissingAWSAccessKeyId](com.vertica.spark.util.error.MissingAWSAccessKeyId.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          301
        </td>
        <td>
          8621
          -
          8659
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.MissingAWSSecretAccessKey](com.vertica.spark.util.error.MissingAWSSecretAccessKey.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          300
        </td>
        <td>
          8621
          -
          8648
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingAWSSecretAccessKey.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MissingAWSSecretAccessKey.apply()
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          302
        </td>
        <td>
          8786
          -
          8793
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Visible
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Visible
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          305
        </td>
        <td>
          8798
          -
          8939
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, &quot;aws_region&quot;, ((x$2: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromEnvVar(visibility)(&quot;AWS_DEFAULT_REGION&quot;)))
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          303
        </td>
        <td>
          8860
          -
          8872
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;aws_region&quot;
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          304
        </td>
        <td>
          8885
          -
          8938
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromEnvVar
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromEnvVar(visibility)(&quot;AWS_DEFAULT_REGION&quot;)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          306
        </td>
        <td>
          9049
          -
          9178
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArg
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArg(com.vertica.spark.config.Secret)(config, &quot;aws_session_token&quot;, &quot;spark.hadoop.fs.s3a.session.token&quot;, &quot;AWS_SESSION_TOKEN&quot;)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          307
        </td>
        <td>
          9312
          -
          9319
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Visible
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Visible
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          313
        </td>
        <td>
          9324
          -
          9539
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, &quot;aws_credentials_provider&quot;, ((x$3: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;, ((x$4: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))))
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          308
        </td>
        <td>
          9386
          -
          9412
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;aws_credentials_provider&quot;
        </td>
      </tr><tr>
        <td>
          263
        </td>
        <td>
          312
        </td>
        <td>
          9425
          -
          9538
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromSparkConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;, ((x$4: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          310
        </td>
        <td>
          9524
          -
          9528
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          309
        </td>
        <td>
          9471
          -
          9517
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          311
        </td>
        <td>
          9524
          -
          9537
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          314
        </td>
        <td>
          9662
          -
          9669
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Visible
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Visible
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          320
        </td>
        <td>
          9674
          -
          9861
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, &quot;aws_endpoint&quot;, ((x$5: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;spark.hadoop.fs.s3a.endpoint&quot;, ((x$6: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))))
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          315
        </td>
        <td>
          9736
          -
          9750
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;aws_endpoint&quot;
        </td>
      </tr><tr>
        <td>
          272
        </td>
        <td>
          319
        </td>
        <td>
          9763
          -
          9860
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromSparkConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;spark.hadoop.fs.s3a.endpoint&quot;, ((x$6: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          316
        </td>
        <td>
          9809
          -
          9839
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;spark.hadoop.fs.s3a.endpoint&quot;
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          318
        </td>
        <td>
          9846
          -
          9859
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          317
        </td>
        <td>
          9846
          -
          9850
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          321
        </td>
        <td>
          9986
          -
          9993
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Visible
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Visible
        </td>
      </tr><tr>
        <td>
          278
        </td>
        <td>
          327
        </td>
        <td>
          9998
          -
          10188
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, &quot;aws_enable_ssl&quot;, ((x$7: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;fs.s3a.connection.ssl.enabled&quot;, ((x$8: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))))
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          322
        </td>
        <td>
          10060
          -
          10076
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;aws_enable_ssl&quot;
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          326
        </td>
        <td>
          10089
          -
          10187
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromSparkConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;fs.s3a.connection.ssl.enabled&quot;, ((x$8: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))
        </td>
      </tr><tr>
        <td>
          282
        </td>
        <td>
          325
        </td>
        <td>
          10173
          -
          10186
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          282
        </td>
        <td>
          324
        </td>
        <td>
          10173
          -
          10177
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          282
        </td>
        <td>
          323
        </td>
        <td>
          10135
          -
          10166
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;fs.s3a.connection.ssl.enabled&quot;
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          328
        </td>
        <td>
          10319
          -
          10326
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.Visible
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.Visible
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          334
        </td>
        <td>
          10331
          -
          10523
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, &quot;aws_enable_path_style&quot;, ((x$9: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;fs.s3a.path.style.access&quot;, ((x$10: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))))
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          329
        </td>
        <td>
          10393
          -
          10416
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;aws_enable_path_style&quot;
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          333
        </td>
        <td>
          10429
          -
          10522
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromSparkConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(&quot;fs.s3a.path.style.access&quot;, ((x$10: Unit) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]))
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          331
        </td>
        <td>
          10508
          -
          10512
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          330
        </td>
        <td>
          10475
          -
          10501
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;fs.s3a.path.style.access&quot;
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          332
        </td>
        <td>
          10508
          -
          10521
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          335
        </td>
        <td>
          10626
          -
          10658
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;backup_server_node&quot;)
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          337
        </td>
        <td>
          10700
          -
          10727
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[String]](scala.Some.apply[String](backUpServer)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          336
        </td>
        <td>
          10700
          -
          10718
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](backUpServer)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          339
        </td>
        <td>
          10747
          -
          10760
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          338
        </td>
        <td>
          10747
          -
          10751
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          342
        </td>
        <td>
          11054
          -
          11273
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromConnectorOption
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromConnectorOption(visibility)(config, connectorOption, ((x$11: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(sparkConfigOption, ((x$12: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromEnvVar(visibility)(envVar)))))
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          341
        </td>
        <td>
          11150
          -
          11272
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromSparkConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromSparkConfig(visibility)(sparkConfigOption, ((x$12: Unit) =&gt; DSConfigSetupUtils.this.getAWSArgFromEnvVar(visibility)(envVar)))
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          340
        </td>
        <td>
          11232
          -
          11271
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSArgFromEnvVar
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getAWSArgFromEnvVar(visibility)(envVar)
        </td>
      </tr><tr>
        <td>
          319
        </td>
        <td>
          343
        </td>
        <td>
          11519
          -
          11546
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(connectorOption)
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          346
        </td>
        <td>
          11581
          -
          11629
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.config.AWSArg[String]](com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.ConnectorOption, token))
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          345
        </td>
        <td>
          11586
          -
          11628
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.AWSArg.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.ConnectorOption, token)
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          344
        </td>
        <td>
          11605
          -
          11620
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.ConnectorOption
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.ConnectorOption
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          347
        </td>
        <td>
          11581
          -
          11638
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[com.vertica.spark.config.AWSArg[String]]](scala.Some.apply[com.vertica.spark.config.AWSArg[String]](com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.ConnectorOption, token))).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          348
        </td>
        <td>
          11658
          -
          11666
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td style="background: #AEF1AE">
          next.apply(())
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          349
        </td>
        <td>
          11883
          -
          11912
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.SparkSession.getActiveSession
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.SparkSession.getActiveSession
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          350
        </td>
        <td>
          11973
          -
          12001
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.SparkContext.getConf
        </td>
        <td style="background: #AEF1AE">
          session.sparkContext.getConf
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          352
        </td>
        <td>
          12010
          -
          12056
        </td>
        <td>
          Select
        </td>
        <td>
          scala.util.Try.toOption
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[String](sparkConf.get(sparkConfigOption)).toOption
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          351
        </td>
        <td>
          12014
          -
          12046
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.SparkConf.get
        </td>
        <td style="background: #AEF1AE">
          sparkConf.get(sparkConfigOption)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          355
        </td>
        <td>
          12095
          -
          12137
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.config.AWSArg[String]](com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.SparkConf, token))
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          354
        </td>
        <td>
          12100
          -
          12136
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.AWSArg.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.SparkConf, token)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          353
        </td>
        <td>
          12119
          -
          12128
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.SparkConf
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.SparkConf
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          356
        </td>
        <td>
          12095
          -
          12146
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[com.vertica.spark.config.AWSArg[String]]](scala.Some.apply[com.vertica.spark.config.AWSArg[String]](com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.SparkConf, token))).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          357
        </td>
        <td>
          12170
          -
          12178
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td style="background: #AEF1AE">
          next.apply(())
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          358
        </td>
        <td>
          12208
          -
          12244
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.LoadConfigMissingSparkSessionError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.LoadConfigMissingSparkSessionError.apply()
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          359
        </td>
        <td>
          12208
          -
          12255
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.LoadConfigMissingSparkSessionError](com.vertica.spark.util.error.LoadConfigMissingSparkSessionError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          361
        </td>
        <td>
          12425
          -
          12458
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.AWSArg.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.EnvVar, token)
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          360
        </td>
        <td>
          12444
          -
          12450
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.EnvVar
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.EnvVar
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          363
        </td>
        <td>
          12392
          -
          12468
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[com.vertica.spark.config.AWSArg[String]]](scala.sys.`package`.env.get(envVar).map[com.vertica.spark.config.AWSArg[String]](((token: String) =&gt; com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.EnvVar, token)))).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          362
        </td>
        <td>
          12392
          -
          12459
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.map
        </td>
        <td style="background: #AEF1AE">
          scala.sys.`package`.env.get(envVar).map[com.vertica.spark.config.AWSArg[String]](((token: String) =&gt; com.vertica.spark.config.AWSArg.apply[String](visibility, com.vertica.spark.config.EnvVar, token)))
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          364
        </td>
        <td>
          12567
          -
          12595
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;key_store_path&quot;)
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          365
        </td>
        <td>
          12567
          -
          12604
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[String]](config.get(&quot;key_store_path&quot;)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          367
        </td>
        <td>
          12707
          -
          12748
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[String]](config.get(&quot;key_store_password&quot;)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          366
        </td>
        <td>
          12707
          -
          12739
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;key_store_password&quot;)
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          369
        </td>
        <td>
          12849
          -
          12888
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[String]](config.get(&quot;trust_store_path&quot;)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          368
        </td>
        <td>
          12849
          -
          12879
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;trust_store_path&quot;)
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          370
        </td>
        <td>
          12993
          -
          13027
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;trust_store_password&quot;)
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          371
        </td>
        <td>
          12993
          -
          13036
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[String]](config.get(&quot;trust_store_password&quot;)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          360
        </td>
        <td>
          372
        </td>
        <td>
          13114
          -
          13133
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;table&quot;)
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          373
        </td>
        <td>
          13207
          -
          13226
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;query&quot;)
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          374
        </td>
        <td>
          13254
          -
          13258
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          367
        </td>
        <td>
          376
        </td>
        <td>
          13363
          -
          13391
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](scala.Predef.augmentString(value).stripSuffix(&quot;;&quot;))
        </td>
      </tr><tr>
        <td>
          367
        </td>
        <td>
          375
        </td>
        <td>
          13368
          -
          13390
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.StringLike.stripSuffix
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(value).stripSuffix(&quot;;&quot;)
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          377
        </td>
        <td>
          13474
          -
          13496
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;dbschema&quot;)
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          379
        </td>
        <td>
          13597
          -
          13636
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Option[String]](config.get(&quot;target_table_sql&quot;)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          378
        </td>
        <td>
          13597
          -
          13627
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;target_table_sql&quot;)
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          380
        </td>
        <td>
          13746
          -
          13776
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;copy_column_list&quot;)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          382
        </td>
        <td>
          13804
          -
          13817
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          381
        </td>
        <td>
          13804
          -
          13808
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          383
        </td>
        <td>
          13846
          -
          13870
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidColumnList.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.ValidColumnList.apply(listStr)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          384
        </td>
        <td>
          13984
          -
          14014
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;file_permissions&quot;)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          385
        </td>
        <td>
          14042
          -
          14069
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidFilePermissions.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.ValidFilePermissions.apply(&quot;700&quot;)
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          386
        </td>
        <td>
          14132
          -
          14157
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidFilePermissions.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.ValidFilePermissions.apply(str)
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          387
        </td>
        <td>
          14363
          -
          14391
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;num_partitions&quot;)
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          388
        </td>
        <td>
          14439
          -
          14459
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toInt
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(partitionCount).toInt
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          389
        </td>
        <td>
          14435
          -
          14460
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Int](scala.Predef.augmentString(partitionCount).toInt)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          391
        </td>
        <td>
          14516
          -
          14523
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Int](i)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          394
        </td>
        <td>
          14538
          -
          14566
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPartitionCountError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidPartitionCountError.apply()
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          396
        </td>
        <td>
          14538
          -
          14577
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPartitionCountError](com.vertica.spark.util.error.InvalidPartitionCountError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          390
        </td>
        <td>
          14509
          -
          14514
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td style="background: #AEF1AE">
          i.&gt;(0)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          393
        </td>
        <td>
          14516
          -
          14532
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[Int]](scala.Some.apply[Int](i)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          392
        </td>
        <td>
          14516
          -
          14532
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Some[Int]](scala.Some.apply[Int](i)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          395
        </td>
        <td>
          14538
          -
          14577
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPartitionCountError](com.vertica.spark.util.error.InvalidPartitionCountError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          397
        </td>
        <td>
          14605
          -
          14633
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPartitionCountError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidPartitionCountError.apply()
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          398
        </td>
        <td>
          14605
          -
          14644
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPartitionCountError](com.vertica.spark.util.error.InvalidPartitionCountError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          400
        </td>
        <td>
          14672
          -
          14685
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          399
        </td>
        <td>
          14672
          -
          14676
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          403
        </td>
        <td>
          14775
          -
          14821
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Try.apply
        </td>
        <td style="background: #AEF1AE">
          scala.util.Try.apply[Long](scala.Predef.augmentString(config.getOrElse[String](&quot;strlen&quot;, &quot;1024&quot;)).toLong)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          402
        </td>
        <td>
          14780
          -
          14820
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.toLong
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(config.getOrElse[String](&quot;strlen&quot;, &quot;1024&quot;)).toLong
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          401
        </td>
        <td>
          14780
          -
          14813
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.getOrElse
        </td>
        <td style="background: #AEF1AE">
          config.getOrElse[String](&quot;strlen&quot;, &quot;1024&quot;)
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          406
        </td>
        <td>
          14859
          -
          14882
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          i.&gt;=(1).&amp;&amp;(i.&lt;=(32000000))
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          409
        </td>
        <td>
          14900
          -
          14920
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidStrlenError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.InvalidStrlenError.apply()
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          405
        </td>
        <td>
          14869
          -
          14882
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&lt;=
        </td>
        <td style="background: #AEF1AE">
          i.&lt;=(32000000)
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          408
        </td>
        <td>
          14884
          -
          14894
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Long](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          411
        </td>
        <td>
          14900
          -
          14931
        </td>
        <td>
          Block
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidStrlenError](com.vertica.spark.util.error.InvalidStrlenError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          410
        </td>
        <td>
          14900
          -
          14931
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidStrlenError](com.vertica.spark.util.error.InvalidStrlenError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          404
        </td>
        <td>
          14864
          -
          14865
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          407
        </td>
        <td>
          14884
          -
          14894
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Long](i).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          412
        </td>
        <td>
          14957
          -
          14977
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidStrlenError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidStrlenError.apply()
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          413
        </td>
        <td>
          14957
          -
          14988
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidStrlenError](com.vertica.spark.util.error.InvalidStrlenError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          414
        </td>
        <td>
          15099
          -
          15122
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;merge_key&quot;)
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          415
        </td>
        <td>
          15150
          -
          15154
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          416
        </td>
        <td>
          15150
          -
          15163
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          417
        </td>
        <td>
          15192
          -
          15216
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidColumnList.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.ValidColumnList.apply(listStr)
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          418
        </td>
        <td>
          15317
          -
          15346
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;prevent_cleanup&quot;)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          420
        </td>
        <td>
          15424
          -
          15437
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](true).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          419
        </td>
        <td>
          15424
          -
          15428
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          421
        </td>
        <td>
          15464
          -
          15469
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          422
        </td>
        <td>
          15464
          -
          15478
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](false).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          424
        </td>
        <td>
          15499
          -
          15539
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.InvalidPreventCleanupOption](com.vertica.spark.util.error.InvalidPreventCleanupOption.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          423
        </td>
        <td>
          15499
          -
          15528
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPreventCleanupOption.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidPreventCleanupOption.apply()
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          426
        </td>
        <td>
          15569
          -
          15583
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](false).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          425
        </td>
        <td>
          15569
          -
          15574
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          427
        </td>
        <td>
          15684
          -
          15713
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.get
        </td>
        <td style="background: #AEF1AE">
          config.get(&quot;time_operations&quot;)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          429
        </td>
        <td>
          15791
          -
          15804
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](true).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          428
        </td>
        <td>
          15791
          -
          15795
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          true
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          430
        </td>
        <td>
          15825
          -
          15830
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          431
        </td>
        <td>
          15825
          -
          15839
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](false).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          433
        </td>
        <td>
          15869
          -
          15882
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[Boolean](true).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          432
        </td>
        <td>
          15869
          -
          15873
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          434
        </td>
        <td>
          16018
          -
          16052
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getUser
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getUser(config)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          435
        </td>
        <td>
          16072
          -
          16110
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getPassword
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getPassword(config)
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          436
        </td>
        <td>
          16134
          -
          16164
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getKerberosServiceName
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getKerberosServiceName(config)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          437
        </td>
        <td>
          16184
          -
          16211
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getKerberosHostname
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getKerberosHostname(config)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          438
        </td>
        <td>
          16233
          -
          16258
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getJaasConfigName
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getJaasConfigName(config)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          439
        </td>
        <td>
          16378
          -
          16402
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.KerberosAuth.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.KerberosAuth.apply(u, s, h, j)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          440
        </td>
        <td>
          16378
          -
          16411
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.KerberosAuth](com.vertica.spark.config.KerberosAuth.apply(u, s, h, j)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          452
        </td>
        <td>
          442
        </td>
        <td>
          16454
          -
          16482
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.BasicJdbcAuth](com.vertica.spark.config.BasicJdbcAuth.apply(u, p)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          452
        </td>
        <td>
          441
        </td>
        <td>
          16454
          -
          16473
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.BasicJdbcAuth.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.BasicJdbcAuth.apply(u, p)
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          444
        </td>
        <td>
          16516
          -
          16545
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.UserMissingError](com.vertica.spark.util.error.UserMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          443
        </td>
        <td>
          16516
          -
          16534
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.UserMissingError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.UserMissingError.apply()
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          445
        </td>
        <td>
          16588
          -
          16610
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.PasswordMissingError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.PasswordMissingError.apply()
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          446
        </td>
        <td>
          16588
          -
          16621
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.PasswordMissingError](com.vertica.spark.util.error.PasswordMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          455
        </td>
        <td>
          448
        </td>
        <td>
          16652
          -
          16689
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.KerberosAuthMissingError](com.vertica.spark.util.error.KerberosAuthMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          455
        </td>
        <td>
          447
        </td>
        <td>
          16652
          -
          16678
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.KerberosAuthMissingError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.KerberosAuthMissingError.apply()
        </td>
      </tr><tr>
        <td>
          460
        </td>
        <td>
          454
        </td>
        <td>
          16805
          -
          16949
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple5.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple5.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.datasource.core.TLSMode], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]]](DSConfigSetupUtils.this.getTLS(config), DSConfigSetupUtils.this.getKeyStorePath(config), DSConfigSetupUtils.this.getKeyStorePassword(config), DSConfigSetupUtils.this.getTrustStorePath(config), DSConfigSetupUtils.this.getTrustStorePassword(config))
        </td>
      </tr><tr>
        <td>
          460
        </td>
        <td>
          449
        </td>
        <td>
          16806
          -
          16820
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTLS
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getTLS(config)
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          450
        </td>
        <td>
          16826
          -
          16849
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getKeyStorePath
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getKeyStorePath(config)
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          451
        </td>
        <td>
          16855
          -
          16882
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getKeyStorePassword
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getKeyStorePassword(config)
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          452
        </td>
        <td>
          16888
          -
          16913
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTrustStorePath
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getTrustStorePath(config)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          460
        </td>
        <td>
          16805
          -
          16969
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.syntax.Tuple5SemigroupalOps.mapN
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple5Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, com.vertica.spark.datasource.core.TLSMode, Option[String], Option[String], Option[String], Option[String]](scala.Tuple5.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.datasource.core.TLSMode], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]]](DSConfigSetupUtils.this.getTLS(config), DSConfigSetupUtils.this.getKeyStorePath(config), DSConfigSetupUtils.this.getKeyStorePassword(config), DSConfigSetupUtils.this.getTrustStorePath(config), DSConfigSetupUtils.this.getTrustStorePassword(config))).mapN[com.vertica.spark.config.JDBCTLSConfig](com.vertica.spark.config.JDBCTLSConfig)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          457
        </td>
        <td>
          16954
          -
          16954
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          456
        </td>
        <td>
          16954
          -
          16954
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          459
        </td>
        <td>
          16954
          -
          16954
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          453
        </td>
        <td>
          16919
          -
          16948
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTrustStorePassword
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.this.getTrustStorePassword(config)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          455
        </td>
        <td>
          16955
          -
          16968
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.JDBCTLSConfig
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.JDBCTLSConfig
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          458
        </td>
        <td>
          16954
          -
          16954
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          461
        </td>
        <td>
          17179
          -
          17213
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getHost
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getHost(config)
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          467
        </td>
        <td>
          17178
          -
          17459
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple6.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple6.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JdbcAuth], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCTLSConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]]](DSConfigSetupUtils.getHost(config), DSConfigSetupUtils.getPort(config), DSConfigSetupUtils.getDb(config), DSConfigSetupUtils.validateAndGetJDBCAuth(config), DSConfigSetupUtils.validateAndGetJDBCSSLConfig(config), DSConfigSetupUtils.getBackupServerNode(config))
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          462
        </td>
        <td>
          17219
          -
          17253
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getPort
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getPort(config)
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          463
        </td>
        <td>
          17259
          -
          17291
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getDb
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getDb(config)
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          464
        </td>
        <td>
          17297
          -
          17346
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetJDBCAuth
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetJDBCAuth(config)
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          465
        </td>
        <td>
          17352
          -
          17406
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetJDBCSSLConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetJDBCSSLConfig(config)
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          469
        </td>
        <td>
          17464
          -
          17464
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          472
        </td>
        <td>
          17464
          -
          17464
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          468
        </td>
        <td>
          17465
          -
          17475
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.JDBCConfig
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.JDBCConfig
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          471
        </td>
        <td>
          17464
          -
          17464
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          470
        </td>
        <td>
          17464
          -
          17464
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          473
        </td>
        <td>
          17178
          -
          17476
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.syntax.Tuple6SemigroupalOps.mapN
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple6Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, String, Int, String, com.vertica.spark.config.JdbcAuth, com.vertica.spark.config.JDBCTLSConfig, Option[String]](scala.Tuple6.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JdbcAuth], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCTLSConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]]](DSConfigSetupUtils.getHost(config), DSConfigSetupUtils.getPort(config), DSConfigSetupUtils.getDb(config), DSConfigSetupUtils.validateAndGetJDBCAuth(config), DSConfigSetupUtils.validateAndGetJDBCSSLConfig(config), DSConfigSetupUtils.getBackupServerNode(config))).mapN[com.vertica.spark.config.JDBCConfig](com.vertica.spark.config.JDBCConfig)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]))
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          466
        </td>
        <td>
          17412
          -
          17458
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getBackupServerNode
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getBackupServerNode(config)
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          474
        </td>
        <td>
          17629
          -
          17671
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getStagingFsUrl
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getStagingFsUrl(config)
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          491
        </td>
        <td>
          17628
          -
          18159
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple4.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple4.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], cats.data.ValidatedNec[Nothing,String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSOptions]](DSConfigSetupUtils.getStagingFsUrl(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing], DSConfigSetupUtils.getPreventCleanup(config), cats.implicits.catsSyntaxTuple7Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, Option[com.vertica.spark.config.AWSAuth], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]]](scala.Tuple7.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSAuth]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]]](DSConfigSetupUtils.getAWSAuth(config), DSConfigSetupUtils.getAWSRegion(config), DSConfigSetupUtils.getAWSSessionToken(config), DSConfigSetupUtils.getAWSCredentialsProvider(config), DSConfigSetupUtils.getAWSEndpoint(config), DSConfigSetupUtils.getAWSSSLEnabled(config), DSConfigSetupUtils.getAWSPathStyleEnabled(config))).mapN[com.vertica.spark.config.AWSOptions](com.vertica.spark.config.AWSOptions)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])))
        </td>
      </tr><tr>
        <td>
          481
        </td>
        <td>
          475
        </td>
        <td>
          17679
          -
          17697
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          476
        </td>
        <td>
          17705
          -
          17749
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getPreventCleanup
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getPreventCleanup(config)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          477
        </td>
        <td>
          17758
          -
          17795
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSAuth
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSAuth(config)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          484
        </td>
        <td>
          17757
          -
          18134
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple7.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple7.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSAuth]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]]](DSConfigSetupUtils.getAWSAuth(config), DSConfigSetupUtils.getAWSRegion(config), DSConfigSetupUtils.getAWSSessionToken(config), DSConfigSetupUtils.getAWSCredentialsProvider(config), DSConfigSetupUtils.getAWSEndpoint(config), DSConfigSetupUtils.getAWSSSLEnabled(config), DSConfigSetupUtils.getAWSPathStyleEnabled(config))
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          478
        </td>
        <td>
          17805
          -
          17844
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSRegion
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSRegion(config)
        </td>
      </tr><tr>
        <td>
          485
        </td>
        <td>
          479
        </td>
        <td>
          17854
          -
          17899
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSSessionToken
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSSessionToken(config)
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          480
        </td>
        <td>
          17909
          -
          17961
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSCredentialsProvider
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSCredentialsProvider(config)
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          481
        </td>
        <td>
          17971
          -
          18012
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSEndpoint
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSEndpoint(config)
        </td>
      </tr><tr>
        <td>
          488
        </td>
        <td>
          482
        </td>
        <td>
          18022
          -
          18065
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSSSLEnabled
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSSSLEnabled(config)
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          483
        </td>
        <td>
          18075
          -
          18124
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getAWSPathStyleEnabled
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getAWSPathStyleEnabled(config)
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          487
        </td>
        <td>
          18139
          -
          18139
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          490
        </td>
        <td>
          17757
          -
          18151
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.syntax.Tuple7SemigroupalOps.mapN
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple7Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, Option[com.vertica.spark.config.AWSAuth], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]]](scala.Tuple7.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSAuth]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]]](DSConfigSetupUtils.getAWSAuth(config), DSConfigSetupUtils.getAWSRegion(config), DSConfigSetupUtils.getAWSSessionToken(config), DSConfigSetupUtils.getAWSCredentialsProvider(config), DSConfigSetupUtils.getAWSEndpoint(config), DSConfigSetupUtils.getAWSSSLEnabled(config), DSConfigSetupUtils.getAWSPathStyleEnabled(config))).mapN[com.vertica.spark.config.AWSOptions](com.vertica.spark.config.AWSOptions)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]))
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          489
        </td>
        <td>
          18139
          -
          18139
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          486
        </td>
        <td>
          18139
          -
          18139
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          488
        </td>
        <td>
          18139
          -
          18139
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          485
        </td>
        <td>
          18140
          -
          18150
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.AWSOptions
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.AWSOptions
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          496
        </td>
        <td>
          18164
          -
          18164
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          492
        </td>
        <td>
          18165
          -
          18180
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.FileStoreConfig
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          495
        </td>
        <td>
          18164
          -
          18164
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          497
        </td>
        <td>
          17628
          -
          18181
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.syntax.Tuple4SemigroupalOps.mapN
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple4Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, String, String, Boolean, com.vertica.spark.config.AWSOptions](scala.Tuple4.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[String], cats.data.ValidatedNec[Nothing,String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.AWSOptions]](DSConfigSetupUtils.getStagingFsUrl(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing], DSConfigSetupUtils.getPreventCleanup(config), cats.implicits.catsSyntaxTuple7Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, Option[com.vertica.spark.config.AWSAuth], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]], Option[com.vertica.spark.config.AWSArg[String]]](scala.Tuple7.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSAuth]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.AWSArg[String]]]](DSConfigSetupUtils.getAWSAuth(config), DSConfigSetupUtils.getAWSRegion(config), DSConfigSetupUtils.getAWSSessionToken(config), DSConfigSetupUtils.getAWSCredentialsProvider(config), DSConfigSetupUtils.getAWSEndpoint(config), DSConfigSetupUtils.getAWSSSLEnabled(config), DSConfigSetupUtils.getAWSPathStyleEnabled(config))).mapN[com.vertica.spark.config.AWSOptions](com.vertica.spark.config.AWSOptions)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])))).mapN[com.vertica.spark.config.FileStoreConfig](com.vertica.spark.config.FileStoreConfig)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]))
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          494
        </td>
        <td>
          18164
          -
          18164
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          493
        </td>
        <td>
          18164
          -
          18164
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          498
        </td>
        <td>
          18317
          -
          18356
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTablename
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getTablename(config)
        </td>
      </tr><tr>
        <td>
          496
        </td>
        <td>
          499
        </td>
        <td>
          18374
          -
          18412
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getDbSchema
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getDbSchema(config)
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          500
        </td>
        <td>
          18429
          -
          18464
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getQuery
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getQuery(config)
        </td>
      </tr><tr>
        <td>
          500
        </td>
        <td>
          501
        </td>
        <td>
          18533
          -
          18548
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.SessionId.getId
        </td>
        <td style="background: #AEF1AE">
          SessionId.getId
        </td>
      </tr><tr>
        <td>
          500
        </td>
        <td>
          503
        </td>
        <td>
          18519
          -
          18558
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.TableQuery](com.vertica.spark.config.TableQuery.apply(q, SessionId.getId)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          500
        </td>
        <td>
          502
        </td>
        <td>
          18519
          -
          18549
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.TableQuery.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.TableQuery.apply(q, SessionId.getId)
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          505
        </td>
        <td>
          18589
          -
          18618
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.TableName](com.vertica.spark.config.TableName.apply(n, schema)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          504
        </td>
        <td>
          18589
          -
          18609
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.TableName.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.TableName.apply(n, schema)
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          507
        </td>
        <td>
          18646
          -
          18684
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.TableAndQueryMissingError](com.vertica.spark.util.error.TableAndQueryMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          506
        </td>
        <td>
          18646
          -
          18673
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.TableAndQueryMissingError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.TableAndQueryMissingError.apply()
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          508
        </td>
        <td>
          18826
          -
          18865
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTablename
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getTablename(config)
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          509
        </td>
        <td>
          18883
          -
          18921
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getDbSchema
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getDbSchema(config)
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          510
        </td>
        <td>
          18938
          -
          18973
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getQuery
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getQuery(config)
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          512
        </td>
        <td>
          19028
          -
          19057
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.TableName](com.vertica.spark.config.TableName.apply(n, schema)).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          511
        </td>
        <td>
          19028
          -
          19048
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.TableName.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.TableName.apply(n, schema)
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          514
        </td>
        <td>
          19088
          -
          19127
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.QuerySpecifiedOnWriteError](com.vertica.spark.util.error.QuerySpecifiedOnWriteError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          513
        </td>
        <td>
          19088
          -
          19116
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.QuerySpecifiedOnWriteError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.QuerySpecifiedOnWriteError.apply()
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          516
        </td>
        <td>
          19155
          -
          19189
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.TablenameMissingError](com.vertica.spark.util.error.TablenameMissingError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          515
        </td>
        <td>
          19155
          -
          19178
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.TablenameMissingError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.TablenameMissingError.apply()
        </td>
      </tr><tr>
        <td>
          526
        </td>
        <td>
          517
        </td>
        <td>
          19509
          -
          19558
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.core.DSReadConfigSetup])
        </td>
      </tr><tr>
        <td>
          534
        </td>
        <td>
          518
        </td>
        <td>
          19846
          -
          19870
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.SessionIdInterface.getId
        </td>
        <td style="background: #AEF1AE">
          DSReadConfigSetup.this.sessionIdInterface.getId
        </td>
      </tr><tr>
        <td>
          536
        </td>
        <td>
          529
        </td>
        <td>
          19886
          -
          20366
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple9.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple9.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.FileStoreConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.TableSource], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[Int]], cats.data.ValidatedNec[Nothing,None.type], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.ValidFilePermissions], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean]](DSConfigSetupUtils.validateAndGetJDBCConfig(config), DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId), DSConfigSetupUtils.validateAndGetTableSource(config), DSConfigSetupUtils.getPartitionCount(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing], DSConfigSetupUtils.getFilePermissions(config), DSConfigSetupUtils.getMaxRowGroupSize(config), DSConfigSetupUtils.getMaxFileSize(config), DSConfigSetupUtils.getTimeOperations(config))
        </td>
      </tr><tr>
        <td>
          537
        </td>
        <td>
          519
        </td>
        <td>
          19894
          -
          19945
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetJDBCConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetJDBCConfig(config)
        </td>
      </tr><tr>
        <td>
          538
        </td>
        <td>
          520
        </td>
        <td>
          19953
          -
          20020
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetFilestoreConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId)
        </td>
      </tr><tr>
        <td>
          539
        </td>
        <td>
          521
        </td>
        <td>
          20028
          -
          20080
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetTableSource
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetTableSource(config)
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          522
        </td>
        <td>
          20088
          -
          20132
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getPartitionCount
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getPartitionCount(config)
        </td>
      </tr><tr>
        <td>
          541
        </td>
        <td>
          523
        </td>
        <td>
          20140
          -
          20144
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          541
        </td>
        <td>
          524
        </td>
        <td>
          20140
          -
          20153
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          542
        </td>
        <td>
          525
        </td>
        <td>
          20161
          -
          20206
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getFilePermissions
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getFilePermissions(config)
        </td>
      </tr><tr>
        <td>
          543
        </td>
        <td>
          526
        </td>
        <td>
          20214
          -
          20259
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getMaxRowGroupSize
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getMaxRowGroupSize(config)
        </td>
      </tr><tr>
        <td>
          544
        </td>
        <td>
          527
        </td>
        <td>
          20267
          -
          20308
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getMaxFileSize
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getMaxFileSize(config)
        </td>
      </tr><tr>
        <td>
          545
        </td>
        <td>
          528
        </td>
        <td>
          20316
          -
          20360
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTimeOperations
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getTimeOperations(config)
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          532
        </td>
        <td>
          20371
          -
          20371
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          534
        </td>
        <td>
          20371
          -
          20371
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          531
        </td>
        <td>
          20371
          -
          20371
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          530
        </td>
        <td>
          20372
          -
          20403
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.DistributedFilesystemReadConfig
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          551
        </td>
        <td>
          19886
          -
          20840
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.data.Validated.andThen
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple9Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, com.vertica.spark.config.JDBCConfig, com.vertica.spark.config.FileStoreConfig, com.vertica.spark.config.TableSource, Option[Int], None.type, com.vertica.spark.config.ValidFilePermissions, Int, Int, Boolean](scala.Tuple9.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.FileStoreConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.TableSource], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[Int]], cats.data.ValidatedNec[Nothing,None.type], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.ValidFilePermissions], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Int], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean]](DSConfigSetupUtils.validateAndGetJDBCConfig(config), DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId), DSConfigSetupUtils.validateAndGetTableSource(config), DSConfigSetupUtils.getPartitionCount(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[None.type](scala.None).validNec[Nothing], DSConfigSetupUtils.getFilePermissions(config), DSConfigSetupUtils.getMaxRowGroupSize(config), DSConfigSetupUtils.getMaxFileSize(config), DSConfigSetupUtils.getTimeOperations(config))).mapN[com.vertica.spark.config.DistributedFilesystemReadConfig](com.vertica.spark.config.DistributedFilesystemReadConfig)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])).andThen[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError], com.vertica.spark.config.DistributedFilesystemReadConfig](((initialConfig: com.vertica.spark.config.DistributedFilesystemReadConfig) =&gt; {
  val pipe: com.vertica.spark.datasource.core.VerticaPipeInterface with com.vertica.spark.datasource.core.VerticaPipeReadInterface = DSReadConfigSetup.this.pipeFactory.getReadPipe(initialConfig);
  val metadata: com.vertica.spark.util.error.ErrorHandling.ConnectorResult[com.vertica.spark.config.VerticaMetadata] = pipe.getMetadata;
  metadata match {
    case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,com.vertica.spark.config.VerticaMetadata]((err @ _)) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.ConnectorError](err).invalidNec[Nothing]
    case (value: com.vertica.spark.config.VerticaMetadata)scala.util.Right[com.vertica.spark.util.error.ConnectorError,com.vertica.spark.config.VerticaMetadata]((meta @ _)) =&gt; meta match {
      case (readMeta @ (_: com.vertica.spark.config.VerticaReadMetadata)) =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.DistributedFilesystemReadConfig]({
  &lt;artifact&gt; val x$1: Some[com.vertica.spark.config.VerticaReadMetadata] @scala.reflect.internal.annotations.uncheckedBounds = scala.Some.apply[com.vertica.spark.config.VerticaReadMetadata](readMeta);
  &lt;artifact&gt; val x$2: com.vertica.spark.config.JDBCConfig = initialConfig.copy$default$1;
  &lt;artifact&gt; val x$3: com.vertica.spark.config.FileStoreConfig = initialConfig.copy$default$2;
  &lt;artifact&gt; val x$4: com.vertica.spark.config.TableSource = initialConfig.copy$default$3;
  &lt;artifact&gt; val x$5: Option[Int] @scala.reflect.internal.annotations.uncheckedBounds = initialConfig.copy$default$4;
  &lt;artifact&gt; val x$6: com.vertica.spark.config.ValidFilePermissions = initialConfig.copy$default$6;
  &lt;artifact&gt; val x$7: Int = initialConfig.copy$default$7;
  &lt;artifact&gt; val x$8: Int = initialConfig.copy$default$8;
  &lt;artifact&gt; val x$9: Boolean = initialConfig.copy$default$9;
  initialConfig.copy(x$2, x$3, x$4, x$5, x$1, x$6, x$7, x$8, x$9)
}).validNec[Nothing]
      case _ =&gt; cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.MissingMetadata](com.vertica.spark.util.error.MissingMetadata.apply()).invalidNec[Nothing]
    }
  }
}))
        </td>
      </tr><tr>
        <td>
          546
        </td>
        <td>
          533
        </td>
        <td>
          20371
          -
          20371
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          547
        </td>
        <td>
          535
        </td>
        <td>
          20449
          -
          20487
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.factory.VerticaPipeFactoryInterface.getReadPipe
        </td>
        <td style="background: #AEF1AE">
          DSReadConfigSetup.this.pipeFactory.getReadPipe(initialConfig)
        </td>
      </tr><tr>
        <td>
          550
        </td>
        <td>
          536
        </td>
        <td>
          20543
          -
          20559
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaPipeInterface.getMetadata
        </td>
        <td style="background: #AEF1AE">
          pipe.getMetadata
        </td>
      </tr><tr>
        <td>
          552
        </td>
        <td>
          537
        </td>
        <td>
          20609
          -
          20623
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.ConnectorError](err).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          541
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$3
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$3
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          540
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$2
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$2
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          543
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$6
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$6
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          546
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$9
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$9
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          548
        </td>
        <td>
          20713
          -
          20767
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.config.DistributedFilesystemReadConfig]({
  &lt;artifact&gt; val x$1: Some[com.vertica.spark.config.VerticaReadMetadata] @scala.reflect.internal.annotations.uncheckedBounds = scala.Some.apply[com.vertica.spark.config.VerticaReadMetadata](readMeta);
  &lt;artifact&gt; val x$2: com.vertica.spark.config.JDBCConfig = initialConfig.copy$default$1;
  &lt;artifact&gt; val x$3: com.vertica.spark.config.FileStoreConfig = initialConfig.copy$default$2;
  &lt;artifact&gt; val x$4: com.vertica.spark.config.TableSource = initialConfig.copy$default$3;
  &lt;artifact&gt; val x$5: Option[Int] @scala.reflect.internal.annotations.uncheckedBounds = initialConfig.copy$default$4;
  &lt;artifact&gt; val x$6: com.vertica.spark.config.ValidFilePermissions = initialConfig.copy$default$6;
  &lt;artifact&gt; val x$7: Int = initialConfig.copy$default$7;
  &lt;artifact&gt; val x$8: Int = initialConfig.copy$default$8;
  &lt;artifact&gt; val x$9: Boolean = initialConfig.copy$default$9;
  initialConfig.copy(x$2, x$3, x$4, x$5, x$1, x$6, x$7, x$8, x$9)
}).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          539
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$1
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$1
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          542
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$4
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$4
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          545
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$8
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$8
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          544
        </td>
        <td>
          20727
          -
          20727
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy$default$7
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy$default$7
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          547
        </td>
        <td>
          20713
          -
          20758
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.copy
        </td>
        <td style="background: #AEF1AE">
          initialConfig.copy(x$2, x$3, x$4, x$5, x$1, x$6, x$7, x$8, x$9)
        </td>
      </tr><tr>
        <td>
          554
        </td>
        <td>
          538
        </td>
        <td>
          20743
          -
          20757
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.config.VerticaReadMetadata](readMeta)
        </td>
      </tr><tr>
        <td>
          555
        </td>
        <td>
          550
        </td>
        <td>
          20788
          -
          20816
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.MissingMetadata](com.vertica.spark.util.error.MissingMetadata.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          555
        </td>
        <td>
          549
        </td>
        <td>
          20788
          -
          20805
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingMetadata.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MissingMetadata.apply()
        </td>
      </tr><tr>
        <td>
          561
        </td>
        <td>
          552
        </td>
        <td>
          20972
          -
          20978
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReadConfigSetup.logger
        </td>
        <td style="background: #AEF1AE">
          DSReadConfigSetup.this.logger
        </td>
      </tr><tr>
        <td>
          561
        </td>
        <td>
          553
        </td>
        <td>
          20900
          -
          20979
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions[com.vertica.spark.config.DistributedFilesystemReadConfig](config, res, DSReadConfigSetup.this.logger)
        </td>
      </tr><tr>
        <td>
          571
        </td>
        <td>
          554
        </td>
        <td>
          21426
          -
          21474
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaPipeReadInterface.doPreReadSteps
        </td>
        <td style="background: #F0ADAD">
          DSReadConfigSetup.this.pipeFactory.getReadPipe(config).doPreReadSteps()
        </td>
      </tr><tr>
        <td>
          572
        </td>
        <td>
          555
        </td>
        <td>
          21524
          -
          21543
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #F0ADAD">
          scala.Some.apply[com.vertica.spark.datasource.core.PartitionInfo](partitionInfo)
        </td>
      </tr><tr>
        <td>
          572
        </td>
        <td>
          556
        </td>
        <td>
          21518
          -
          21544
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Some[com.vertica.spark.datasource.core.PartitionInfo]](scala.Some.apply[com.vertica.spark.datasource.core.PartitionInfo](partitionInfo))
        </td>
      </tr><tr>
        <td>
          573
        </td>
        <td>
          557
        </td>
        <td>
          21569
          -
          21578
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          587
        </td>
        <td>
          559
        </td>
        <td>
          22102
          -
          22130
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaDiscoveryError, Nothing](com.vertica.spark.util.error.SchemaDiscoveryError.apply())
        </td>
      </tr><tr>
        <td>
          587
        </td>
        <td>
          558
        </td>
        <td>
          22107
          -
          22129
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.SchemaDiscoveryError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.SchemaDiscoveryError.apply()
        </td>
      </tr><tr>
        <td>
          588
        </td>
        <td>
          561
        </td>
        <td>
          22164
          -
          22186
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, org.apache.spark.sql.types.StructType](metadata.schema)
        </td>
      </tr><tr>
        <td>
          588
        </td>
        <td>
          560
        </td>
        <td>
          22170
          -
          22185
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.VerticaReadMetadata.schema
        </td>
        <td style="background: #F0ADAD">
          metadata.schema
        </td>
      </tr><tr>
        <td>
          598
        </td>
        <td>
          562
        </td>
        <td>
          22544
          -
          22594
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.core.DSWriteConfigSetup])
        </td>
      </tr><tr>
        <td>
          606
        </td>
        <td>
          563
        </td>
        <td>
          22872
          -
          22896
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.SessionIdInterface.getId
        </td>
        <td style="background: #AEF1AE">
          DSWriteConfigSetup.this.sessionIdInterface.getId
        </td>
      </tr><tr>
        <td>
          609
        </td>
        <td>
          564
        </td>
        <td>
          23093
          -
          23099
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.DSWriteConfigSetup.schema
        </td>
        <td style="background: #AEF1AE">
          DSWriteConfigSetup.this.schema
        </td>
      </tr><tr>
        <td>
          611
        </td>
        <td>
          579
        </td>
        <td>
          23151
          -
          23946
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple14.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple14.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.FileStoreConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.TableName], cats.data.ValidatedNec[Nothing,org.apache.spark.sql.types.StructType], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Long], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.ValidColumnList]], cats.data.ValidatedNec[Nothing,String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Float], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.ValidFilePermissions], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.datasource.core.CreateExternalTableOption]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.ValidColumnList]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean]](DSConfigSetupUtils.validateAndGetJDBCConfig(config), DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId), DSConfigSetupUtils.validateAndGetFullTableName(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[org.apache.spark.sql.types.StructType](passedInSchema).validNec[Nothing], DSConfigSetupUtils.getStrLen(config), DSConfigSetupUtils.getTargetTableSQL(config), DSConfigSetupUtils.getCopyColumnList(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing], DSConfigSetupUtils.getFailedRowsPercentTolerance(config), DSConfigSetupUtils.getFilePermissions(config), DSConfigSetupUtils.getCreateExternalTable(config), DSConfigSetupUtils.getSaveJobStatusTable(config), DSConfigSetupUtils.getMergeKey(config), DSConfigSetupUtils.getTimeOperations(config))
        </td>
      </tr><tr>
        <td>
          612
        </td>
        <td>
          565
        </td>
        <td>
          23163
          -
          23214
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetJDBCConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetJDBCConfig(config)
        </td>
      </tr><tr>
        <td>
          613
        </td>
        <td>
          566
        </td>
        <td>
          23226
          -
          23293
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetFilestoreConfig
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId)
        </td>
      </tr><tr>
        <td>
          614
        </td>
        <td>
          567
        </td>
        <td>
          23305
          -
          23359
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.validateAndGetFullTableName
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.validateAndGetFullTableName(config)
        </td>
      </tr><tr>
        <td>
          615
        </td>
        <td>
          568
        </td>
        <td>
          23371
          -
          23394
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[org.apache.spark.sql.types.StructType](passedInSchema).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          616
        </td>
        <td>
          569
        </td>
        <td>
          23406
          -
          23442
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getStrLen
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getStrLen(config)
        </td>
      </tr><tr>
        <td>
          617
        </td>
        <td>
          570
        </td>
        <td>
          23454
          -
          23498
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTargetTableSQL
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getTargetTableSQL(config)
        </td>
      </tr><tr>
        <td>
          618
        </td>
        <td>
          571
        </td>
        <td>
          23510
          -
          23554
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getCopyColumnList
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getCopyColumnList(config)
        </td>
      </tr><tr>
        <td>
          619
        </td>
        <td>
          572
        </td>
        <td>
          23566
          -
          23584
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.validNec
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing]
        </td>
      </tr><tr>
        <td>
          620
        </td>
        <td>
          573
        </td>
        <td>
          23596
          -
          23652
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getFailedRowsPercentTolerance
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getFailedRowsPercentTolerance(config)
        </td>
      </tr><tr>
        <td>
          621
        </td>
        <td>
          574
        </td>
        <td>
          23664
          -
          23709
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getFilePermissions
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getFilePermissions(config)
        </td>
      </tr><tr>
        <td>
          622
        </td>
        <td>
          575
        </td>
        <td>
          23721
          -
          23770
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getCreateExternalTable
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getCreateExternalTable(config)
        </td>
      </tr><tr>
        <td>
          623
        </td>
        <td>
          576
        </td>
        <td>
          23782
          -
          23830
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getSaveJobStatusTable
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getSaveJobStatusTable(config)
        </td>
      </tr><tr>
        <td>
          624
        </td>
        <td>
          577
        </td>
        <td>
          23842
          -
          23880
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getMergeKey
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getMergeKey(config)
        </td>
      </tr><tr>
        <td>
          625
        </td>
        <td>
          578
        </td>
        <td>
          23892
          -
          23936
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.getTimeOperations
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.getTimeOperations(config)
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          585
        </td>
        <td>
          23151
          -
          23985
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.syntax.Tuple14SemigroupalOps.mapN
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxTuple14Semigroupal[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult, com.vertica.spark.config.JDBCConfig, com.vertica.spark.config.FileStoreConfig, com.vertica.spark.config.TableName, org.apache.spark.sql.types.StructType, Long, Option[String], Option[com.vertica.spark.config.ValidColumnList], String, Float, com.vertica.spark.config.ValidFilePermissions, Option[com.vertica.spark.datasource.core.CreateExternalTableOption], Boolean, Option[com.vertica.spark.config.ValidColumnList], Boolean](scala.Tuple14.apply[com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.JDBCConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.FileStoreConfig], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.TableName], cats.data.ValidatedNec[Nothing,org.apache.spark.sql.types.StructType], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Long], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[String]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.ValidColumnList]], cats.data.ValidatedNec[Nothing,String], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Float], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[com.vertica.spark.config.ValidFilePermissions], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.datasource.core.CreateExternalTableOption]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Option[com.vertica.spark.config.ValidColumnList]], com.vertica.spark.datasource.core.DSConfigSetupUtils.ValidationResult[Boolean]](DSConfigSetupUtils.validateAndGetJDBCConfig(config), DSConfigSetupUtils.validateAndGetFilestoreConfig(config, sessionId), DSConfigSetupUtils.validateAndGetFullTableName(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[org.apache.spark.sql.types.StructType](passedInSchema).validNec[Nothing], DSConfigSetupUtils.getStrLen(config), DSConfigSetupUtils.getTargetTableSQL(config), DSConfigSetupUtils.getCopyColumnList(config), cats.implicits.catsSyntaxValidatedIdBinCompat0[String](sessionId).validNec[Nothing], DSConfigSetupUtils.getFailedRowsPercentTolerance(config), DSConfigSetupUtils.getFilePermissions(config), DSConfigSetupUtils.getCreateExternalTable(config), DSConfigSetupUtils.getSaveJobStatusTable(config), DSConfigSetupUtils.getMergeKey(config), DSConfigSetupUtils.getTimeOperations(config))).mapN[com.vertica.spark.config.DistributedFilesystemWriteConfig](com.vertica.spark.config.DistributedFilesystemWriteConfig)(cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]), cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]))
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          582
        </td>
        <td>
          23951
          -
          23951
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          581
        </td>
        <td>
          23951
          -
          23951
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          584
        </td>
        <td>
          23951
          -
          23951
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          cats.data.ValidatedInstances.catsDataApplicativeErrorForValidated
        </td>
        <td style="background: #AEF1AE">
          cats.data.Validated.catsDataApplicativeErrorForValidated[cats.data.NonEmptyChain[com.vertica.spark.util.error.ConnectorError]](data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError])
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          580
        </td>
        <td>
          23952
          -
          23984
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemWriteConfig
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.DistributedFilesystemWriteConfig
        </td>
      </tr><tr>
        <td>
          626
        </td>
        <td>
          583
        </td>
        <td>
          23951
          -
          23951
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.data.NonEmptyChainInstances.catsDataSemigroupForNonEmptyChain
        </td>
        <td style="background: #AEF1AE">
          data.this.NonEmptyChainImpl.catsDataSemigroupForNonEmptyChain[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          628
        </td>
        <td>
          587
        </td>
        <td>
          24013
          -
          24044
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.syntax.ValidatedIdOpsBinCompat0.invalidNec
        </td>
        <td style="background: #F0ADAD">
          cats.implicits.catsSyntaxValidatedIdBinCompat0[com.vertica.spark.util.error.MissingSchemaError](com.vertica.spark.util.error.MissingSchemaError.apply()).invalidNec[Nothing]
        </td>
      </tr><tr>
        <td>
          628
        </td>
        <td>
          586
        </td>
        <td>
          24013
          -
          24033
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.MissingSchemaError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.MissingSchemaError.apply()
        </td>
      </tr><tr>
        <td>
          632
        </td>
        <td>
          588
        </td>
        <td>
          24182
          -
          24188
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.DSWriteConfigSetup.logger
        </td>
        <td style="background: #AEF1AE">
          DSWriteConfigSetup.this.logger
        </td>
      </tr><tr>
        <td>
          632
        </td>
        <td>
          589
        </td>
        <td>
          24110
          -
          24189
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions
        </td>
        <td style="background: #AEF1AE">
          DSConfigSetupUtils.logOrAppendErrorsForOldConnectorOptions[com.vertica.spark.config.DistributedFilesystemWriteConfig](config, res, DSWriteConfigSetup.this.logger)
        </td>
      </tr><tr>
        <td>
          641
        </td>
        <td>
          590
        </td>
        <td>
          24445
          -
          24477
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.factory.VerticaPipeFactoryInterface.getWritePipe
        </td>
        <td style="background: #F0ADAD">
          DSWriteConfigSetup.this.pipeFactory.getWritePipe(config)
        </td>
      </tr><tr>
        <td>
          642
        </td>
        <td>
          591
        </td>
        <td>
          24482
          -
          24504
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaPipeWriteInterface.doPreWriteSteps
        </td>
        <td style="background: #F0ADAD">
          pipe.doPreWriteSteps()
        </td>
      </tr><tr>
        <td>
          643
        </td>
        <td>
          592
        </td>
        <td>
          24537
          -
          24546
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          644
        </td>
        <td>
          594
        </td>
        <td>
          24570
          -
          24581
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, None.type](scala.None)
        </td>
      </tr><tr>
        <td>
          644
        </td>
        <td>
          593
        </td>
        <td>
          24576
          -
          24580
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          651
        </td>
        <td>
          595
        </td>
        <td>
          24750
          -
          24761
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.DSWriteConfigSetup.schema
        </td>
        <td style="background: #F0ADAD">
          this.schema
        </td>
      </tr><tr>
        <td>
          652
        </td>
        <td>
          596
        </td>
        <td>
          24795
          -
          24808
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, org.apache.spark.sql.types.StructType](schema)
        </td>
      </tr><tr>
        <td>
          653
        </td>
        <td>
          597
        </td>
        <td>
          24831
          -
          24853
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.SchemaDiscoveryError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.SchemaDiscoveryError.apply()
        </td>
      </tr><tr>
        <td>
          653
        </td>
        <td>
          598
        </td>
        <td>
          24826
          -
          24854
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.SchemaDiscoveryError, Nothing](com.vertica.spark.util.error.SchemaDiscoveryError.apply())
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>