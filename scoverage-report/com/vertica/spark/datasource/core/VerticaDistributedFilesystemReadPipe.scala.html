<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/vertica/spark/datasource/core/VerticaDistributedFilesystemReadPipe.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>// (c) Copyright [2020-2021] Micro Focus or one of its affiliates.
</span>2 <span style=''>// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
</span>3 <span style=''>// You may not use this file except in compliance with the License.
</span>4 <span style=''>// You may obtain a copy of the License at
</span>5 <span style=''>//
</span>6 <span style=''>// http://www.apache.org/licenses/LICENSE-2.0
</span>7 <span style=''>//
</span>8 <span style=''>// Unless required by applicable law or agreed to in writing, software
</span>9 <span style=''>// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
</span>10 <span style=''>// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span>11 <span style=''>// See the License for the specific language governing permissions and
</span>12 <span style=''>// limitations under the License.
</span>13 <span style=''>
</span>14 <span style=''>package com.vertica.spark.datasource.core
</span>15 <span style=''>
</span>16 <span style=''>import com.typesafe.scalalogging.Logger
</span>17 <span style=''>import com.vertica.spark.util.error._
</span>18 <span style=''>import com.vertica.spark.config._
</span>19 <span style=''>import com.vertica.spark.datasource.jdbc._
</span>20 <span style=''>import cats.implicits._
</span>21 <span style=''>import com.vertica.spark.util.schema.SchemaToolsInterface
</span>22 <span style=''>import com.vertica.spark.datasource.fs._
</span>23 <span style=''>import com.vertica.spark.datasource.v2.PushdownFilter
</span>24 <span style=''>import com.vertica.spark.util.Timer
</span>25 <span style=''>import com.vertica.spark.util.cleanup.{CleanupUtilsInterface, FileCleanupInfo}
</span>26 <span style=''>import com.vertica.spark.util.error.ErrorHandling.ConnectorResult
</span>27 <span style=''>import org.apache.spark.sql.connector.expressions.aggregate._
</span>28 <span style=''>import com.vertica.spark.util.listeners.{ApplicationParquetCleaner, SparkContextWrapper}
</span>29 <span style=''>import org.apache.spark.sql.connector.read.InputPartition
</span>30 <span style=''>import org.apache.spark.sql.types.StructType
</span>31 <span style=''>
</span>32 <span style=''>/**
</span>33 <span style=''> * Represents a portion of a parquet file
</span>34 <span style=''> *
</span>35 <span style=''> * @param filename Full path with name of the parquet file
</span>36 <span style=''> * @param minRowGroup First row group to read from parquet file
</span>37 <span style=''> * @param maxRowGroup Last row group to read from parquet file
</span>38 <span style=''> * @param rangeIdx Range index for this file. Used to track access to this file / cleanup among different nodes. If there are three ranges for a given file this will be a value between 0 and 2
</span>39 <span style=''> */
</span>40 <span style=''>final case class ParquetFileRange(filename: String, minRowGroup: Int, maxRowGroup: Int, rangeIdx: Option[Int] = None)
</span>41 <span style=''>
</span>42 <span style=''>/**
</span>43 <span style=''> * Partition for distributed filesystem transport method using parquet files
</span>44 <span style=''> *
</span>45 <span style=''> * @param fileRanges List of files and ranges of row groups to read for those files
</span>46 <span style=''> * @param rangeCountMap Map representing how many file ranges exist for each file. Used for tracking and cleanup.
</span>47 <span style=''> */
</span>48 <span style=''>final case class VerticaDistributedFilesystemPartition(fileRanges: Seq[ParquetFileRange], rangeCountMap: Option[Map[String, Int]] = None) extends VerticaPartition
</span>49 <span style=''>
</span>50 <span style=''>
</span>51 <span style=''>/**
</span>52 <span style=''> * Implementation of the pipe to Vertica using a distributed filesystem as an intermediary layer.
</span>53 <span style=''> *
</span>54 <span style=''> * @param config Specific configuration data for read using intermediary filesystem
</span>55 <span style=''> * @param fileStoreLayer Dependency for communication with the intermediary filesystem
</span>56 <span style=''> * @param jdbcLayer Dependency for communication with the database over JDBC
</span>57 <span style=''> * @param schemaTools Dependency for schema conversion between Vertica and Spark
</span>58 <span style=''> * @param cleanupUtils Depedency for handling cleanup of files used in intermediary filesystem
</span>59 <span style=''> */
</span>60 <span style=''>class VerticaDistributedFilesystemReadPipe(
</span>61 <span style=''>                                            val config: DistributedFilesystemReadConfig,
</span>62 <span style=''>                                            val fileStoreLayer: FileStoreLayerInterface,
</span>63 <span style=''>                                            val jdbcLayer: JdbcLayerInterface,
</span>64 <span style=''>                                            val schemaTools: SchemaToolsInterface,
</span>65 <span style=''>                                            val cleanupUtils: CleanupUtilsInterface,
</span>66 <span style=''>                                            val sparkContext: SparkContextWrapper,
</span>67 <span style=''>                                            val dataSize: Int = 1
</span>68 <span style=''>                                          ) extends VerticaPipeInterface with VerticaPipeReadInterface {
</span>69 <span style=''>  private val logger: Logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[VerticaDistributedFilesystemReadPipe])</span><span style=''>
</span>70 <span style=''>
</span>71 <span style=''>  // File size params. The max size of a single file, and the max size of an individual row group inside the parquet file.
</span>72 <span style=''>  // TODO: Tune these with performance tests. Determine whether a single value is suitable or if we need to add a user option.
</span>73 <span style=''>  private val maxFileSize = </span><span style='background: #AEF1AE'>config.maxFileSize</span><span style=''>
</span>74 <span style=''>  private val maxRowGroupSize = </span><span style='background: #AEF1AE'>config.maxRowGroupSize</span><span style=''>
</span>75 <span style=''>
</span>76 <span style=''>
</span>77 <span style=''>  private def retrieveMetadata(): ConnectorResult[VerticaMetadata] = {
</span>78 <span style=''>    </span><span style='background: #AEF1AE'>schemaTools.readSchema(this.jdbcLayer, this.config.tableSource)</span><span style=''> match {
</span>79 <span style=''>      case Right(schema) =&gt; </span><span style='background: #AEF1AE'>Right(VerticaReadMetadata(schema))</span><span style=''>
</span>80 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>Left(err)</span><span style=''>
</span>81 <span style=''>    }
</span>82 <span style=''>  }
</span>83 <span style=''>
</span>84 <span style=''>  private def addPushdownFilters(pushdownFilters: List[PushdownFilter]): String = {
</span>85 <span style=''>    pushdownFilters match {
</span>86 <span style=''>      case Nil =&gt; </span><span style='background: #AEF1AE'>&quot;&quot;</span><span style=''>
</span>87 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>&quot; WHERE &quot; + pushdownFilters.map(_.getFilterString).mkString(&quot; AND &quot;)</span><span style=''>
</span>88 <span style=''>    }
</span>89 <span style=''>  }
</span>90 <span style=''>
</span>91 <span style=''>  /**
</span>92 <span style=''>   * Gets metadata, either cached in configuration object or retrieved from Vertica if we haven't yet.
</span>93 <span style=''>   *
</span>94 <span style=''>   * At current time, the only metadata is the table schema.
</span>95 <span style=''>   */
</span>96 <span style=''>  override def getMetadata: ConnectorResult[VerticaMetadata] = {
</span>97 <span style=''>    </span><span style='background: #AEF1AE'>this.config.metadata</span><span style=''> match {
</span>98 <span style=''>      case Some(data) =&gt; </span><span style='background: #AEF1AE'>Right(data)</span><span style=''>
</span>99 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>this.retrieveMetadata()</span><span style=''>
</span>100 <span style=''>    }
</span>101 <span style=''>  }
</span>102 <span style=''>
</span>103 <span style=''>  /**
</span>104 <span style=''>   * Returns the default number of rows to read/write from this pipe at a time.
</span>105 <span style=''>   */
</span>106 <span style=''>  override def getDataBlockSize: ConnectorResult[Long] = </span><span style='background: #F0ADAD'>Right(dataSize)</span><span style=''>
</span>107 <span style=''>
</span>108 <span style=''>  /**
</span>109 <span style=''>   * Increments a count for a given file and returns an index (count - 1)
</span>110 <span style=''>   */
</span>111 <span style=''>  private def incrementRangeMapGetIndex(map: scala.collection.mutable.Map[String, Int], filename: String) : Int = {
</span>112 <span style=''>    if(</span><span style='background: #AEF1AE'>!map.contains(filename)</span><span style=''>){
</span>113 <span style=''>      </span><span style='background: #AEF1AE'>map(filename) = 1</span><span style=''>
</span>114 <span style=''>    }
</span>115 <span style=''>    else {
</span>116 <span style=''>      </span><span style='background: #AEF1AE'>map(filename) += 1</span><span style=''>
</span>117 <span style=''>    }
</span>118 <span style=''>    </span><span style='background: #AEF1AE'>map(filename) - 1</span><span style=''>
</span>119 <span style=''>  }
</span>120 <span style=''>
</span>121 <span style=''>  private def getPartitionInfo(fileMetadata: Seq[ParquetFileMetadata], partitionCount: Int): PartitionInfo = {
</span>122 <span style=''>    val totalRowGroups = </span><span style='background: #AEF1AE'>fileMetadata.map(_.rowGroupCount).sum</span><span style=''>
</span>123 <span style=''>
</span>124 <span style=''>    // If no data, return empty partition list
</span>125 <span style=''>    if(</span><span style='background: #AEF1AE'>totalRowGroups == 0</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>126 <span style=''></span><span style='background: #AEF1AE'>      logger.info(&quot;No data. Returning empty partition list.&quot;)
</span>127 <span style=''></span><span style='background: #AEF1AE'>      PartitionInfo(Array[InputPartition]())
</span>128 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>129 <span style=''>    else </span><span style='background: #AEF1AE'>{
</span>130 <span style=''></span><span style='background: #AEF1AE'>      val extraSpace = if(totalRowGroups % partitionCount == 0) 0 else 1
</span>131 <span style=''></span><span style='background: #AEF1AE'>      val rowGroupRoom = (totalRowGroups / partitionCount) + extraSpace
</span>132 <span style=''></span><span style='background: #AEF1AE'>
</span>133 <span style=''></span><span style='background: #AEF1AE'>      // Now, create partitions splitting up files roughly evenly
</span>134 <span style=''></span><span style='background: #AEF1AE'>      var i = 0
</span>135 <span style=''></span><span style='background: #AEF1AE'>      var partitions = List[VerticaDistributedFilesystemPartition]()
</span>136 <span style=''></span><span style='background: #AEF1AE'>      var curFileRanges = List[ParquetFileRange]()
</span>137 <span style=''></span><span style='background: #AEF1AE'>      val rangeCountMap = scala.collection.mutable.Map[String, Int]()
</span>138 <span style=''></span><span style='background: #AEF1AE'>
</span>139 <span style=''></span><span style='background: #AEF1AE'>      logger.info(&quot;Creating partitions.&quot;)
</span>140 <span style=''></span><span style='background: #AEF1AE'>      for(m &lt;- fileMetadata) {
</span>141 <span style=''></span><span style='background: #AEF1AE'>        val size = m.rowGroupCount
</span>142 <span style=''></span><span style='background: #AEF1AE'>        logger.debug(&quot;Splitting file &quot; + m.filename + &quot; with row group count &quot; + size)
</span>143 <span style=''></span><span style='background: #AEF1AE'>        var j = 0
</span>144 <span style=''></span><span style='background: #AEF1AE'>        var low = 0
</span>145 <span style=''></span><span style='background: #AEF1AE'>        while(j &lt; size){
</span>146 <span style=''></span><span style='background: #AEF1AE'>          if(i == rowGroupRoom-1){ // Reached end of partition, cut off here
</span>147 <span style=''></span><span style='background: #AEF1AE'>            val rangeIdx = incrementRangeMapGetIndex(rangeCountMap, m.filename)
</span>148 <span style=''></span><span style='background: #AEF1AE'>
</span>149 <span style=''></span><span style='background: #AEF1AE'>            val frange = ParquetFileRange(m.filename, low, j, Some(rangeIdx))
</span>150 <span style=''></span><span style='background: #AEF1AE'>
</span>151 <span style=''></span><span style='background: #AEF1AE'>            curFileRanges = curFileRanges :+ frange
</span>152 <span style=''></span><span style='background: #AEF1AE'>            val partition = VerticaDistributedFilesystemPartition(curFileRanges)
</span>153 <span style=''></span><span style='background: #AEF1AE'>            partitions = partitions :+ partition
</span>154 <span style=''></span><span style='background: #AEF1AE'>            curFileRanges = List[ParquetFileRange]()
</span>155 <span style=''></span><span style='background: #AEF1AE'>            logger.debug(&quot;Reached partition with file &quot; + m.filename + &quot; , range low: &quot; +
</span>156 <span style=''></span><span style='background: #AEF1AE'>              low + &quot; , range high: &quot; + j + &quot; , idx: &quot; + rangeIdx)
</span>157 <span style=''></span><span style='background: #AEF1AE'>            i = 0
</span>158 <span style=''></span><span style='background: #AEF1AE'>            low = j + 1
</span>159 <span style=''></span><span style='background: #AEF1AE'>          }
</span>160 <span style=''></span><span style='background: #AEF1AE'>          else if(j == size - 1){ // Reached end of file's row groups, add to file ranges
</span>161 <span style=''></span><span style='background: #AEF1AE'>            val rangeIdx = incrementRangeMapGetIndex(rangeCountMap, m.filename)
</span>162 <span style=''></span><span style='background: #AEF1AE'>            val frange = ParquetFileRange(m.filename, low, j, Some(rangeIdx))
</span>163 <span style=''></span><span style='background: #AEF1AE'>            curFileRanges = curFileRanges :+ frange
</span>164 <span style=''></span><span style='background: #AEF1AE'>            logger.debug(&quot;Reached end of file &quot; + m.filename + &quot; , range low: &quot; +
</span>165 <span style=''></span><span style='background: #AEF1AE'>              low + &quot; , range high: &quot; + j + &quot; , idx: &quot; + rangeIdx)
</span>166 <span style=''></span><span style='background: #AEF1AE'>            i += 1
</span>167 <span style=''></span><span style='background: #AEF1AE'>          }
</span>168 <span style=''></span><span style='background: #AEF1AE'>          else {
</span>169 <span style=''></span><span style='background: #AEF1AE'>            i += 1
</span>170 <span style=''></span><span style='background: #AEF1AE'>          }
</span>171 <span style=''></span><span style='background: #AEF1AE'>          j += 1
</span>172 <span style=''></span><span style='background: #AEF1AE'>        }
</span>173 <span style=''></span><span style='background: #AEF1AE'>      }
</span>174 <span style=''></span><span style='background: #AEF1AE'>
</span>175 <span style=''></span><span style='background: #AEF1AE'>      // Last partition if leftover (only partition not of rowGroupRoom size)
</span>176 <span style=''></span><span style='background: #AEF1AE'>      if(curFileRanges.nonEmpty) {
</span>177 <span style=''></span><span style='background: #AEF1AE'>        val partition = VerticaDistributedFilesystemPartition(curFileRanges)
</span>178 <span style=''></span><span style='background: #AEF1AE'>        partitions = partitions :+ partition
</span>179 <span style=''></span><span style='background: #AEF1AE'>      }
</span>180 <span style=''></span><span style='background: #AEF1AE'>
</span>181 <span style=''></span><span style='background: #AEF1AE'>      // Add range count map info to partition
</span>182 <span style=''></span><span style='background: #AEF1AE'>      partitions = partitions.map(part =&gt; part.copy(rangeCountMap = Some(rangeCountMap.toMap)))
</span>183 <span style=''></span><span style='background: #AEF1AE'>
</span>184 <span style=''></span><span style='background: #AEF1AE'>      PartitionInfo(partitions.toArray)
</span>185 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>186 <span style=''>  }
</span>187 <span style=''>
</span>188 <span style=''>  private def getSelectClause: ConnectorResult[String] = {
</span>189 <span style=''>    if(</span><span style='background: #AEF1AE'>config.isAggPushedDown</span><span style=''>) </span><span style='background: #F0ADAD'>getPushdownAggregateColumns</span><span style=''> else </span><span style='background: #AEF1AE'>getColumnNames(this.config.getRequiredSchema)</span><span style=''>
</span>190 <span style=''>  }
</span>191 <span style=''>
</span>192 <span style=''>  private def getPushdownAggregateColumns: ConnectorResult[String] = {
</span>193 <span style=''>    val selectClause = </span><span style='background: #F0ADAD'>this.config.getRequiredSchema.map(col =&gt; s&quot;${col.name} as &quot; + &quot;\&quot;&quot; + col.name + &quot;\&quot;&quot;).mkString(&quot;, &quot;)</span><span style=''>
</span>194 <span style=''>    </span><span style='background: #F0ADAD'>Right(selectClause)</span><span style=''>
</span>195 <span style=''>  }
</span>196 <span style=''>
</span>197 <span style=''>  private def getGroupbyClause: String = {
</span>198 <span style=''>    if(</span><span style='background: #AEF1AE'>this.config.getGroupBy.nonEmpty</span><span style=''>) </span><span style='background: #F0ADAD'>&quot; GROUP BY &quot; + this.config.getGroupBy.map(_.name).mkString(&quot;, &quot;)</span><span style=''> else </span><span style='background: #AEF1AE'>&quot;&quot;</span><span style=''>
</span>199 <span style=''>  }
</span>200 <span style=''>
</span>201 <span style=''>
</span>202 <span style=''>  private def getColumnNames(requiredSchema: StructType): ConnectorResult[String] = {
</span>203 <span style=''>    </span><span style='background: #AEF1AE'>schemaTools.getColumnInfo(jdbcLayer, config.tableSource)</span><span style=''> match {
</span>204 <span style=''>      case Left(err) =&gt; </span><span style='background: #F0ADAD'>Left(err.context(&quot;Failed to get table schema when checking for fields that need casts.&quot;))</span><span style=''>
</span>205 <span style=''>      case Right(columnDefs) =&gt; </span><span style='background: #AEF1AE'>Right(schemaTools.makeColumnsString(columnDefs, requiredSchema))</span><span style=''>
</span>206 <span style=''>    }
</span>207 <span style=''>  }
</span>208 <span style=''>
</span>209 <span style=''>  /**
</span>210 <span style=''>   * Initial setup for the whole read operation. Called by driver.
</span>211 <span style=''>   *
</span>212 <span style=''>   * Will:
</span>213 <span style=''>   * - Create a unique directory to export to
</span>214 <span style=''>   * - Tell Vertica to export to a subdirectory there
</span>215 <span style=''>   * - Parse the list of files Vertica exported, and create a list of PartitionInfo structs representing how to split up the read.
</span>216 <span style=''>   *
</span>217 <span style=''>   * Partitioning info is composed af a group of file ranges. A file range could represent reading a whole file or part of one.
</span>218 <span style=''>   * This way a single file could be split up among 10 partitions to read, or a single partition could read 10 files.
</span>219 <span style=''>   */
</span>220 <span style=''>  override def doPreReadSteps(): ConnectorResult[PartitionInfo] = {
</span>221 <span style=''>    val fileStoreConfig = </span><span style='background: #AEF1AE'>config.fileStoreConfig</span><span style=''>
</span>222 <span style=''>    val delimiter = if(</span><span style='background: #AEF1AE'>fileStoreConfig.address.takeRight(1) == &quot;/&quot; || fileStoreConfig.address.takeRight(1) == &quot;\\&quot;</span><span style=''>) </span><span style='background: #F0ADAD'>&quot;&quot;</span><span style=''> else </span><span style='background: #AEF1AE'>&quot;/&quot;</span><span style=''>
</span>223 <span style=''>    val hdfsPath = </span><span style='background: #AEF1AE'>fileStoreConfig.address + delimiter + config.tableSource.identifier</span><span style=''>
</span>224 <span style=''>    logger.debug(&quot;Export path: &quot; + hdfsPath)
</span>225 <span style=''>
</span>226 <span style=''>    val ret: ConnectorResult[PartitionInfo] = </span><span style='background: #AEF1AE'>for {
</span>227 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- getMetadata
</span>228 <span style=''></span><span style='background: #AEF1AE'>
</span>229 <span style=''></span><span style='background: #AEF1AE'>      // Set Vertica to work with kerberos and HDFS/AWS
</span>230 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- jdbcLayer.configureSession(fileStoreLayer)
</span>231 <span style=''></span><span style='background: #AEF1AE'>
</span>232 <span style=''></span><span style='background: #AEF1AE'>      // Create unique directory for session
</span>233 <span style=''></span><span style='background: #AEF1AE'>      perm = config.filePermissions
</span>234 <span style=''></span><span style='background: #AEF1AE'>
</span>235 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Creating unique directory: &quot; + fileStoreConfig.address + &quot; with permissions: &quot; + perm)
</span>236 <span style=''></span><span style='background: #AEF1AE'>
</span>237 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- fileStoreLayer.createDir(fileStoreConfig.address, perm.toString) match {
</span>238 <span style=''></span><span style='background: #AEF1AE'>        case Left(err) =&gt;
</span>239 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>err.getUnderlyingError</span><span style='background: #AEF1AE'> match {
</span>240 <span style=''></span><span style='background: #AEF1AE'>            case CreateDirectoryAlreadyExistsError(_) =&gt;
</span>241 <span style=''></span><span style='background: #AEF1AE'>              logger.info(&quot;Directory already existed: &quot; + fileStoreConfig.address)
</span>242 <span style=''></span><span style='background: #AEF1AE'>              </span><span style='background: #F0ADAD'>Right(())</span><span style='background: #AEF1AE'>
</span>243 <span style=''></span><span style='background: #AEF1AE'>            case _ =&gt; </span><span style='background: #F0ADAD'>Left(err.context(&quot;Failed to create directory: &quot; + fileStoreConfig.address))</span><span style='background: #AEF1AE'>
</span>244 <span style=''></span><span style='background: #AEF1AE'>          }
</span>245 <span style=''></span><span style='background: #AEF1AE'>        case Right(_) =&gt; Right(())
</span>246 <span style=''></span><span style='background: #AEF1AE'>      }
</span>247 <span style=''></span><span style='background: #AEF1AE'>
</span>248 <span style=''></span><span style='background: #AEF1AE'>      // Check if export is already done (previous call of this function)
</span>249 <span style=''></span><span style='background: #AEF1AE'>      exportDone &lt;- fileStoreLayer.fileExists(hdfsPath)
</span>250 <span style=''></span><span style='background: #AEF1AE'>
</span>251 <span style=''></span><span style='background: #AEF1AE'>      // File permissions.
</span>252 <span style=''></span><span style='background: #AEF1AE'>      filePermissions = config.filePermissions
</span>253 <span style=''></span><span style='background: #AEF1AE'>
</span>254 <span style=''></span><span style='background: #AEF1AE'>      selectClause &lt;- this.getSelectClause
</span>255 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Select clause requested: &quot; + selectClause)
</span>256 <span style=''></span><span style='background: #AEF1AE'>
</span>257 <span style=''></span><span style='background: #AEF1AE'>      groupbyClause = this.getGroupbyClause
</span>258 <span style=''></span><span style='background: #AEF1AE'>
</span>259 <span style=''></span><span style='background: #AEF1AE'>      pushdownFilters = this.addPushdownFilters(this.config.getPushdownFilters)
</span>260 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Pushdown filters: &quot; + pushdownFilters)
</span>261 <span style=''></span><span style='background: #AEF1AE'>
</span>262 <span style=''></span><span style='background: #AEF1AE'>      exportSource = config.tableSource match {
</span>263 <span style=''></span><span style='background: #AEF1AE'>        case tablename: TableName =&gt; tablename.getFullTableName
</span>264 <span style=''></span><span style='background: #AEF1AE'>        case TableQuery(query, _) =&gt; &quot;(&quot; + query + &quot;) AS x&quot;
</span>265 <span style=''></span><span style='background: #AEF1AE'>      }
</span>266 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Export Source: &quot; + exportSource)
</span>267 <span style=''></span><span style='background: #AEF1AE'>
</span>268 <span style=''></span><span style='background: #AEF1AE'>      exportStatement = &quot;EXPORT TO PARQUET(&quot; +
</span>269 <span style=''></span><span style='background: #AEF1AE'>        &quot;directory = '&quot; + hdfsPath +
</span>270 <span style=''></span><span style='background: #AEF1AE'>        &quot;', fileSizeMB = &quot; + maxFileSize +
</span>271 <span style=''></span><span style='background: #AEF1AE'>        &quot;, rowGroupSizeMB = &quot; + maxRowGroupSize +
</span>272 <span style=''></span><span style='background: #AEF1AE'>        &quot;, fileMode = '&quot; + filePermissions +
</span>273 <span style=''></span><span style='background: #AEF1AE'>        &quot;', dirMode = '&quot; + filePermissions +
</span>274 <span style=''></span><span style='background: #AEF1AE'>        &quot;') AS SELECT &quot; + selectClause + &quot; FROM &quot; + exportSource + pushdownFilters + groupbyClause + &quot;;&quot;
</span>275 <span style=''></span><span style='background: #AEF1AE'>
</span>276 <span style=''></span><span style='background: #AEF1AE'>      // Export if not already exported
</span>277 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if(exportDone) </span><span style='background: #F0ADAD'>{
</span>278 <span style=''></span><span style='background: #F0ADAD'>        logger.info(&quot;Export already done, skipping export step.&quot;)
</span>279 <span style=''></span><span style='background: #F0ADAD'>        Right(())
</span>280 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style='background: #AEF1AE'> else {
</span>281 <span style=''></span><span style='background: #AEF1AE'>        logger.info(&quot;Exporting using statement: \n&quot; + exportStatement)
</span>282 <span style=''></span><span style='background: #AEF1AE'>
</span>283 <span style=''></span><span style='background: #AEF1AE'>        val timer = new Timer(config.timeOperations, logger, &quot;Export To Parquet From Vertica&quot;)
</span>284 <span style=''></span><span style='background: #AEF1AE'>        timer.startTime()
</span>285 <span style=''></span><span style='background: #AEF1AE'>        val res = jdbcLayer.execute(exportStatement).leftMap(err =&gt; ExportFromVerticaError(err))
</span>286 <span style=''></span><span style='background: #AEF1AE'>        sparkContext.addSparkListener(new ApplicationParquetCleaner(config))
</span>287 <span style=''></span><span style='background: #AEF1AE'>        timer.endTime()
</span>288 <span style=''></span><span style='background: #AEF1AE'>        res
</span>289 <span style=''></span><span style='background: #AEF1AE'>      }
</span>290 <span style=''></span><span style='background: #AEF1AE'>
</span>291 <span style=''></span><span style='background: #AEF1AE'>      // Retrieve all parquet files created by Vertica
</span>292 <span style=''></span><span style='background: #AEF1AE'>      dirExists &lt;- fileStoreLayer.fileExists(hdfsPath)
</span>293 <span style=''></span><span style='background: #AEF1AE'>      fullFileList &lt;- if(!dirExists) </span><span style='background: #F0ADAD'>Right(List())</span><span style='background: #AEF1AE'> else fileStoreLayer.getFileList(hdfsPath)
</span>294 <span style=''></span><span style='background: #AEF1AE'>      parquetFileList = fullFileList.filter(x =&gt; x.endsWith(&quot;.parquet&quot;))
</span>295 <span style=''></span><span style='background: #AEF1AE'>      requestedPartitionCount = config.partitionCount match {
</span>296 <span style=''></span><span style='background: #AEF1AE'>          case Some(count) =&gt; count
</span>297 <span style=''></span><span style='background: #AEF1AE'>          case None =&gt; parquetFileList.size // Default to 1 partition / file
</span>298 <span style=''></span><span style='background: #AEF1AE'>      }
</span>299 <span style=''></span><span style='background: #AEF1AE'>
</span>300 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Requested partition count: &quot; + requestedPartitionCount)
</span>301 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Parquet file list size: &quot; + parquetFileList.size)
</span>302 <span style=''></span><span style='background: #AEF1AE'>
</span>303 <span style=''></span><span style='background: #AEF1AE'>      partitionTimer = new Timer(config.timeOperations, logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;)
</span>304 <span style=''></span><span style='background: #AEF1AE'>      _ = partitionTimer.startTime()
</span>305 <span style=''></span><span style='background: #AEF1AE'>
</span>306 <span style=''></span><span style='background: #AEF1AE'>      fileMetadata &lt;- parquetFileList.toList.traverse(filename =&gt; fileStoreLayer.getParquetFileMetadata(filename))
</span>307 <span style=''></span><span style='background: #AEF1AE'>      totalRowGroups = fileMetadata.map(_.rowGroupCount).sum
</span>308 <span style=''></span><span style='background: #AEF1AE'>
</span>309 <span style=''></span><span style='background: #AEF1AE'>      _ = logger.info(&quot;Total row groups: &quot; + totalRowGroups)
</span>310 <span style=''></span><span style='background: #AEF1AE'>      // If table is empty, cleanup
</span>311 <span style=''></span><span style='background: #AEF1AE'>      _ =  if(totalRowGroups == 0) {
</span>312 <span style=''></span><span style='background: #AEF1AE'>        if(!config.fileStoreConfig.preventCleanup) {
</span>313 <span style=''></span><span style='background: #AEF1AE'>          logger.debug(&quot;Cleaning up empty directory in path: &quot; + hdfsPath)
</span>314 <span style=''></span><span style='background: #AEF1AE'>          cleanupUtils.cleanupAll(fileStoreLayer, hdfsPath)
</span>315 <span style=''></span><span style='background: #AEF1AE'>        }
</span>316 <span style=''></span><span style='background: #AEF1AE'>        else {
</span>317 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>Right()</span><span style='background: #AEF1AE'>
</span>318 <span style=''></span><span style='background: #AEF1AE'>        }
</span>319 <span style=''></span><span style='background: #AEF1AE'>      }
</span>320 <span style=''></span><span style='background: #AEF1AE'>
</span>321 <span style=''></span><span style='background: #AEF1AE'>      partitionCount = if (totalRowGroups &lt; requestedPartitionCount) </span><span style='background: #F0ADAD'>{
</span>322 <span style=''></span><span style='background: #F0ADAD'>        logger.info(&quot;Less than &quot; + requestedPartitionCount + &quot; partitions required, only using &quot; + totalRowGroups)
</span>323 <span style=''></span><span style='background: #F0ADAD'>        totalRowGroups
</span>324 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style='background: #AEF1AE'> else {
</span>325 <span style=''></span><span style='background: #AEF1AE'>        requestedPartitionCount
</span>326 <span style=''></span><span style='background: #AEF1AE'>      }
</span>327 <span style=''></span><span style='background: #AEF1AE'>
</span>328 <span style=''></span><span style='background: #AEF1AE'>      partitionInfo = getPartitionInfo(fileMetadata, partitionCount)
</span>329 <span style=''></span><span style='background: #AEF1AE'>
</span>330 <span style=''></span><span style='background: #AEF1AE'>      _ = partitionTimer.endTime()
</span>331 <span style=''></span><span style='background: #AEF1AE'>
</span>332 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- jdbcLayer.close()
</span>333 <span style=''></span><span style='background: #AEF1AE'>    } yield partitionInfo</span><span style=''>
</span>334 <span style=''>
</span>335 <span style=''>    // If there's an error, cleanup
</span>336 <span style=''>    ret match {
</span>337 <span style=''>      case Left(_) =&gt;
</span>338 <span style=''>        if(</span><span style='background: #AEF1AE'>!config.fileStoreConfig.preventCleanup</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>339 <span style=''></span><span style='background: #AEF1AE'>          logger.info(&quot;Cleaning up all files in path: &quot; + hdfsPath)
</span>340 <span style=''></span><span style='background: #AEF1AE'>          cleanupUtils.cleanupAll(fileStoreLayer, hdfsPath)
</span>341 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>342 <span style=''>        </span><span style='background: #AEF1AE'>jdbcLayer.close()</span><span style=''>
</span>343 <span style=''>      case _ =&gt; logger.info(&quot;Reading data from Parquet file.&quot;)
</span>344 <span style=''>    }
</span>345 <span style=''>
</span>346 <span style=''>    ret
</span>347 <span style=''>  }
</span>348 <span style=''>
</span>349 <span style=''>  var partition : Option[VerticaDistributedFilesystemPartition] = </span><span style='background: #AEF1AE'>None</span><span style=''>
</span>350 <span style=''>  var fileIdx = </span><span style='background: #AEF1AE'>0</span><span style=''>
</span>351 <span style=''>
</span>352 <span style=''>
</span>353 <span style=''>  val timer = </span><span style='background: #AEF1AE'>new Timer(config.timeOperations, logger, &quot;Partition Read&quot;)</span><span style=''>
</span>354 <span style=''>
</span>355 <span style=''>  /**
</span>356 <span style=''>   * Initial setup for the read of an individual partition. Called by executor.
</span>357 <span style=''>   */
</span>358 <span style=''>  def startPartitionRead(verticaPartition: VerticaPartition): ConnectorResult[Unit] = {
</span>359 <span style=''>    </span><span style='background: #AEF1AE'>timer.startTime()</span><span style=''>
</span>360 <span style=''>    logger.info(&quot;Starting partition read.&quot;)
</span>361 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>362 <span style=''></span><span style='background: #AEF1AE'>      part &lt;- verticaPartition match {
</span>363 <span style=''></span><span style='background: #AEF1AE'>          case p: VerticaDistributedFilesystemPartition =&gt; Right(p)
</span>364 <span style=''></span><span style='background: #AEF1AE'>          case _ =&gt; Left(InvalidPartition())
</span>365 <span style=''></span><span style='background: #AEF1AE'>        }
</span>366 <span style=''></span><span style='background: #AEF1AE'>      _ = this.partition = Some(part)
</span>367 <span style=''></span><span style='background: #AEF1AE'>      _ = this.fileIdx = 0
</span>368 <span style=''></span><span style='background: #AEF1AE'>
</span>369 <span style=''></span><span style='background: #AEF1AE'>      // Check if empty and initialize with first file range
</span>370 <span style=''></span><span style='background: #AEF1AE'>      ret &lt;- part.fileRanges.headOption match {
</span>371 <span style=''></span><span style='background: #AEF1AE'>        case None =&gt;
</span>372 <span style=''></span><span style='background: #AEF1AE'>          logger.warn(&quot;No files to read set on partition.&quot;)
</span>373 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>Left(DoneReading())</span><span style='background: #AEF1AE'>
</span>374 <span style=''></span><span style='background: #AEF1AE'>        case Some(head) =&gt;
</span>375 <span style=''></span><span style='background: #AEF1AE'>          fileStoreLayer.openReadParquetFile(head)
</span>376 <span style=''></span><span style='background: #AEF1AE'>      }
</span>377 <span style=''></span><span style='background: #AEF1AE'>    } yield ret</span><span style=''>
</span>378 <span style=''>  }
</span>379 <span style=''>
</span>380 <span style=''>  private def getCleanupInfo(part: VerticaDistributedFilesystemPartition, curIdx: Int): Option[FileCleanupInfo] = {
</span>381 <span style=''>    logger.debug(&quot;Getting cleanup info for partition with idx &quot; + curIdx)
</span>382 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>383 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- if (curIdx &gt;= part.fileRanges.size) {
</span>384 <span style=''></span><span style='background: #AEF1AE'>        logger.warn(&quot;Invalid fileIdx &quot; + this.fileIdx + &quot;, can't perform cleanup.&quot;)
</span>385 <span style=''></span><span style='background: #AEF1AE'>        None
</span>386 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>387 <span style=''></span><span style='background: #AEF1AE'>        Some(())
</span>388 <span style=''></span><span style='background: #AEF1AE'>      }
</span>389 <span style=''></span><span style='background: #AEF1AE'>
</span>390 <span style=''></span><span style='background: #AEF1AE'>      curRange = part.fileRanges(curIdx)
</span>391 <span style=''></span><span style='background: #AEF1AE'>      ret &lt;- part.rangeCountMap match {
</span>392 <span style=''></span><span style='background: #AEF1AE'>        case Some (rangeCountMap) if rangeCountMap.contains (curRange.filename) =&gt; curRange.rangeIdx match {
</span>393 <span style=''></span><span style='background: #AEF1AE'>          case Some (rangeIdx) =&gt; Some (FileCleanupInfo (curRange.filename, rangeIdx, rangeCountMap (curRange.filename)))
</span>394 <span style=''></span><span style='background: #AEF1AE'>          case None =&gt;
</span>395 <span style=''></span><span style='background: #AEF1AE'>            logger.warn (&quot;Missing range count index. Not performing any cleanup for file &quot; + curRange.filename)
</span>396 <span style=''></span><span style='background: #AEF1AE'>            </span><span style='background: #F0ADAD'>None</span><span style='background: #AEF1AE'>
</span>397 <span style=''></span><span style='background: #AEF1AE'>        }
</span>398 <span style=''></span><span style='background: #AEF1AE'>        case None =&gt;
</span>399 <span style=''></span><span style='background: #AEF1AE'>          logger.warn (&quot;Missing range count map. Not performing any cleanup for file &quot; + curRange.filename)
</span>400 <span style=''></span><span style='background: #AEF1AE'>          None
</span>401 <span style=''></span><span style='background: #AEF1AE'>        case _ =&gt;
</span>402 <span style=''></span><span style='background: #AEF1AE'>          logger.warn (&quot;Missing value in range count map. Not performing any cleanup for file &quot; + curRange.filename)
</span>403 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>None</span><span style='background: #AEF1AE'>
</span>404 <span style=''></span><span style='background: #AEF1AE'>      }
</span>405 <span style=''></span><span style='background: #AEF1AE'>    } yield ret</span><span style=''>
</span>406 <span style=''>  }
</span>407 <span style=''>
</span>408 <span style=''>  /**
</span>409 <span style=''>   * Reads a block of data to the underlying source. Called by executor.
</span>410 <span style=''>   *
</span>411 <span style=''>   * May return empty block if at end of read. If attempting to read after end of read, DoneReading will be returned.
</span>412 <span style=''>   */
</span>413 <span style=''>  // scalastyle:off
</span>414 <span style=''>  def readData: ConnectorResult[DataBlock] = {
</span>415 <span style=''>    val part = </span><span style='background: #AEF1AE'>this.partition</span><span style=''> match {
</span>416 <span style=''>      case None =&gt; return </span><span style='background: #F0ADAD'>Left(UninitializedReadError())</span><span style=''>
</span>417 <span style=''>      case Some(p) =&gt; p
</span>418 <span style=''>    }
</span>419 <span style=''>
</span>420 <span style=''>    val ret = </span><span style='background: #AEF1AE'>fileStoreLayer.readDataFromParquetFile(dataSize)</span><span style=''> match {
</span>421 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>Left(err)</span><span style=''>
</span>422 <span style=''>      case Right(data) =&gt;
</span>423 <span style=''>        if(</span><span style='background: #AEF1AE'>data.data.nonEmpty</span><span style=''>) {
</span>424 <span style=''>          </span><span style='background: #AEF1AE'>Right(data)</span><span style=''>
</span>425 <span style=''>        }
</span>426 <span style=''>        else </span><span style='background: #AEF1AE'>{
</span>427 <span style=''></span><span style='background: #AEF1AE'>
</span>428 <span style=''></span><span style='background: #AEF1AE'>          logger.info(&quot;Hit done reading for file segment.&quot;)
</span>429 <span style=''></span><span style='background: #AEF1AE'>          // Next file
</span>430 <span style=''></span><span style='background: #AEF1AE'>          this.fileIdx += 1
</span>431 <span style=''></span><span style='background: #AEF1AE'>          if (this.fileIdx &gt;= part.fileRanges.size) return Right(data)
</span>432 <span style=''></span><span style='background: #AEF1AE'>
</span>433 <span style=''></span><span style='background: #AEF1AE'>          for {
</span>434 <span style=''></span><span style='background: #AEF1AE'>            _ &lt;- fileStoreLayer.closeReadParquetFile()
</span>435 <span style=''></span><span style='background: #AEF1AE'>            _ &lt;- fileStoreLayer.openReadParquetFile(part.fileRanges(this.fileIdx))
</span>436 <span style=''></span><span style='background: #AEF1AE'>            data &lt;- fileStoreLayer.readDataFromParquetFile(dataSize)
</span>437 <span style=''></span><span style='background: #AEF1AE'>          } yield data
</span>438 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>439 <span style=''>    }
</span>440 <span style=''>
</span>441 <span style=''>    // If there was an underlying error, call cleanup
</span>442 <span style=''>    (ret, getCleanupInfo(part,this.fileIdx)) match {
</span>443 <span style=''>      case (Left(_), Some(cleanupInfo)) =&gt;
</span>444 <span style=''>        if(</span><span style='background: #F0ADAD'>!config.fileStoreConfig.preventCleanup</span><span style=''>) </span><span style='background: #F0ADAD'>cleanupUtils.checkAndCleanup(fileStoreLayer, cleanupInfo) match {
</span>445 <span style=''></span><span style='background: #F0ADAD'>          case Right(()) =&gt; ()
</span>446 <span style=''></span><span style='background: #F0ADAD'>          case Left(err) =&gt; logger.warn(&quot;Ran into error when cleaning up: &quot; + err.getFullContext)
</span>447 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>448 <span style=''>      case (Left(_), None) =&gt; logger.warn(&quot;No cleanup info found&quot;)
</span>449 <span style=''>      case (Right(dataBlock), Some(cleanupInfo)) =&gt;
</span>450 <span style=''>        if(</span><span style='background: #AEF1AE'>dataBlock.data.isEmpty &amp;&amp; </span><span style='background: #F0ADAD'>!config.fileStoreConfig.preventCleanup</span><span style=''>) </span><span style='background: #F0ADAD'>cleanupUtils.checkAndCleanup(fileStoreLayer, cleanupInfo) match {
</span>451 <span style=''></span><span style='background: #F0ADAD'>          case Right(()) =&gt; ()
</span>452 <span style=''></span><span style='background: #F0ADAD'>          case Left(err) =&gt; logger.warn(&quot;Ran into error when cleaning up: &quot; + err.getFullContext)
</span>453 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>454 <span style=''>      case (Right(_), None) =&gt; </span><span style='background: #AEF1AE'>()</span><span style=''>
</span>455 <span style=''>    }
</span>456 <span style=''>    ret
</span>457 <span style=''>  }
</span>458 <span style=''>
</span>459 <span style=''>
</span>460 <span style=''>  /**
</span>461 <span style=''>   * Ends the read, doing any necessary cleanup. Called by executor once reading the partition is done.
</span>462 <span style=''>   */
</span>463 <span style=''>  def endPartitionRead(): ConnectorResult[Unit] = {
</span>464 <span style=''>    </span><span style='background: #AEF1AE'>timer.endTime()</span><span style=''>
</span>465 <span style=''>    </span><span style='background: #AEF1AE'>for {
</span>466 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- cleanupFiles()
</span>467 <span style=''></span><span style='background: #AEF1AE'>      _ &lt;- fileStoreLayer.closeReadParquetFile()
</span>468 <span style=''></span><span style='background: #AEF1AE'>    } yield ()</span><span style=''>
</span>469 <span style=''>  }
</span>470 <span style=''>
</span>471 <span style=''>  def cleanupFiles(): ConnectorResult[Unit] ={
</span>472 <span style=''>    logger.info(&quot;Removing files before closing read pipe.&quot;)
</span>473 <span style=''>    val part = </span><span style='background: #AEF1AE'>this.partition</span><span style=''> match {
</span>474 <span style=''>      case None =&gt; return </span><span style='background: #F0ADAD'>Left(UninitializedReadError())</span><span style=''>
</span>475 <span style=''>      case Some(p) =&gt; p
</span>476 <span style=''>    }
</span>477 <span style=''>
</span>478 <span style=''>    </span><span style='background: #AEF1AE'>for(fileIdx &lt;- 0 to part.fileRanges.size ){
</span>479 <span style=''></span><span style='background: #AEF1AE'>      if(!config.fileStoreConfig.preventCleanup) {
</span>480 <span style=''></span><span style='background: #AEF1AE'>        // Cleanup old file if required
</span>481 <span style=''></span><span style='background: #AEF1AE'>        getCleanupInfo(part, fileIdx) match {
</span>482 <span style=''></span><span style='background: #AEF1AE'>          case Some(cleanupInfo) =&gt; cleanupUtils.checkAndCleanup(fileStoreLayer, cleanupInfo) match {
</span>483 <span style=''></span><span style='background: #AEF1AE'>            case Left(err) =&gt; logger.warn(&quot;Ran into error when calling cleaning up. Treating as non-fatal. Err: &quot; + err.getFullContext)
</span>484 <span style=''></span><span style='background: #AEF1AE'>            case Right(_) =&gt; ()
</span>485 <span style=''></span><span style='background: #AEF1AE'>          }
</span>486 <span style=''></span><span style='background: #AEF1AE'>          case None =&gt; logger.warn(&quot;No cleanup info found.&quot;)
</span>487 <span style=''></span><span style='background: #AEF1AE'>        }
</span>488 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>489 <span style=''>    }
</span>490 <span style=''>    </span><span style='background: #AEF1AE'>Right()</span><span style=''>
</span>491 <span style=''>  }
</span>492 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          678
        </td>
        <td>
          3719
          -
          3787
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe])
        </td>
      </tr><tr>
        <td>
          73
        </td>
        <td>
          679
        </td>
        <td>
          4067
          -
          4085
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.maxFileSize
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.maxFileSize
        </td>
      </tr><tr>
        <td>
          74
        </td>
        <td>
          680
        </td>
        <td>
          4118
          -
          4140
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.maxRowGroupSize
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.maxRowGroupSize
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          681
        </td>
        <td>
          4241
          -
          4255
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          683
        </td>
        <td>
          4218
          -
          4281
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.readSchema
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.schemaTools.readSchema(this.jdbcLayer, this.config.tableSource)
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          682
        </td>
        <td>
          4257
          -
          4280
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.tableSource
        </td>
        <td style="background: #AEF1AE">
          this.config.tableSource
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          684
        </td>
        <td>
          4324
          -
          4351
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.VerticaReadMetadata.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.VerticaReadMetadata.apply(schema)
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          685
        </td>
        <td>
          4318
          -
          4352
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.config.VerticaReadMetadata](com.vertica.spark.config.VerticaReadMetadata.apply(schema))
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          686
        </td>
        <td>
          4377
          -
          4386
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          687
        </td>
        <td>
          4528
          -
          4530
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          690
        </td>
        <td>
          4547
          -
          4615
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot; WHERE &quot;.+(pushdownFilters.map[String, List[String]](((x$1: com.vertica.spark.datasource.v2.PushdownFilter) =&gt; x$1.getFilterString))(immutable.this.List.canBuildFrom[String]).mkString(&quot; AND &quot;))
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          689
        </td>
        <td>
          4559
          -
          4615
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td style="background: #AEF1AE">
          pushdownFilters.map[String, List[String]](((x$1: com.vertica.spark.datasource.v2.PushdownFilter) =&gt; x$1.getFilterString))(immutable.this.List.canBuildFrom[String]).mkString(&quot; AND &quot;)
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          688
        </td>
        <td>
          4547
          -
          4556
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot; WHERE &quot;
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          691
        </td>
        <td>
          4877
          -
          4897
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.metadata
        </td>
        <td style="background: #AEF1AE">
          this.config.metadata
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          692
        </td>
        <td>
          4931
          -
          4942
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.config.VerticaReadMetadata](data)
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          693
        </td>
        <td>
          4962
          -
          4985
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.retrieveMetadata
        </td>
        <td style="background: #AEF1AE">
          this.retrieveMetadata()
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          695
        </td>
        <td>
          5146
          -
          5161
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Long](VerticaDistributedFilesystemReadPipe.this.dataSize.toLong)
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          694
        </td>
        <td>
          5152
          -
          5160
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.dataSize.toLong
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          696
        </td>
        <td>
          5372
          -
          5395
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          map.contains(filename).unary_!
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          698
        </td>
        <td>
          5404
          -
          5421
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.mutable.MapLike.update
        </td>
        <td style="background: #AEF1AE">
          map.update(filename, 1)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          697
        </td>
        <td>
          5404
          -
          5421
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.MapLike.update
        </td>
        <td style="background: #AEF1AE">
          map.update(filename, 1)
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          699
        </td>
        <td>
          5445
          -
          5463
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          map.apply(filename).+(1)
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          701
        </td>
        <td>
          5445
          -
          5463
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.mutable.MapLike.update
        </td>
        <td style="background: #AEF1AE">
          map.update(filename, map.apply(filename).+(1))
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          700
        </td>
        <td>
          5445
          -
          5463
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.MapLike.update
        </td>
        <td style="background: #AEF1AE">
          map.update(filename, map.apply(filename).+(1))
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          702
        </td>
        <td>
          5474
          -
          5491
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td style="background: #AEF1AE">
          map.apply(filename).-(1)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          705
        </td>
        <td>
          5667
          -
          5667
        </td>
        <td>
          Select
        </td>
        <td>
          scala.math.Numeric.IntIsIntegral
        </td>
        <td style="background: #AEF1AE">
          math.this.Numeric.IntIsIntegral
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          704
        </td>
        <td>
          5649
          -
          5649
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[Int]
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          706
        </td>
        <td>
          5633
          -
          5670
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.sum
        </td>
        <td style="background: #AEF1AE">
          fileMetadata.map[Int, Seq[Int]](((x$2: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$2.rowGroupCount))(collection.this.Seq.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          703
        </td>
        <td>
          5650
          -
          5665
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.rowGroupCount
        </td>
        <td style="background: #AEF1AE">
          x$2.rowGroupCount
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          710
        </td>
        <td>
          5747
          -
          5861
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;No data. Returning empty partition list.&quot;)
  else
    (): Unit);
  PartitionInfo.apply(scala.Array.apply[org.apache.spark.sql.connector.read.InputPartition]()((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition])))
}
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          707
        </td>
        <td>
          5726
          -
          5745
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          totalRowGroups.==(0)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          708
        </td>
        <td>
          5831
          -
          5854
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[org.apache.spark.sql.connector.read.InputPartition]()((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition]))
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          709
        </td>
        <td>
          5817
          -
          5855
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.PartitionInfo.apply
        </td>
        <td style="background: #AEF1AE">
          PartitionInfo.apply(scala.Array.apply[org.apache.spark.sql.connector.read.InputPartition]()((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition])))
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          779
        </td>
        <td>
          5871
          -
          8230
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val extraSpace: Int = if (totalRowGroups.%(partitionCount).==(0))
    0
  else
    1;
  val rowGroupRoom: Int = totalRowGroups./(partitionCount).+(extraSpace);
  var i: Int = 0;
  var partitions: List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition] = scala.collection.immutable.Nil;
  var curFileRanges: List[com.vertica.spark.datasource.core.ParquetFileRange] = scala.collection.immutable.Nil;
  val rangeCountMap: scala.collection.mutable.Map[String,Int] = scala.collection.mutable.Map.apply[String, Int]();
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Creating partitions.&quot;)
  else
    (): Unit);
  fileMetadata.foreach[Unit](((m: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; {
    val size: Int = m.rowGroupCount;
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Splitting file &quot;.+(m.filename).+(&quot; with row group count &quot;).+(size))
    else
      (): Unit);
    var j: Int = 0;
    var low: Int = 0;
    while$1(){
      if (j.&lt;(size))
        {
          {
            if (i.==(rowGroupRoom.-(1)))
              {
                val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
                val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
                curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
                val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
                partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]);
                curFileRanges = scala.collection.immutable.Nil;
                (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
                  VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached partition with file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
                else
                  (): Unit);
                i = 0;
                low = j.+(1)
              }
            else
              if (j.==(size.-(1)))
                {
                  val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
                  val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
                  curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
                  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
                    VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached end of file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
                  else
                    (): Unit);
                  i = i.+(1)
                }
              else
                i = i.+(1);
            j = j.+(1)
          };
          while$1()
        }
      else
        ()
    }
  }));
  if (curFileRanges.nonEmpty)
    {
      val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
      partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition])
    }
  else
    ();
  partitions = partitions.map[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](((part: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition) =&gt; {
    &lt;artifact&gt; val x$1: Some[scala.collection.immutable.Map[String,Int]] @scala.reflect.internal.annotations.uncheckedBounds = scala.Some.apply[scala.collection.immutable.Map[String,Int]](rangeCountMap.toMap[String, Int](scala.Predef.$conforms[(String, Int)]));
    &lt;artifact&gt; val x$2: Seq[com.vertica.spark.datasource.core.ParquetFileRange] @scala.reflect.internal.annotations.uncheckedBounds = part.copy$default$1;
    part.copy(x$2, x$1)
  }))(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]);
  PartitionInfo.apply(partitions.toArray[org.apache.spark.sql.connector.read.InputPartition]((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition])))
}
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          714
        </td>
        <td>
          5944
          -
          5945
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          711
        </td>
        <td>
          5899
          -
          5935
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          totalRowGroups.%(partitionCount).==(0)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          713
        </td>
        <td>
          5937
          -
          5938
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          715
        </td>
        <td>
          5944
          -
          5945
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          712
        </td>
        <td>
          5937
          -
          5938
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          716
        </td>
        <td>
          5971
          -
          6017
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          totalRowGroups./(partitionCount).+(extraSpace)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          717
        </td>
        <td>
          6099
          -
          6100
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          718
        </td>
        <td>
          6124
          -
          6169
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          719
        </td>
        <td>
          6196
          -
          6220
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          720
        </td>
        <td>
          6247
          -
          6290
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenMapFactory.apply
        </td>
        <td style="background: #AEF1AE">
          scala.collection.mutable.Map.apply[String, Int]()
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          761
        </td>
        <td>
          6340
          -
          7795
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td style="background: #AEF1AE">
          fileMetadata.foreach[Unit](((m: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; {
  val size: Int = m.rowGroupCount;
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Splitting file &quot;.+(m.filename).+(&quot; with row group count &quot;).+(size))
  else
    (): Unit);
  var j: Int = 0;
  var low: Int = 0;
  while$1(){
    if (j.&lt;(size))
      {
        {
          if (i.==(rowGroupRoom.-(1)))
            {
              val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
              val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
              curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
              val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
              partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]);
              curFileRanges = scala.collection.immutable.Nil;
              (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
                VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached partition with file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
              else
                (): Unit);
              i = 0;
              low = j.+(1)
            }
          else
            if (j.==(size.-(1)))
              {
                val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
                val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
                curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
                (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
                  VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached end of file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
                else
                  (): Unit);
                i = i.+(1)
              }
            else
              i = i.+(1);
          j = j.+(1)
        };
        while$1()
      }
    else
      ()
  }
}))
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          721
        </td>
        <td>
          6384
          -
          6399
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.rowGroupCount
        </td>
        <td style="background: #AEF1AE">
          m.rowGroupCount
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          722
        </td>
        <td>
          6503
          -
          6504
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          723
        </td>
        <td>
          6523
          -
          6524
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          759
        </td>
        <td>
          6533
          -
          6533
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          758
        </td>
        <td>
          6548
          -
          7787
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  {
    if (i.==(rowGroupRoom.-(1)))
      {
        val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
        val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
        curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
        val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
        partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]);
        curFileRanges = scala.collection.immutable.Nil;
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached partition with file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
        else
          (): Unit);
        i = 0;
        low = j.+(1)
      }
    else
      if (j.==(size.-(1)))
        {
          val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
          val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
          curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
          (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
            VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached end of file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
          else
            (): Unit);
          i = i.+(1)
        }
      else
        i = i.+(1);
    j = j.+(1)
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          757
        </td>
        <td>
          6548
          -
          6548
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.while$1
        </td>
        <td style="background: #AEF1AE">
          while$1()
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          724
        </td>
        <td>
          6539
          -
          6547
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td style="background: #AEF1AE">
          j.&lt;(size)
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          760
        </td>
        <td>
          6533
          -
          6533
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          741
        </td>
        <td>
          6583
          -
          7232
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
  val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
  curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
  val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
  partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]);
  curFileRanges = scala.collection.immutable.Nil;
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached partition with file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
  else
    (): Unit);
  i = 0;
  low = j.+(1)
}
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          726
        </td>
        <td>
          6563
          -
          6582
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          i.==(rowGroupRoom.-(1))
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          725
        </td>
        <td>
          6568
          -
          6582
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td style="background: #AEF1AE">
          rowGroupRoom.-(1)
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          728
        </td>
        <td>
          6654
          -
          6706
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.incrementRangeMapGetIndex
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename)
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          727
        </td>
        <td>
          6695
          -
          6705
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.filename
        </td>
        <td style="background: #AEF1AE">
          m.filename
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          729
        </td>
        <td>
          6750
          -
          6760
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.filename
        </td>
        <td style="background: #AEF1AE">
          m.filename
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          731
        </td>
        <td>
          6733
          -
          6785
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.apply
        </td>
        <td style="background: #AEF1AE">
          ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx))
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          730
        </td>
        <td>
          6770
          -
          6784
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Int](rangeIdx)
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          732
        </td>
        <td>
          6829
          -
          6829
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          733
        </td>
        <td>
          6815
          -
          6838
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td style="background: #AEF1AE">
          curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange])
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          735
        </td>
        <td>
          6867
          -
          6919
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.apply
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2)
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          734
        </td>
        <td>
          6867
          -
          6867
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.apply$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemPartition.apply$default$2
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          737
        </td>
        <td>
          6945
          -
          6968
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td style="background: #AEF1AE">
          partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition])
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          736
        </td>
        <td>
          6956
          -
          6956
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]
        </td>
      </tr><tr>
        <td>
          154
        </td>
        <td>
          738
        </td>
        <td>
          6997
          -
          7021
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          739
        </td>
        <td>
          7195
          -
          7196
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          740
        </td>
        <td>
          7215
          -
          7220
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          j.+(1)
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          752
        </td>
        <td>
          7265
          -
          7712
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
  val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
  curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached end of file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
  else
    (): Unit);
  i = i.+(1)
}
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          743
        </td>
        <td>
          7251
          -
          7264
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          j.==(size.-(1))
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          755
        </td>
        <td>
          7248
          -
          7760
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          if (j.==(size.-(1)))
  {
    val rangeIdx: Int = VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename);
    val frange: com.vertica.spark.datasource.core.ParquetFileRange = ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx));
    curFileRanges = curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]);
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Reached end of file &quot;.+(m.filename).+(&quot; , range low: &quot;).+(low).+(&quot; , range high: &quot;).+(j).+(&quot; , idx: &quot;).+(rangeIdx))
    else
      (): Unit);
    i = i.+(1)
  }
else
  i = i.+(1)
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          742
        </td>
        <td>
          7256
          -
          7264
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td style="background: #AEF1AE">
          size.-(1)
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          744
        </td>
        <td>
          7391
          -
          7401
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.filename
        </td>
        <td style="background: #AEF1AE">
          m.filename
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          745
        </td>
        <td>
          7350
          -
          7402
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.incrementRangeMapGetIndex
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.incrementRangeMapGetIndex(rangeCountMap, m.filename)
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          747
        </td>
        <td>
          7465
          -
          7479
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Int](rangeIdx)
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          746
        </td>
        <td>
          7445
          -
          7455
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.filename
        </td>
        <td style="background: #AEF1AE">
          m.filename
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          748
        </td>
        <td>
          7428
          -
          7480
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.apply
        </td>
        <td style="background: #AEF1AE">
          ParquetFileRange.apply(m.filename, low, j, scala.Some.apply[Int](rangeIdx))
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          750
        </td>
        <td>
          7509
          -
          7532
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td style="background: #AEF1AE">
          curFileRanges.:+[com.vertica.spark.datasource.core.ParquetFileRange, List[com.vertica.spark.datasource.core.ParquetFileRange]](frange)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange])
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          749
        </td>
        <td>
          7523
          -
          7523
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.ParquetFileRange]
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          751
        </td>
        <td>
          7694
          -
          7700
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          753
        </td>
        <td>
          7742
          -
          7748
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          754
        </td>
        <td>
          7742
          -
          7748
        </td>
        <td>
          Assign
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          i = i.+(1)
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          756
        </td>
        <td>
          7771
          -
          7777
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          j.+(1)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          768
        </td>
        <td>
          7881
          -
          7881
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          762
        </td>
        <td>
          7884
          -
          7906
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td style="background: #AEF1AE">
          curFileRanges.nonEmpty
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          767
        </td>
        <td>
          7908
          -
          8039
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  val partition: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition = VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2);
  partitions = partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition])
}
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          769
        </td>
        <td>
          7881
          -
          7881
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          764
        </td>
        <td>
          7934
          -
          7986
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.apply
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemPartition.apply(curFileRanges, VerticaDistributedFilesystemPartition.apply$default$2)
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          763
        </td>
        <td>
          7934
          -
          7934
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.apply$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemPartition.apply$default$2
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          765
        </td>
        <td>
          8019
          -
          8019
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          766
        </td>
        <td>
          8008
          -
          8031
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td style="background: #AEF1AE">
          partitions.:+[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](partition)(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition])
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          774
        </td>
        <td>
          8130
          -
          8182
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.copy
        </td>
        <td style="background: #AEF1AE">
          part.copy(x$2, x$1)
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          771
        </td>
        <td>
          8161
          -
          8180
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td style="background: #AEF1AE">
          rangeCountMap.toMap[String, Int](scala.Predef.$conforms[(String, Int)])
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          770
        </td>
        <td>
          8175
          -
          8175
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, Int)]
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          773
        </td>
        <td>
          8135
          -
          8135
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.copy$default$1
        </td>
        <td style="background: #AEF1AE">
          part.copy$default$1
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          776
        </td>
        <td>
          8107
          -
          8183
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td style="background: #AEF1AE">
          partitions.map[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, List[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]](((part: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition) =&gt; {
  &lt;artifact&gt; val x$1: Some[scala.collection.immutable.Map[String,Int]] @scala.reflect.internal.annotations.uncheckedBounds = scala.Some.apply[scala.collection.immutable.Map[String,Int]](rangeCountMap.toMap[String, Int](scala.Predef.$conforms[(String, Int)]));
  &lt;artifact&gt; val x$2: Seq[com.vertica.spark.datasource.core.ParquetFileRange] @scala.reflect.internal.annotations.uncheckedBounds = part.copy$default$1;
  part.copy(x$2, x$1)
}))(immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition])
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          775
        </td>
        <td>
          8121
          -
          8121
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition]
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          772
        </td>
        <td>
          8156
          -
          8181
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[scala.collection.immutable.Map[String,Int]](rangeCountMap.toMap[String, Int](scala.Predef.$conforms[(String, Int)]))
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          777
        </td>
        <td>
          8205
          -
          8223
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td style="background: #AEF1AE">
          partitions.toArray[org.apache.spark.sql.connector.read.InputPartition]((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition]))
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          778
        </td>
        <td>
          8191
          -
          8224
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.PartitionInfo.apply
        </td>
        <td style="background: #AEF1AE">
          PartitionInfo.apply(partitions.toArray[org.apache.spark.sql.connector.read.InputPartition]((ClassTag.apply[org.apache.spark.sql.connector.read.InputPartition](classOf[org.apache.spark.sql.connector.read.InputPartition]): scala.reflect.ClassTag[org.apache.spark.sql.connector.read.InputPartition])))
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          783
        </td>
        <td>
          8374
          -
          8403
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.getRequiredSchema
        </td>
        <td style="background: #AEF1AE">
          this.config.getRequiredSchema
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          782
        </td>
        <td>
          8326
          -
          8353
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getPushdownAggregateColumns
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.getPushdownAggregateColumns
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          785
        </td>
        <td>
          8359
          -
          8404
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getColumnNames
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getColumnNames(this.config.getRequiredSchema)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          784
        </td>
        <td>
          8359
          -
          8404
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getColumnNames
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getColumnNames(this.config.getRequiredSchema)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          781
        </td>
        <td>
          8326
          -
          8353
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getPushdownAggregateColumns
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.getPushdownAggregateColumns
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          780
        </td>
        <td>
          8302
          -
          8324
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.isAggPushedDown
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.isAggPushedDown
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          786
        </td>
        <td>
          8504
          -
          8604
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td style="background: #F0ADAD">
          this.config.getRequiredSchema.map[String, Seq[String]](((col: org.apache.spark.sql.types.StructField) =&gt; scala.StringContext.apply(&quot;&quot;, &quot; as &quot;).s(col.name).+(&quot;\&quot;&quot;).+(col.name).+(&quot;\&quot;&quot;)))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          787
        </td>
        <td>
          8609
          -
          8628
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, String](selectClause)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          792
        </td>
        <td>
          8717
          -
          8781
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #F0ADAD">
          &quot; GROUP BY &quot;.+(scala.Predef.refArrayOps[String](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](this.config.getGroupBy).map[String, Array[String]](((x$3: org.apache.spark.sql.types.StructField) =&gt; x$3.name))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))).mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          795
        </td>
        <td>
          8787
          -
          8789
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          788
        </td>
        <td>
          8684
          -
          8706
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.getGroupBy
        </td>
        <td style="background: #AEF1AE">
          this.config.getGroupBy
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          791
        </td>
        <td>
          8732
          -
          8781
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[String](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](this.config.getGroupBy).map[String, Array[String]](((x$3: org.apache.spark.sql.types.StructField) =&gt; x$3.name))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))).mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          794
        </td>
        <td>
          8787
          -
          8789
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          793
        </td>
        <td>
          8717
          -
          8781
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #F0ADAD">
          &quot; GROUP BY &quot;.+(scala.Predef.refArrayOps[String](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](this.config.getGroupBy).map[String, Array[String]](((x$3: org.apache.spark.sql.types.StructField) =&gt; x$3.name))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))).mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          790
        </td>
        <td>
          8717
          -
          8729
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot; GROUP BY &quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          789
        </td>
        <td>
          8684
          -
          8715
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](this.config.getGroupBy).nonEmpty
        </td>
      </tr><tr>
        <td>
          203
        </td>
        <td>
          797
        </td>
        <td>
          8923
          -
          8941
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.tableSource
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.tableSource
        </td>
      </tr><tr>
        <td>
          203
        </td>
        <td>
          796
        </td>
        <td>
          8912
          -
          8921
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.jdbcLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer
        </td>
      </tr><tr>
        <td>
          203
        </td>
        <td>
          798
        </td>
        <td>
          8886
          -
          8942
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.getColumnInfo
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.schemaTools.getColumnInfo(VerticaDistributedFilesystemReadPipe.this.jdbcLayer, VerticaDistributedFilesystemReadPipe.this.config.tableSource)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          800
        </td>
        <td>
          8975
          -
          9064
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err.context(&quot;Failed to get table schema when checking for fields that need casts.&quot;))
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          799
        </td>
        <td>
          8980
          -
          9063
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          err.context(&quot;Failed to get table schema when checking for fields that need casts.&quot;)
        </td>
      </tr><tr>
        <td>
          205
        </td>
        <td>
          801
        </td>
        <td>
          9103
          -
          9160
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.schema.SchemaToolsInterface.makeColumnsString
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.schemaTools.makeColumnsString(columnDefs, requiredSchema)
        </td>
      </tr><tr>
        <td>
          205
        </td>
        <td>
          802
        </td>
        <td>
          9097
          -
          9161
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, String](VerticaDistributedFilesystemReadPipe.this.schemaTools.makeColumnsString(columnDefs, requiredSchema))
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          803
        </td>
        <td>
          9844
          -
          9866
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.fileStoreConfig
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          810
        </td>
        <td>
          9983
          -
          9985
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          804
        </td>
        <td>
          9890
          -
          9913
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #AEF1AE">
          fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          812
        </td>
        <td>
          9991
          -
          9994
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          806
        </td>
        <td>
          9930
          -
          9933
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          809
        </td>
        <td>
          9983
          -
          9985
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          811
        </td>
        <td>
          9991
          -
          9994
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;/&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          805
        </td>
        <td>
          9924
          -
          9925
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          808
        </td>
        <td>
          9890
          -
          9981
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(fileStoreConfig.address).takeRight(1).==(&quot;/&quot;).||(scala.Predef.augmentString(fileStoreConfig.address).takeRight(1).==(&quot;\\&quot;))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          807
        </td>
        <td>
          9937
          -
          9981
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(fileStoreConfig.address).takeRight(1).==(&quot;\\&quot;)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          813
        </td>
        <td>
          10052
          -
          10081
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableSource.identifier
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.tableSource.identifier
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          814
        </td>
        <td>
          10014
          -
          10081
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          fileStoreConfig.address.+(delimiter).+(VerticaDistributedFilesystemReadPipe.this.config.tableSource.identifier)
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          904
        </td>
        <td>
          10174
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getMetadata.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: com.vertica.spark.config.VerticaMetadata) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.configureSession(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Unit)](((x$5: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  val x$6: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Creating unique directory: &quot;.+(fileStoreConfig.address).+(&quot; with permissions: &quot;).+(perm))
  else
    (): Unit);
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Unit](x$5, perm, x$6)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$20: (Unit, com.vertica.spark.config.ValidFilePermissions, Unit)) =&gt; (x$20: (Unit, com.vertica.spark.config.ValidFilePermissions, Unit) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Unit)(Unit, com.vertica.spark.config.ValidFilePermissions, Unit)(_, (perm @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.createDir(fileStoreConfig.address, perm.toString()) match {
  case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; err.getUnderlyingError match {
    case (filename: String)com.vertica.spark.util.error.CreateDirectoryAlreadyExistsError(_) =&gt; {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Directory already existed: &quot;.+(fileStoreConfig.address))
      else
        (): Unit);
      scala.`package`.Right.apply[Nothing, Unit](())
    }
    case _ =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err.context(&quot;Failed to create directory: &quot;.+(fileStoreConfig.address)))
  }
  case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](_) =&gt; scala.`package`.Right.apply[Nothing, Unit](())
}.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).map[(Boolean, com.vertica.spark.config.ValidFilePermissions)](((exportDone: Boolean) =&gt; {
  val filePermissions: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  scala.Tuple2.apply[Boolean, com.vertica.spark.config.ValidFilePermissions](exportDone, filePermissions)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions)) =&gt; (x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions) @unchecked) match {
    case (_1: Boolean, _2: com.vertica.spark.config.ValidFilePermissions)(Boolean, com.vertica.spark.config.ValidFilePermissions)((exportDone @ _), (filePermissions @ _)) =&gt; this.getSelectClause.map[(String, Unit, String, String, Unit, String, Unit, String)](((selectClause: String) =&gt; {
  val x$7: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Select clause requested: &quot;.+(selectClause))
  else
    (): Unit);
  val groupbyClause: String = this.getGroupbyClause;
  val pushdownFilters: String = this.addPushdownFilters(this.config.getPushdownFilters);
  val x$8: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Pushdown filters: &quot;.+(pushdownFilters))
  else
    (): Unit);
  val exportSource: String = VerticaDistributedFilesystemReadPipe.this.config.tableSource match {
    case (tablename @ (_: com.vertica.spark.config.TableName)) =&gt; tablename.getFullTableName
    case (query: String, uniqueId: String)com.vertica.spark.config.TableQuery((query @ _), _) =&gt; &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
  };
  val x$9: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export Source: &quot;.+(exportSource))
  else
    (): Unit);
  val exportStatement: String = &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;);
  scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$18: (String, Unit, String, String, Unit, String, Unit, String)) =&gt; (x$18: (String, Unit, String, String, Unit, String, Unit, String) @unchecked) match {
      case (_1: String, _2: Unit, _3: String, _4: String, _5: Unit, _6: String, _7: Unit, _8: String)(String, Unit, String, String, Unit, String, Unit, String)((selectClause @ _), _, (groupbyClause @ _), (pushdownFilters @ _), _, (exportSource @ _), _, (exportStatement @ _)) =&gt; if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
        case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
          case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
        }))
      }))))))
    }))
  }))))
}))))
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          815
        </td>
        <td>
          10298
          -
          10312
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          817
        </td>
        <td>
          10266
          -
          10266
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Unit](x$5, perm, x$6)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          903
        </td>
        <td>
          10266
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer.configureSession(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer).map[(Unit, com.vertica.spark.config.ValidFilePermissions, Unit)](((x$5: Unit) =&gt; {
  val perm: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  val x$6: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Creating unique directory: &quot;.+(fileStoreConfig.address).+(&quot; with permissions: &quot;).+(perm))
  else
    (): Unit);
  scala.Tuple3.apply[Unit, com.vertica.spark.config.ValidFilePermissions, Unit](x$5, perm, x$6)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$20: (Unit, com.vertica.spark.config.ValidFilePermissions, Unit)) =&gt; (x$20: (Unit, com.vertica.spark.config.ValidFilePermissions, Unit) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.config.ValidFilePermissions, _3: Unit)(Unit, com.vertica.spark.config.ValidFilePermissions, Unit)(_, (perm @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.createDir(fileStoreConfig.address, perm.toString()) match {
  case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; err.getUnderlyingError match {
    case (filename: String)com.vertica.spark.util.error.CreateDirectoryAlreadyExistsError(_) =&gt; {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Directory already existed: &quot;.+(fileStoreConfig.address))
      else
        (): Unit);
      scala.`package`.Right.apply[Nothing, Unit](())
    }
    case _ =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err.context(&quot;Failed to create directory: &quot;.+(fileStoreConfig.address)))
  }
  case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](_) =&gt; scala.`package`.Right.apply[Nothing, Unit](())
}.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).map[(Boolean, com.vertica.spark.config.ValidFilePermissions)](((exportDone: Boolean) =&gt; {
  val filePermissions: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  scala.Tuple2.apply[Boolean, com.vertica.spark.config.ValidFilePermissions](exportDone, filePermissions)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions)) =&gt; (x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions) @unchecked) match {
    case (_1: Boolean, _2: com.vertica.spark.config.ValidFilePermissions)(Boolean, com.vertica.spark.config.ValidFilePermissions)((exportDone @ _), (filePermissions @ _)) =&gt; this.getSelectClause.map[(String, Unit, String, String, Unit, String, Unit, String)](((selectClause: String) =&gt; {
  val x$7: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Select clause requested: &quot;.+(selectClause))
  else
    (): Unit);
  val groupbyClause: String = this.getGroupbyClause;
  val pushdownFilters: String = this.addPushdownFilters(this.config.getPushdownFilters);
  val x$8: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Pushdown filters: &quot;.+(pushdownFilters))
  else
    (): Unit);
  val exportSource: String = VerticaDistributedFilesystemReadPipe.this.config.tableSource match {
    case (tablename @ (_: com.vertica.spark.config.TableName)) =&gt; tablename.getFullTableName
    case (query: String, uniqueId: String)com.vertica.spark.config.TableQuery((query @ _), _) =&gt; &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
  };
  val x$9: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export Source: &quot;.+(exportSource))
  else
    (): Unit);
  val exportStatement: String = &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;);
  scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$18: (String, Unit, String, String, Unit, String, Unit, String)) =&gt; (x$18: (String, Unit, String, String, Unit, String, Unit, String) @unchecked) match {
      case (_1: String, _2: Unit, _3: String, _4: String, _5: Unit, _6: String, _7: Unit, _8: String)(String, Unit, String, String, Unit, String, Unit, String)((selectClause @ _), _, (groupbyClause @ _), (pushdownFilters @ _), _, (exportSource @ _), _, (exportStatement @ _)) =&gt; if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
        case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
          case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
        }))
      }))))))
    }))
  }))))
}))
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          816
        </td>
        <td>
          10373
          -
          10395
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.filePermissions
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.filePermissions
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          819
        </td>
        <td>
          10569
          -
          10582
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ValidFilePermissions.toString
        </td>
        <td style="background: #AEF1AE">
          perm.toString()
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          902
        </td>
        <td>
          10514
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.createDir(fileStoreConfig.address, perm.toString()) match {
  case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; err.getUnderlyingError match {
    case (filename: String)com.vertica.spark.util.error.CreateDirectoryAlreadyExistsError(_) =&gt; {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Directory already existed: &quot;.+(fileStoreConfig.address))
      else
        (): Unit);
      scala.`package`.Right.apply[Nothing, Unit](())
    }
    case _ =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err.context(&quot;Failed to create directory: &quot;.+(fileStoreConfig.address)))
  }
  case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](_) =&gt; scala.`package`.Right.apply[Nothing, Unit](())
}.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).map[(Boolean, com.vertica.spark.config.ValidFilePermissions)](((exportDone: Boolean) =&gt; {
  val filePermissions: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  scala.Tuple2.apply[Boolean, com.vertica.spark.config.ValidFilePermissions](exportDone, filePermissions)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions)) =&gt; (x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions) @unchecked) match {
  case (_1: Boolean, _2: com.vertica.spark.config.ValidFilePermissions)(Boolean, com.vertica.spark.config.ValidFilePermissions)((exportDone @ _), (filePermissions @ _)) =&gt; this.getSelectClause.map[(String, Unit, String, String, Unit, String, Unit, String)](((selectClause: String) =&gt; {
  val x$7: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Select clause requested: &quot;.+(selectClause))
  else
    (): Unit);
  val groupbyClause: String = this.getGroupbyClause;
  val pushdownFilters: String = this.addPushdownFilters(this.config.getPushdownFilters);
  val x$8: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Pushdown filters: &quot;.+(pushdownFilters))
  else
    (): Unit);
  val exportSource: String = VerticaDistributedFilesystemReadPipe.this.config.tableSource match {
    case (tablename @ (_: com.vertica.spark.config.TableName)) =&gt; tablename.getFullTableName
    case (query: String, uniqueId: String)com.vertica.spark.config.TableQuery((query @ _), _) =&gt; &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
  };
  val x$9: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export Source: &quot;.+(exportSource))
  else
    (): Unit);
  val exportStatement: String = &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;);
  scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$18: (String, Unit, String, String, Unit, String, Unit, String)) =&gt; (x$18: (String, Unit, String, String, Unit, String, Unit, String) @unchecked) match {
    case (_1: String, _2: Unit, _3: String, _4: String, _5: Unit, _6: String, _7: Unit, _8: String)(String, Unit, String, String, Unit, String, Unit, String)((selectClause @ _), _, (groupbyClause @ _), (pushdownFilters @ _), _, (exportSource @ _), _, (exportStatement @ _)) =&gt; if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
      case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
        case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
      }))
    }))))))
  }))
}))))
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          818
        </td>
        <td>
          10544
          -
          10567
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #AEF1AE">
          fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          820
        </td>
        <td>
          10519
          -
          10583
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.createDir
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.createDir(fileStoreConfig.address, perm.toString())
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          821
        </td>
        <td>
          10628
          -
          10650
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.getUnderlyingError
        </td>
        <td style="background: #F0ADAD">
          err.getUnderlyingError
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          822
        </td>
        <td>
          10813
          -
          10822
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          824
        </td>
        <td>
          10895
          -
          10918
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.FileStoreConfig.address
        </td>
        <td style="background: #F0ADAD">
          fileStoreConfig.address
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          827
        </td>
        <td>
          10845
          -
          10920
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err.context(&quot;Failed to create directory: &quot;.+(fileStoreConfig.address)))
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          826
        </td>
        <td>
          10850
          -
          10919
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td style="background: #F0ADAD">
          err.context(&quot;Failed to create directory: &quot;.+(fileStoreConfig.address))
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          823
        </td>
        <td>
          10862
          -
          10892
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;Failed to create directory: &quot;
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          825
        </td>
        <td>
          10862
          -
          10918
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #F0ADAD">
          &quot;Failed to create directory: &quot;.+(fileStoreConfig.address)
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          828
        </td>
        <td>
          10958
          -
          10967
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          830
        </td>
        <td>
          11057
          -
          11057
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Boolean, com.vertica.spark.config.ValidFilePermissions](exportDone, filePermissions)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          901
        </td>
        <td>
          11057
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).map[(Boolean, com.vertica.spark.config.ValidFilePermissions)](((exportDone: Boolean) =&gt; {
  val filePermissions: com.vertica.spark.config.ValidFilePermissions = VerticaDistributedFilesystemReadPipe.this.config.filePermissions;
  scala.Tuple2.apply[Boolean, com.vertica.spark.config.ValidFilePermissions](exportDone, filePermissions)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions)) =&gt; (x$19: (Boolean, com.vertica.spark.config.ValidFilePermissions) @unchecked) match {
  case (_1: Boolean, _2: com.vertica.spark.config.ValidFilePermissions)(Boolean, com.vertica.spark.config.ValidFilePermissions)((exportDone @ _), (filePermissions @ _)) =&gt; this.getSelectClause.map[(String, Unit, String, String, Unit, String, Unit, String)](((selectClause: String) =&gt; {
  val x$7: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Select clause requested: &quot;.+(selectClause))
  else
    (): Unit);
  val groupbyClause: String = this.getGroupbyClause;
  val pushdownFilters: String = this.addPushdownFilters(this.config.getPushdownFilters);
  val x$8: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Pushdown filters: &quot;.+(pushdownFilters))
  else
    (): Unit);
  val exportSource: String = VerticaDistributedFilesystemReadPipe.this.config.tableSource match {
    case (tablename @ (_: com.vertica.spark.config.TableName)) =&gt; tablename.getFullTableName
    case (query: String, uniqueId: String)com.vertica.spark.config.TableQuery((query @ _), _) =&gt; &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
  };
  val x$9: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export Source: &quot;.+(exportSource))
  else
    (): Unit);
  val exportStatement: String = &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;);
  scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$18: (String, Unit, String, String, Unit, String, Unit, String)) =&gt; (x$18: (String, Unit, String, String, Unit, String, Unit, String) @unchecked) match {
    case (_1: String, _2: Unit, _3: String, _4: String, _5: Unit, _6: String, _7: Unit, _8: String)(String, Unit, String, String, Unit, String, Unit, String)((selectClause @ _), _, (groupbyClause @ _), (pushdownFilters @ _), _, (exportSource @ _), _, (exportStatement @ _)) =&gt; if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
      case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
        case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
      }))
    }))))))
  }))
}))
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          829
        </td>
        <td>
          11159
          -
          11181
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.filePermissions
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.filePermissions
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          838
        </td>
        <td>
          11189
          -
          11189
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple8.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          900
        </td>
        <td>
          11189
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          this.getSelectClause.map[(String, Unit, String, String, Unit, String, Unit, String)](((selectClause: String) =&gt; {
  val x$7: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Select clause requested: &quot;.+(selectClause))
  else
    (): Unit);
  val groupbyClause: String = this.getGroupbyClause;
  val pushdownFilters: String = this.addPushdownFilters(this.config.getPushdownFilters);
  val x$8: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Pushdown filters: &quot;.+(pushdownFilters))
  else
    (): Unit);
  val exportSource: String = VerticaDistributedFilesystemReadPipe.this.config.tableSource match {
    case (tablename @ (_: com.vertica.spark.config.TableName)) =&gt; tablename.getFullTableName
    case (query: String, uniqueId: String)com.vertica.spark.config.TableQuery((query @ _), _) =&gt; &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
  };
  val x$9: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export Source: &quot;.+(exportSource))
  else
    (): Unit);
  val exportStatement: String = &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;);
  scala.Tuple8.apply[String, Unit, String, String, Unit, String, Unit, String](selectClause, x$7, groupbyClause, pushdownFilters, x$8, exportSource, x$9, exportStatement)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$18: (String, Unit, String, String, Unit, String, Unit, String)) =&gt; (x$18: (String, Unit, String, String, Unit, String, Unit, String) @unchecked) match {
  case (_1: String, _2: Unit, _3: String, _4: String, _5: Unit, _6: String, _7: Unit, _8: String)(String, Unit, String, String, Unit, String, Unit, String)((selectClause @ _), _, (groupbyClause @ _), (pushdownFilters @ _), _, (exportSource @ _), _, (exportStatement @ _)) =&gt; if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
    case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
      case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
    }))
  }))))))
}))
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          831
        </td>
        <td>
          11315
          -
          11336
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getGroupbyClause
        </td>
        <td style="background: #AEF1AE">
          this.getGroupbyClause
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          833
        </td>
        <td>
          11362
          -
          11417
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.addPushdownFilters
        </td>
        <td style="background: #AEF1AE">
          this.addPushdownFilters(this.config.getPushdownFilters)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          832
        </td>
        <td>
          11386
          -
          11416
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.getPushdownFilters
        </td>
        <td style="background: #AEF1AE">
          this.config.getPushdownFilters
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          834
        </td>
        <td>
          11502
          -
          11520
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.tableSource
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.tableSource
        </td>
      </tr><tr>
        <td>
          263
        </td>
        <td>
          835
        </td>
        <td>
          11566
          -
          11592
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.TableName.getFullTableName
        </td>
        <td style="background: #AEF1AE">
          tablename.getFullTableName
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          836
        </td>
        <td>
          11630
          -
          11652
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;(&quot;.+(query).+(&quot;) AS x&quot;)
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          837
        </td>
        <td>
          11742
          -
          12089
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td style="background: #AEF1AE">
          &quot;EXPORT TO PARQUET(directory = \'&quot;.+(hdfsPath).+(&quot;\', fileSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxFileSize).+(&quot;, rowGroupSizeMB = &quot;).+(VerticaDistributedFilesystemReadPipe.this.maxRowGroupSize).+(&quot;, fileMode = \'&quot;).+(filePermissions).+(&quot;\', dirMode = \'&quot;).+(filePermissions).+(&quot;\') AS SELECT &quot;).+(selectClause).+(&quot; FROM &quot;).+(exportSource).+(pushdownFilters).+(groupbyClause).+(&quot;;&quot;)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          899
        </td>
        <td>
          12137
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (exportDone)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
    else
      (): Unit);
    scala.`package`.Right.apply[Nothing, Unit](())
  }
else
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
    else
      (): Unit);
    val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
    timer.startTime();
    val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
    VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
    timer.endTime();
    res
  }.flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
  case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
    case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
  }))
}))))))
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          840
        </td>
        <td>
          12157
          -
          12250
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Export already done, skipping export step.&quot;)
  else
    (): Unit);
  scala.`package`.Right.apply[Nothing, Unit](())
}
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          839
        </td>
        <td>
          12233
          -
          12242
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          854
        </td>
        <td>
          12256
          -
          12668
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Exporting using statement: \n&quot;.+(exportStatement))
  else
    (): Unit);
  val timer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;);
  timer.startTime();
  val res: Either[com.vertica.spark.util.error.ExportFromVerticaError,Unit] = cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)));
  VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config));
  timer.endTime();
  res
}
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          842
        </td>
        <td>
          12383
          -
          12389
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.logger
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          844
        </td>
        <td>
          12350
          -
          12424
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Export To Parquet From Vertica&quot;)
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          841
        </td>
        <td>
          12360
          -
          12381
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          283
        </td>
        <td>
          843
        </td>
        <td>
          12391
          -
          12423
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Export To Parquet From Vertica&quot;
        </td>
      </tr><tr>
        <td>
          284
        </td>
        <td>
          845
        </td>
        <td>
          12433
          -
          12450
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          timer.startTime()
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          846
        </td>
        <td>
          12479
          -
          12479
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.execute$default$2
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          848
        </td>
        <td>
          12519
          -
          12546
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ExportFromVerticaError.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ExportFromVerticaError.apply(err)
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          847
        </td>
        <td>
          12469
          -
          12503
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.execute
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          849
        </td>
        <td>
          12469
          -
          12547
        </td>
        <td>
          Apply
        </td>
        <td>
          cats.syntax.EitherOps.leftMap
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsSyntaxEither[com.vertica.spark.util.error.ConnectorError, Unit](VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute(exportStatement, VerticaDistributedFilesystemReadPipe.this.jdbcLayer.execute$default$2)).leftMap[com.vertica.spark.util.error.ExportFromVerticaError](((err: com.vertica.spark.util.error.ConnectorError) =&gt; com.vertica.spark.util.error.ExportFromVerticaError.apply(err)))
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          851
        </td>
        <td>
          12586
          -
          12623
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.listeners.ApplicationParquetCleaner.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config)
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          850
        </td>
        <td>
          12616
          -
          12622
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.config
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          852
        </td>
        <td>
          12556
          -
          12624
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.listeners.SparkContextWrapper.addSparkListener
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.sparkContext.addSparkListener(new com.vertica.spark.util.listeners.ApplicationParquetCleaner(VerticaDistributedFilesystemReadPipe.this.config))
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          853
        </td>
        <td>
          12633
          -
          12648
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          timer.endTime()
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          898
        </td>
        <td>
          12731
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.fileExists(hdfsPath).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((dirExists: Boolean) =&gt; if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
  case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
    case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
  }))
}))))
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          855
        </td>
        <td>
          12805
          -
          12815
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          dirExists.unary_!
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          857
        </td>
        <td>
          12817
          -
          12830
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          860
        </td>
        <td>
          12836
          -
          12872
        </td>
        <td>
          Block
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.getFileList
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          856
        </td>
        <td>
          12823
          -
          12829
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td style="background: #F0ADAD">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          859
        </td>
        <td>
          12836
          -
          12872
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.getFileList
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          858
        </td>
        <td>
          12817
          -
          12830
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          897
        </td>
        <td>
          12786
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (dirExists.unary_!)
  scala.`package`.Right.apply[Nothing, List[Nothing]](scala.collection.immutable.Nil)
else
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getFileList(hdfsPath).map[(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)](((fullFileList: Seq[String]) =&gt; {
  val parquetFileList: Seq[String] = fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)));
  val requestedPartitionCount: Int = VerticaDistributedFilesystemReadPipe.this.config.partitionCount match {
    case (value: Int)Some[Int]((count @ _)) =&gt; count
    case scala.None =&gt; parquetFileList.size
  };
  val x$10: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Requested partition count: &quot;.+(requestedPartitionCount))
  else
    (): Unit);
  val x$11: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Parquet file list size: &quot;.+(parquetFileList.size))
  else
    (): Unit);
  val partitionTimer: com.vertica.spark.util.Timer = new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;);
  val x$12: Unit = partitionTimer.startTime();
  scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)) =&gt; (x$17: (Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit) @unchecked) match {
  case (_1: Seq[String], _2: Seq[String], _3: Int, _4: Unit, _5: Unit, _6: com.vertica.spark.util.Timer, _7: Unit)(Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit)((fullFileList @ _), (parquetFileList @ _), (requestedPartitionCount @ _), _, _, (partitionTimer @ _), _) =&gt; cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
    case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
  }))
}))
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          870
        </td>
        <td>
          12786
          -
          12786
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple7.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple7.apply[Seq[String], Seq[String], Int, Unit, Unit, com.vertica.spark.util.Timer, Unit](fullFileList, parquetFileList, requestedPartitionCount, x$10, x$11, partitionTimer, x$12)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          862
        </td>
        <td>
          12897
          -
          12945
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td style="background: #AEF1AE">
          fullFileList.filter(((x: String) =&gt; x.endsWith(&quot;.parquet&quot;)))
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          861
        </td>
        <td>
          12922
          -
          12944
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.endsWith
        </td>
        <td style="background: #AEF1AE">
          x.endsWith(&quot;.parquet&quot;)
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          863
        </td>
        <td>
          12978
          -
          12999
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.partitionCount
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.partitionCount
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          864
        </td>
        <td>
          13067
          -
          13087
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td style="background: #AEF1AE">
          parquetFileList.size
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          866
        </td>
        <td>
          13339
          -
          13345
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.logger
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          865
        </td>
        <td>
          13316
          -
          13337
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          868
        </td>
        <td>
          13306
          -
          13404
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Reading Parquet Files Metadata and creating partitions&quot;)
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          867
        </td>
        <td>
          13347
          -
          13403
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Reading Parquet Files Metadata and creating partitions&quot;
        </td>
      </tr><tr>
        <td>
          304
        </td>
        <td>
          869
        </td>
        <td>
          13415
          -
          13441
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          partitionTimer.startTime()
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          873
        </td>
        <td>
          13509
          -
          13556
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.getParquetFileMetadata
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          872
        </td>
        <td>
          13481
          -
          13481
        </td>
        <td>
          Select
        </td>
        <td>
          cats.instances.ListInstances.catsStdInstancesForList
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsStdInstancesForList
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          896
        </td>
        <td>
          13449
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.toTraverseOps[List, String](parquetFileList.toList)(cats.implicits.catsStdInstancesForList).traverse[com.vertica.spark.util.error.ErrorHandling.ConnectorResult, com.vertica.spark.datasource.fs.ParquetFileMetadata](((filename: String) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.getParquetFileMetadata(filename)))(cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]).map[(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)](((fileMetadata: List[com.vertica.spark.datasource.fs.ParquetFileMetadata]) =&gt; {
  val totalRowGroups: Int = fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral);
  val x$13: Unit = (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Total row groups: &quot;.+(totalRowGroups))
  else
    (): Unit);
  val x$14: Any = if (totalRowGroups.==(0))
    if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
      {
        (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
          VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
        else
          (): Unit);
        VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
      }
    else
      scala.`package`.Right.apply[Nothing, Unit](())
  else
    ();
  val partitionCount: Int = if (totalRowGroups.&lt;(requestedPartitionCount))
    {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
      else
        (): Unit);
      totalRowGroups
    }
  else
    requestedPartitionCount;
  val partitionInfo: com.vertica.spark.datasource.core.PartitionInfo = VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount);
  val x$15: Unit = partitionTimer.endTime();
  scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
})).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.PartitionInfo](((x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)) =&gt; (x$16: (List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit) @unchecked) match {
  case (_1: List[com.vertica.spark.datasource.fs.ParquetFileMetadata], _2: Int, _3: Unit, _4: Any, _5: Int, _6: com.vertica.spark.datasource.core.PartitionInfo, _7: Unit)(List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit)((fileMetadata @ _), (totalRowGroups @ _), _, _, (partitionCount @ _), (partitionInfo @ _), _) =&gt; VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
}))
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          871
        </td>
        <td>
          13465
          -
          13487
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.toList
        </td>
        <td style="background: #AEF1AE">
          parquetFileList.toList
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          874
        </td>
        <td>
          13496
          -
          13496
        </td>
        <td>
          TypeApply
        </td>
        <td>
          cats.instances.EitherInstances.catsStdInstancesForEither
        </td>
        <td style="background: #AEF1AE">
          cats.implicits.catsStdInstancesForEither[com.vertica.spark.util.error.ConnectorError]
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          894
        </td>
        <td>
          13449
          -
          13449
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple7.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple7.apply[List[com.vertica.spark.datasource.fs.ParquetFileMetadata], Int, Unit, Any, Int, com.vertica.spark.datasource.core.PartitionInfo, Unit](fileMetadata, totalRowGroups, x$13, x$14, partitionCount, partitionInfo, x$15)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          875
        </td>
        <td>
          13598
          -
          13613
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.fs.ParquetFileMetadata.rowGroupCount
        </td>
        <td style="background: #AEF1AE">
          x$4.rowGroupCount
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          878
        </td>
        <td>
          13581
          -
          13618
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.sum
        </td>
        <td style="background: #AEF1AE">
          fileMetadata.map[Int, List[Int]](((x$4: com.vertica.spark.datasource.fs.ParquetFileMetadata) =&gt; x$4.rowGroupCount))(immutable.this.List.canBuildFrom[Int]).sum[Int](math.this.Numeric.IntIsIntegral)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          877
        </td>
        <td>
          13615
          -
          13615
        </td>
        <td>
          Select
        </td>
        <td>
          scala.math.Numeric.IntIsIntegral
        </td>
        <td style="background: #AEF1AE">
          math.this.Numeric.IntIsIntegral
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          876
        </td>
        <td>
          13597
          -
          13597
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[Int]
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          887
        </td>
        <td>
          13728
          -
          13728
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          888
        </td>
        <td>
          13728
          -
          13728
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          879
        </td>
        <td>
          13731
          -
          13750
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td style="background: #AEF1AE">
          totalRowGroups.==(0)
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          886
        </td>
        <td>
          13762
          -
          13994
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
    else
      (): Unit);
    VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
  }
else
  scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          880
        </td>
        <td>
          13765
          -
          13803
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          883
        </td>
        <td>
          13805
          -
          13951
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isDebugEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.debug(&quot;Cleaning up empty directory in path: &quot;.+(hdfsPath))
  else
    (): Unit);
  VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
}
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          882
        </td>
        <td>
          13892
          -
          13941
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.CleanupUtilsInterface.cleanupAll
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          881
        </td>
        <td>
          13916
          -
          13930
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          884
        </td>
        <td>
          13977
          -
          13984
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          885
        </td>
        <td>
          13977
          -
          13984
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          890
        </td>
        <td>
          14073
          -
          14220
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Less than &quot;.+(requestedPartitionCount).+(&quot; partitions required, only using &quot;).+(totalRowGroups))
  else
    (): Unit);
  totalRowGroups
}
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          889
        </td>
        <td>
          14031
          -
          14071
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td style="background: #AEF1AE">
          totalRowGroups.&lt;(requestedPartitionCount)
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          891
        </td>
        <td>
          14236
          -
          14259
        </td>
        <td>
          Ident
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.requestedPartitionCount
        </td>
        <td style="background: #AEF1AE">
          requestedPartitionCount
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          892
        </td>
        <td>
          14291
          -
          14337
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getPartitionInfo
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getPartitionInfo(fileMetadata, partitionCount)
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          893
        </td>
        <td>
          14349
          -
          14373
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          partitionTimer.endTime()
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          895
        </td>
        <td>
          14381
          -
          14429
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close().map[com.vertica.spark.datasource.core.PartitionInfo](((_: Unit) =&gt; partitionInfo))
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          908
        </td>
        <td>
          14556
          -
          14695
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Cleaning up all files in path: &quot;.+(hdfsPath))
  else
    (): Unit);
  VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
}
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          905
        </td>
        <td>
          14516
          -
          14554
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          910
        </td>
        <td>
          14513
          -
          14513
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          909
        </td>
        <td>
          14513
          -
          14513
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          907
        </td>
        <td>
          14636
          -
          14685
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.CleanupUtilsInterface.cleanupAll
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.cleanupAll(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, hdfsPath)
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          906
        </td>
        <td>
          14660
          -
          14674
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          911
        </td>
        <td>
          14704
          -
          14721
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.jdbc.JdbcLayerInterface.close
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.jdbcLayer.close()
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          912
        </td>
        <td>
          14871
          -
          14875
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          913
        </td>
        <td>
          14892
          -
          14893
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          917
        </td>
        <td>
          14910
          -
          14968
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new com.vertica.spark.util.Timer(VerticaDistributedFilesystemReadPipe.this.config.timeOperations, VerticaDistributedFilesystemReadPipe.this.logger, &quot;Partition Read&quot;)
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          914
        </td>
        <td>
          14920
          -
          14941
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.config.DistributedFilesystemReadConfig.timeOperations
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.timeOperations
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          916
        </td>
        <td>
          14951
          -
          14967
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Partition Read&quot;
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          915
        </td>
        <td>
          14943
          -
          14949
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.logger
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.logger
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          918
        </td>
        <td>
          15154
          -
          15171
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.startTime
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.timer.startTime()
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          931
        </td>
        <td>
          15220
          -
          15775
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          verticaPartition match {
  case (p @ (_: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition)) =&gt; scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition](p)
  case _ =&gt; scala.`package`.Left.apply[com.vertica.spark.util.error.InvalidPartition, Nothing](com.vertica.spark.util.error.InvalidPartition.apply())
}.map[(com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit)](((part: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition) =&gt; {
  val x$21: Unit = this.partition_=(scala.Some.apply[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition](part));
  val x$22: Unit = this.fileIdx_=(0);
  scala.Tuple3.apply[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit](part, x$21, x$22)
})).flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((x$23: (com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit)) =&gt; (x$23: (com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit) @unchecked) match {
  case (_1: com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, _2: Unit, _3: Unit)(com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit)((part @ _), _, _) =&gt; part.fileRanges.headOption match {
  case scala.None =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;No files to read set on partition.&quot;)
    else
      (): Unit);
    scala.`package`.Left.apply[com.vertica.spark.util.error.DoneReading, Nothing](com.vertica.spark.util.error.DoneReading.apply())
  }
  case (value: com.vertica.spark.datasource.core.ParquetFileRange)Some[com.vertica.spark.datasource.core.ParquetFileRange]((head @ _)) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(head)
}.map[Unit](((ret: Unit) =&gt; ret))
}))
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          925
        </td>
        <td>
          15232
          -
          15232
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition, Unit, Unit](part, x$21, x$22)
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          919
        </td>
        <td>
          15324
          -
          15332
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition](p)
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          920
        </td>
        <td>
          15358
          -
          15376
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InvalidPartition.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InvalidPartition.apply()
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          921
        </td>
        <td>
          15353
          -
          15377
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.InvalidPartition, Nothing](com.vertica.spark.util.error.InvalidPartition.apply())
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          923
        </td>
        <td>
          15398
          -
          15425
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.partition_=
        </td>
        <td style="background: #AEF1AE">
          this.partition_=(scala.Some.apply[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition](part))
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          922
        </td>
        <td>
          15415
          -
          15425
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition](part)
        </td>
      </tr><tr>
        <td>
          367
        </td>
        <td>
          924
        </td>
        <td>
          15436
          -
          15452
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileIdx_=
        </td>
        <td style="background: #AEF1AE">
          this.fileIdx_=(0)
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          926
        </td>
        <td>
          15528
          -
          15554
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.headOption
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.headOption
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          930
        </td>
        <td>
          15521
          -
          15775
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.headOption match {
  case scala.None =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;No files to read set on partition.&quot;)
    else
      (): Unit);
    scala.`package`.Left.apply[com.vertica.spark.util.error.DoneReading, Nothing](com.vertica.spark.util.error.DoneReading.apply())
  }
  case (value: com.vertica.spark.datasource.core.ParquetFileRange)Some[com.vertica.spark.datasource.core.ParquetFileRange]((head @ _)) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(head)
}.map[Unit](((ret: Unit) =&gt; ret))
        </td>
      </tr><tr>
        <td>
          373
        </td>
        <td>
          928
        </td>
        <td>
          15654
          -
          15673
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.DoneReading, Nothing](com.vertica.spark.util.error.DoneReading.apply())
        </td>
      </tr><tr>
        <td>
          373
        </td>
        <td>
          927
        </td>
        <td>
          15659
          -
          15672
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.DoneReading.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.DoneReading.apply()
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          929
        </td>
        <td>
          15711
          -
          15751
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.openReadParquetFile
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(head)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          932
        </td>
        <td>
          16006
          -
          16026
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.size
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          935
        </td>
        <td>
          16028
          -
          16134
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Invalid fileIdx &quot;.+(this.fileIdx).+(&quot;, can\'t perform cleanup.&quot;))
  else
    (): Unit);
  scala.None
}
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          953
        </td>
        <td>
          15975
          -
          16959
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.flatMap
        </td>
        <td style="background: #AEF1AE">
          if (curIdx.&gt;=(part.fileRanges.size))
  {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Invalid fileIdx &quot;.+(this.fileIdx).+(&quot;, can\'t perform cleanup.&quot;))
    else
      (): Unit);
    scala.None
  }
else
  scala.Some.apply[Unit](()).map[(Unit, com.vertica.spark.datasource.core.ParquetFileRange)](((x$24: Unit) =&gt; {
  val curRange: com.vertica.spark.datasource.core.ParquetFileRange = part.fileRanges.apply(curIdx);
  scala.Tuple2.apply[Unit, com.vertica.spark.datasource.core.ParquetFileRange](x$24, curRange)
})).flatMap[com.vertica.spark.util.cleanup.FileCleanupInfo](((x$25: (Unit, com.vertica.spark.datasource.core.ParquetFileRange)) =&gt; (x$25: (Unit, com.vertica.spark.datasource.core.ParquetFileRange) @unchecked) match {
  case (_1: Unit, _2: com.vertica.spark.datasource.core.ParquetFileRange)(Unit, com.vertica.spark.datasource.core.ParquetFileRange)(_, (curRange @ _)) =&gt; part.rangeCountMap match {
  case (value: Map[String,Int])Some[Map[String,Int]]((rangeCountMap @ _)) if rangeCountMap.contains(curRange.filename) =&gt; curRange.rangeIdx match {
    case (value: Int)Some[Int]((rangeIdx @ _)) =&gt; scala.Some.apply[com.vertica.spark.util.cleanup.FileCleanupInfo](com.vertica.spark.util.cleanup.FileCleanupInfo.apply(curRange.filename, rangeIdx, rangeCountMap.apply(curRange.filename)))
    case scala.None =&gt; {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing range count index. Not performing any cleanup for file &quot;.+(curRange.filename))
      else
        (): Unit);
      scala.None
    }
  }
  case scala.None =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing range count map. Not performing any cleanup for file &quot;.+(curRange.filename))
    else
      (): Unit);
    scala.None
  }
  case _ =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing value in range count map. Not performing any cleanup for file &quot;.+(curRange.filename))
    else
      (): Unit);
    scala.None
  }
}.map[com.vertica.spark.util.cleanup.FileCleanupInfo](((ret: com.vertica.spark.util.cleanup.FileCleanupInfo) =&gt; ret))
}))
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          939
        </td>
        <td>
          15987
          -
          15987
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[Unit, com.vertica.spark.datasource.core.ParquetFileRange](x$24, curRange)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          933
        </td>
        <td>
          15996
          -
          16026
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;=
        </td>
        <td style="background: #AEF1AE">
          curIdx.&gt;=(part.fileRanges.size)
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          934
        </td>
        <td>
          16122
          -
          16126
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          937
        </td>
        <td>
          16150
          -
          16158
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Unit](())
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          936
        </td>
        <td>
          16150
          -
          16158
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Unit](())
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          938
        </td>
        <td>
          16185
          -
          16208
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.apply(curIdx)
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          940
        </td>
        <td>
          16222
          -
          16240
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemPartition.rangeCountMap
        </td>
        <td style="background: #AEF1AE">
          part.rangeCountMap
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          952
        </td>
        <td>
          16215
          -
          16959
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.map
        </td>
        <td style="background: #AEF1AE">
          part.rangeCountMap match {
  case (value: Map[String,Int])Some[Map[String,Int]]((rangeCountMap @ _)) if rangeCountMap.contains(curRange.filename) =&gt; curRange.rangeIdx match {
    case (value: Int)Some[Int]((rangeIdx @ _)) =&gt; scala.Some.apply[com.vertica.spark.util.cleanup.FileCleanupInfo](com.vertica.spark.util.cleanup.FileCleanupInfo.apply(curRange.filename, rangeIdx, rangeCountMap.apply(curRange.filename)))
    case scala.None =&gt; {
      (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing range count index. Not performing any cleanup for file &quot;.+(curRange.filename))
      else
        (): Unit);
      scala.None
    }
  }
  case scala.None =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing range count map. Not performing any cleanup for file &quot;.+(curRange.filename))
    else
      (): Unit);
    scala.None
  }
  case _ =&gt; {
    (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Missing value in range count map. Not performing any cleanup for file &quot;.+(curRange.filename))
    else
      (): Unit);
    scala.None
  }
}.map[com.vertica.spark.util.cleanup.FileCleanupInfo](((ret: com.vertica.spark.util.cleanup.FileCleanupInfo) =&gt; ret))
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          941
        </td>
        <td>
          16310
          -
          16327
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.filename
        </td>
        <td style="background: #AEF1AE">
          curRange.filename
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          943
        </td>
        <td>
          16332
          -
          16349
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.rangeIdx
        </td>
        <td style="background: #AEF1AE">
          curRange.rangeIdx
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          942
        </td>
        <td>
          16286
          -
          16328
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td style="background: #AEF1AE">
          rangeCountMap.contains(curRange.filename)
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          944
        </td>
        <td>
          16415
          -
          16432
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.filename
        </td>
        <td style="background: #AEF1AE">
          curRange.filename
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          947
        </td>
        <td>
          16398
          -
          16478
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.FileCleanupInfo.apply
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.cleanup.FileCleanupInfo.apply(curRange.filename, rangeIdx, rangeCountMap.apply(curRange.filename))
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          946
        </td>
        <td>
          16444
          -
          16477
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td style="background: #AEF1AE">
          rangeCountMap.apply(curRange.filename)
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          945
        </td>
        <td>
          16459
          -
          16476
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.ParquetFileRange.filename
        </td>
        <td style="background: #AEF1AE">
          curRange.filename
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          948
        </td>
        <td>
          16392
          -
          16479
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.vertica.spark.util.cleanup.FileCleanupInfo](com.vertica.spark.util.cleanup.FileCleanupInfo.apply(curRange.filename, rangeIdx, rangeCountMap.apply(curRange.filename)))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          949
        </td>
        <td>
          16627
          -
          16631
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          950
        </td>
        <td>
          16781
          -
          16785
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          951
        </td>
        <td>
          16931
          -
          16935
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          954
        </td>
        <td>
          17255
          -
          17269
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.partition
        </td>
        <td style="background: #AEF1AE">
          this.partition
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          956
        </td>
        <td>
          17304
          -
          17334
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.UninitializedReadError, Nothing](com.vertica.spark.util.error.UninitializedReadError.apply())
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          955
        </td>
        <td>
          17309
          -
          17333
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.UninitializedReadError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.UninitializedReadError.apply()
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          958
        </td>
        <td>
          17380
          -
          17428
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.fs.FileStoreLayerInterface.readDataFromParquetFile
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.readDataFromParquetFile(VerticaDistributedFilesystemReadPipe.this.dataSize)
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          957
        </td>
        <td>
          17419
          -
          17427
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.dataSize
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.dataSize
        </td>
      </tr><tr>
        <td>
          421
        </td>
        <td>
          959
        </td>
        <td>
          17461
          -
          17470
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Left.apply[com.vertica.spark.util.error.ConnectorError, Nothing](err)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          960
        </td>
        <td>
          17508
          -
          17526
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td style="background: #AEF1AE">
          data.data.nonEmpty
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          962
        </td>
        <td>
          17540
          -
          17551
        </td>
        <td>
          Block
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.DataBlock](data)
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          961
        </td>
        <td>
          17540
          -
          17551
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.DataBlock](data)
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          977
        </td>
        <td>
          17575
          -
          18016
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isInfoEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.info(&quot;Hit done reading for file segment.&quot;)
  else
    (): Unit);
  this.fileIdx_=(this.fileIdx.+(1));
  if (this.fileIdx.&gt;=(part.fileRanges.size))
    return scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.DataBlock](data)
  else
    ();
  VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.closeReadParquetFile().flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.DataBlock](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(part.fileRanges.apply(this.fileIdx)).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.DataBlock](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.readDataFromParquetFile(VerticaDistributedFilesystemReadPipe.this.dataSize).map[com.vertica.spark.datasource.core.DataBlock](((data: com.vertica.spark.datasource.core.DataBlock) =&gt; data))))))
}
        </td>
      </tr><tr>
        <td>
          430
        </td>
        <td>
          964
        </td>
        <td>
          17671
          -
          17688
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileIdx_=
        </td>
        <td style="background: #AEF1AE">
          this.fileIdx_=(this.fileIdx.+(1))
        </td>
      </tr><tr>
        <td>
          430
        </td>
        <td>
          963
        </td>
        <td>
          17671
          -
          17688
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          this.fileIdx.+(1)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          968
        </td>
        <td>
          17741
          -
          17759
        </td>
        <td>
          Return
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.readData
        </td>
        <td style="background: #AEF1AE">
          return scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.DataBlock](data)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          965
        </td>
        <td>
          17719
          -
          17739
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.size
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          967
        </td>
        <td>
          17748
          -
          17759
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, com.vertica.spark.datasource.core.DataBlock](data)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          970
        </td>
        <td>
          17699
          -
          17699
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          969
        </td>
        <td>
          17699
          -
          17699
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          966
        </td>
        <td>
          17703
          -
          17739
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;=
        </td>
        <td style="background: #AEF1AE">
          this.fileIdx.&gt;=(part.fileRanges.size)
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          976
        </td>
        <td>
          17771
          -
          18006
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.closeReadParquetFile().flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.DataBlock](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(part.fileRanges.apply(this.fileIdx)).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.DataBlock](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.readDataFromParquetFile(VerticaDistributedFilesystemReadPipe.this.dataSize).map[com.vertica.spark.datasource.core.DataBlock](((data: com.vertica.spark.datasource.core.DataBlock) =&gt; data))))))
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          971
        </td>
        <td>
          17900
          -
          17912
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileIdx
        </td>
        <td style="background: #AEF1AE">
          this.fileIdx
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          972
        </td>
        <td>
          17884
          -
          17913
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.apply(this.fileIdx)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          975
        </td>
        <td>
          17844
          -
          18006
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.openReadParquetFile(part.fileRanges.apply(this.fileIdx)).flatMap[com.vertica.spark.util.error.ConnectorError, com.vertica.spark.datasource.core.DataBlock](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.readDataFromParquetFile(VerticaDistributedFilesystemReadPipe.this.dataSize).map[com.vertica.spark.datasource.core.DataBlock](((data: com.vertica.spark.datasource.core.DataBlock) =&gt; data))))
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          974
        </td>
        <td>
          17927
          -
          18006
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.readDataFromParquetFile(VerticaDistributedFilesystemReadPipe.this.dataSize).map[com.vertica.spark.datasource.core.DataBlock](((data: com.vertica.spark.datasource.core.DataBlock) =&gt; data))
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          973
        </td>
        <td>
          17974
          -
          17982
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.dataSize
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.dataSize
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          983
        </td>
        <td>
          18182
          -
          18182
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          980
        </td>
        <td>
          18225
          -
          18282
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.CleanupUtilsInterface.checkAndCleanup
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          982
        </td>
        <td>
          18225
          -
          18429
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo) match {
  case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](()) =&gt; ()
  case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Ran into error when cleaning up: &quot;.+(err.getFullContext))
  else
    (): Unit)
}
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          979
        </td>
        <td>
          18254
          -
          18268
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          978
        </td>
        <td>
          18185
          -
          18223
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          984
        </td>
        <td>
          18182
          -
          18182
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          981
        </td>
        <td>
          18319
          -
          18321
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          992
        </td>
        <td>
          18557
          -
          18557
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          986
        </td>
        <td>
          18560
          -
          18624
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          dataBlock.data.isEmpty.&amp;&amp;(VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          991
        </td>
        <td>
          18557
          -
          18557
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          985
        </td>
        <td>
          18586
          -
          18624
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          988
        </td>
        <td>
          18626
          -
          18683
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.CleanupUtilsInterface.checkAndCleanup
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo)
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          987
        </td>
        <td>
          18655
          -
          18669
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          990
        </td>
        <td>
          18626
          -
          18830
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo) match {
  case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](()) =&gt; ()
  case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Ran into error when cleaning up: &quot;.+(err.getFullContext))
  else
    (): Unit)
}
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          989
        </td>
        <td>
          18720
          -
          18722
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          993
        </td>
        <td>
          18862
          -
          18864
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          994
        </td>
        <td>
          19057
          -
          19072
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.Timer.endTime
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.timer.endTime()
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          997
        </td>
        <td>
          19077
          -
          19172
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.flatMap
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.cleanupFiles().flatMap[com.vertica.spark.util.error.ConnectorError, Unit](((_: Unit) =&gt; VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.closeReadParquetFile().map[Unit](((_: Unit) =&gt; ()))))
        </td>
      </tr><tr>
        <td>
          467
        </td>
        <td>
          996
        </td>
        <td>
          19115
          -
          19172
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Either.map
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer.closeReadParquetFile().map[Unit](((_: Unit) =&gt; ()))
        </td>
      </tr><tr>
        <td>
          468
        </td>
        <td>
          995
        </td>
        <td>
          19170
          -
          19172
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          998
        </td>
        <td>
          19300
          -
          19314
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.partition
        </td>
        <td style="background: #AEF1AE">
          this.partition
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          1000
        </td>
        <td>
          19349
          -
          19379
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Left.apply
        </td>
        <td style="background: #F0ADAD">
          scala.`package`.Left.apply[com.vertica.spark.util.error.UninitializedReadError, Nothing](com.vertica.spark.util.error.UninitializedReadError.apply())
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          999
        </td>
        <td>
          19354
          -
          19378
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.UninitializedReadError.apply
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.UninitializedReadError.apply()
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1001
        </td>
        <td>
          19430
          -
          19431
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1011
        </td>
        <td>
          19415
          -
          19956
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.Range.foreach
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.intWrapper(0).to(part.fileRanges.size).foreach[Unit](((fileIdx: Int) =&gt; if (VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!)
  VerticaDistributedFilesystemReadPipe.this.getCleanupInfo(part, fileIdx) match {
    case (value: com.vertica.spark.util.cleanup.FileCleanupInfo)Some[com.vertica.spark.util.cleanup.FileCleanupInfo]((cleanupInfo @ _)) =&gt; VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo) match {
      case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
        VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Ran into error when calling cleaning up. Treating as non-fatal. Err: &quot;.+(err.getFullContext))
      else
        (): Unit)
      case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](_) =&gt; ()
    }
    case scala.None =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;No cleanup info found.&quot;)
    else
      (): Unit)
  }
else
  ()))
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          1002
        </td>
        <td>
          19435
          -
          19455
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td style="background: #AEF1AE">
          part.fileRanges.size
        </td>
      </tr><tr>
        <td>
          479
        </td>
        <td>
          1010
        </td>
        <td>
          19465
          -
          19465
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          479
        </td>
        <td>
          1009
        </td>
        <td>
          19465
          -
          19465
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          479
        </td>
        <td>
          1003
        </td>
        <td>
          19468
          -
          19506
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.config.fileStoreConfig.preventCleanup.unary_!
        </td>
      </tr><tr>
        <td>
          481
        </td>
        <td>
          1004
        </td>
        <td>
          19558
          -
          19587
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.getCleanupInfo
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getCleanupInfo(part, fileIdx)
        </td>
      </tr><tr>
        <td>
          481
        </td>
        <td>
          1008
        </td>
        <td>
          19558
          -
          19948
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.getCleanupInfo(part, fileIdx) match {
  case (value: com.vertica.spark.util.cleanup.FileCleanupInfo)Some[com.vertica.spark.util.cleanup.FileCleanupInfo]((cleanupInfo @ _)) =&gt; VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo) match {
    case (value: com.vertica.spark.util.error.ConnectorError)scala.util.Left[com.vertica.spark.util.error.ConnectorError,Unit]((err @ _)) =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
      VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;Ran into error when calling cleaning up. Treating as non-fatal. Err: &quot;.+(err.getFullContext))
    else
      (): Unit)
    case (value: Unit)scala.util.Right[com.vertica.spark.util.error.ConnectorError,Unit](_) =&gt; ()
  }
  case scala.None =&gt; (if (VerticaDistributedFilesystemReadPipe.this.logger.underlying.isWarnEnabled())
    VerticaDistributedFilesystemReadPipe.this.logger.underlying.warn(&quot;No cleanup info found.&quot;)
  else
    (): Unit)
}
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1006
        </td>
        <td>
          19632
          -
          19689
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.cleanup.CleanupUtilsInterface.checkAndCleanup
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.cleanupUtils.checkAndCleanup(VerticaDistributedFilesystemReadPipe.this.fileStoreLayer, cleanupInfo)
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          1005
        </td>
        <td>
          19661
          -
          19675
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.VerticaDistributedFilesystemReadPipe.fileStoreLayer
        </td>
        <td style="background: #AEF1AE">
          VerticaDistributedFilesystemReadPipe.this.fileStoreLayer
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          1007
        </td>
        <td>
          19863
          -
          19865
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          1012
        </td>
        <td>
          19967
          -
          19974
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.util.Right.apply
        </td>
        <td style="background: #AEF1AE">
          scala.`package`.Right.apply[Nothing, Unit](())
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>