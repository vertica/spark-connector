<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com\vertica\spark\datasource\v2\VerticaDatasourceV2Read.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>// (c) Copyright [2020-2021] Micro Focus or one of its affiliates.
</span>2 <span style=''>// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
</span>3 <span style=''>// You may not use this file except in compliance with the License.
</span>4 <span style=''>// You may obtain a copy of the License at
</span>5 <span style=''>//
</span>6 <span style=''>// http://www.apache.org/licenses/LICENSE-2.0
</span>7 <span style=''>//
</span>8 <span style=''>// Unless required by applicable law or agreed to in writing, software
</span>9 <span style=''>// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
</span>10 <span style=''>// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span>11 <span style=''>// See the License for the specific language governing permissions and
</span>12 <span style=''>// limitations under the License.
</span>13 <span style=''>
</span>14 <span style=''>package com.vertica.spark.datasource.v2
</span>15 <span style=''>
</span>16 <span style=''>import com.typesafe.scalalogging.Logger
</span>17 <span style=''>import org.apache.spark.sql.connector.read._
</span>18 <span style=''>import org.apache.spark.sql.types._
</span>19 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>20 <span style=''>import com.vertica.spark.config.{DistributedFilesystemReadConfig, LogProvider, ReadConfig}
</span>21 <span style=''>import com.vertica.spark.datasource.core.{DSConfigSetupInterface, DSReader, DSReaderInterface}
</span>22 <span style=''>import com.vertica.spark.util.error.{ConnectorError, ErrorHandling, InitialSetupPartitioningError}
</span>23 <span style=''>import com.vertica.spark.util.listeners.ApplicationParquetCleaner
</span>24 <span style=''>import com.vertica.spark.util.error.{ConnectorError, ConnectorException, ErrorHandling, InitialSetupPartitioningError}
</span>25 <span style=''>import com.vertica.spark.util.pushdown.PushdownUtils
</span>26 <span style=''>import org.apache.spark.sql.connector.expressions.aggregate._
</span>27 <span style=''>import org.apache.spark.sql.sources.Filter
</span>28 <span style=''>
</span>29 <span style=''>trait PushdownFilter {
</span>30 <span style=''>  def getFilterString: String
</span>31 <span style=''>}
</span>32 <span style=''>
</span>33 <span style=''>case class PushFilter(filter: Filter, filterString: String) extends PushdownFilter {
</span>34 <span style=''>  def getFilterString: String = </span><span style='background: #AEF1AE'>this.filterString</span><span style=''>
</span>35 <span style=''>}
</span>36 <span style=''>
</span>37 <span style=''>case class PushdownAggregate(aggregation: Aggregation, aggregationString: String) {
</span>38 <span style=''>  def getAggregationString: String = </span><span style='background: #F0ADAD'>this.aggregationString</span><span style=''>
</span>39 <span style=''>}
</span>40 <span style=''>
</span>41 <span style=''>case class NonPushFilter(filter: Filter) extends AnyVal
</span>42 <span style=''>
</span>43 <span style=''>case class ExpectedRowDidNotExistError() extends ConnectorError {
</span>44 <span style=''>  def getFullContext: String = </span><span style='background: #AEF1AE'>&quot;Fatal error: expected row did not exist&quot;</span><span style=''>
</span>45 <span style=''>}
</span>46 <span style=''>
</span>47 <span style=''>case class AggregateNotSupported(aggregate: String) extends ConnectorError {
</span>48 <span style=''>  def getFullContext: String = </span><span style='background: #AEF1AE'>s&quot;$aggregate is not supported&quot;</span><span style=''>
</span>49 <span style=''>}
</span>50 <span style=''>
</span>51 <span style=''>case class UnknownColumnName(colName: String) extends ConnectorError {
</span>52 <span style=''>  def getFullContext: String = </span><span style='background: #F0ADAD'>s&quot;$colName could not be found&quot;</span><span style=''>
</span>53 <span style=''>}
</span>54 <span style=''>
</span>55 <span style=''>/**
</span>56 <span style=''>  * Builds the scan class for use in reading of Vertica
</span>57 <span style=''>  */
</span>58 <span style=''>class VerticaScanBuilder(config: ReadConfig, readConfigSetup: DSConfigSetupInterface[ReadConfig]) extends ScanBuilder with
</span>59 <span style=''>  SupportsPushDownFilters  with SupportsPushDownRequiredColumns {
</span>60 <span style=''>  protected var pushFilters: List[PushFilter] = </span><span style='background: #AEF1AE'>Nil</span><span style=''>
</span>61 <span style=''>
</span>62 <span style=''>  protected var requiredSchema: StructType = </span><span style='background: #AEF1AE'>StructType(Nil)</span><span style=''>
</span>63 <span style=''>
</span>64 <span style=''>  protected var aggPushedDown: Boolean = </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>65 <span style=''>
</span>66 <span style=''>  protected var groupBy: Array[StructField] = </span><span style='background: #AEF1AE'>Array()</span><span style=''>
</span>67 <span style=''>
</span>68 <span style=''>  protected val logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[VerticaScanBuilder])</span><span style=''>
</span>69 <span style=''>
</span>70 <span style=''>/**
</span>71 <span style=''>  * Builds the class representing a scan of a Vertica table
</span>72 <span style=''>  *
</span>73 <span style=''>  * @return [[VerticaScan]]
</span>74 <span style=''>  */
</span>75 <span style=''>  override def build(): Scan = {
</span>76 <span style=''>    val cfg = </span><span style='background: #AEF1AE'>config.copyConfig()</span><span style=''>
</span>77 <span style=''>    </span><span style='background: #AEF1AE'>cfg.setPushdownFilters(this.pushFilters)</span><span style=''>
</span>78 <span style=''>    </span><span style='background: #AEF1AE'>cfg.setRequiredSchema(this.requiredSchema)</span><span style=''>
</span>79 <span style=''>    </span><span style='background: #AEF1AE'>cfg.setPushdownAgg(this.aggPushedDown)</span><span style=''>
</span>80 <span style=''>    </span><span style='background: #AEF1AE'>cfg.setGroupBy(this.groupBy)</span><span style=''>
</span>81 <span style=''>    </span><span style='background: #AEF1AE'>new VerticaScan(cfg, readConfigSetup)</span><span style=''>
</span>82 <span style=''>  }
</span>83 <span style=''>
</span>84 <span style=''>  override def pushFilters(filters: Array[Filter]): Array[Filter] = {
</span>85 <span style=''>    val initialLists: (List[NonPushFilter], List[PushFilter]) = </span><span style='background: #AEF1AE'>(List(), List())</span><span style=''>
</span>86 <span style=''>    val (nonPushFilters, pushFilters): (List[NonPushFilter], List[PushFilter]) = filters
</span>87 <span style=''>      .map(PushdownUtils.genFilter)
</span>88 <span style=''>      .foldLeft(initialLists)((acc, filter) =&gt; {
</span>89 <span style=''>        val (nonPushFilters, pushFilters) = acc
</span>90 <span style=''>        filter match {
</span>91 <span style=''>          case Left(nonPushFilter) =&gt; (nonPushFilter :: nonPushFilters, pushFilters)
</span>92 <span style=''>          case Right(pushFilter) =&gt; (nonPushFilters, pushFilter :: pushFilters)
</span>93 <span style=''>        }
</span>94 <span style=''>      })
</span>95 <span style=''>
</span>96 <span style=''>    </span><span style='background: #AEF1AE'>this.pushFilters = pushFilters</span><span style=''>
</span>97 <span style=''>
</span>98 <span style=''>    </span><span style='background: #AEF1AE'>nonPushFilters.map(</span><span style='background: #F0ADAD'>_.filter</span><span style='background: #AEF1AE'>).toArray</span><span style=''>
</span>99 <span style=''>  }
</span>100 <span style=''>
</span>101 <span style=''>  override def pushedFilters(): Array[Filter] = {
</span>102 <span style=''>    </span><span style='background: #AEF1AE'>this.pushFilters.map(_.filter).toArray</span><span style=''>
</span>103 <span style=''>  }
</span>104 <span style=''>
</span>105 <span style=''>  override def pruneColumns(requiredSchema: StructType): Unit = {
</span>106 <span style=''>    </span><span style='background: #AEF1AE'>this.requiredSchema = requiredSchema</span><span style=''>
</span>107 <span style=''>    </span><span style='background: #AEF1AE'>this.aggPushedDown = false</span><span style=''>
</span>108 <span style=''>    </span><span style='background: #AEF1AE'>this.groupBy = Array()</span><span style=''>
</span>109 <span style=''>  }
</span>110 <span style=''>
</span>111 <span style=''>  protected def getColType(colName: String): DataType = {
</span>112 <span style=''>    </span><span style='background: #AEF1AE'>tableSchema.find(_.name.equalsIgnoreCase(colName))</span><span style=''> match {
</span>113 <span style=''>      case Some(col) =&gt; </span><span style='background: #AEF1AE'>col.dataType</span><span style=''>
</span>114 <span style=''>      case None =&gt; </span><span style='background: #F0ADAD'>ErrorHandling.logAndThrowError(logger, UnknownColumnName(colName))</span><span style=''>
</span>115 <span style=''>    }
</span>116 <span style=''>  }
</span>117 <span style=''>
</span>118 <span style=''>  protected def tableSchema: StructType = </span><span style='background: #AEF1AE'>readConfigSetup.getTableSchema(config)</span><span style=''> match {
</span>119 <span style=''>    case Right(schema) =&gt; schema
</span>120 <span style=''>    case Left(err) =&gt; </span><span style='background: #F0ADAD'>ErrorHandling.logAndThrowError(logger, err.context(&quot;Scan builder failed to get table schema&quot;))</span><span style=''>
</span>121 <span style=''>  }
</span>122 <span style=''>}
</span>123 <span style=''>
</span>124 <span style=''>class VerticaScanBuilderWithPushdown(config: ReadConfig, readConfigSetup: DSConfigSetupInterface[ReadConfig]) extends VerticaScanBuilder(config, readConfigSetup) with SupportsPushDownAggregates {
</span>125 <span style=''>
</span>126 <span style=''>  override def pushAggregation(aggregation: Aggregation): Boolean = {
</span>127 <span style=''>    try{
</span>128 <span style=''>      </span><span style='background: #AEF1AE'>val aggregatesStructFields: Array[StructField] = aggregation.aggregateExpressions().map {
</span>129 <span style=''></span><span style='background: #AEF1AE'>        case _: CountStar =&gt; </span><span style='background: #F0ADAD'>StructField(&quot;COUNT(*)&quot;, LongType, nullable = false, Metadata.empty)</span><span style='background: #AEF1AE'>
</span>130 <span style=''></span><span style='background: #AEF1AE'>        case aggregate: Count =&gt; StructField(aggregate.describe(), LongType, nullable = false, Metadata.empty)
</span>131 <span style=''></span><span style='background: #AEF1AE'>        case aggregate: Sum =&gt; </span><span style='background: #F0ADAD'>StructField(aggregate.describe(), getColType(aggregate.column().describe()), nullable = false, Metadata.empty)</span><span style='background: #AEF1AE'>
</span>132 <span style=''></span><span style='background: #AEF1AE'>        case aggregate: Min =&gt; </span><span style='background: #F0ADAD'>StructField(aggregate.describe(), getColType(aggregate.column().describe()), nullable = false, Metadata.empty)</span><span style='background: #AEF1AE'>
</span>133 <span style=''></span><span style='background: #AEF1AE'>        case aggregate: Max =&gt; StructField(aggregate.describe(), getColType(aggregate.column().describe()), nullable = false, Metadata.empty)
</span>134 <span style=''></span><span style='background: #AEF1AE'>        // short circuit
</span>135 <span style=''></span><span style='background: #AEF1AE'>        case aggregate =&gt; ErrorHandling.logAndThrowError(logger, AggregateNotSupported(aggregate.describe()))
</span>136 <span style=''></span><span style='background: #AEF1AE'>      }
</span>137 <span style=''></span><span style='background: #AEF1AE'>      val groupByColumnsStructFields = aggregation.groupByColumns.map(col =&gt; {
</span>138 <span style=''></span><span style='background: #AEF1AE'>        StructField(col.describe, getColType(col.describe), nullable = false, Metadata.empty)
</span>139 <span style=''></span><span style='background: #AEF1AE'>      })
</span>140 <span style=''></span><span style='background: #AEF1AE'>      this.requiredSchema = StructType(groupByColumnsStructFields ++ aggregatesStructFields)
</span>141 <span style=''></span><span style='background: #AEF1AE'>      this.groupBy = groupByColumnsStructFields
</span>142 <span style=''></span><span style='background: #AEF1AE'>      this.aggPushedDown = true</span><span style=''>
</span>143 <span style=''>    } catch{
</span>144 <span style=''>      case e: ConnectorException =&gt; </span><span style='background: #AEF1AE'>e.error</span><span style=''> match{
</span>145 <span style=''>        // This instance of builder may be reused, so we reset.
</span>146 <span style=''>        case _: AggregateNotSupported =&gt;
</span>147 <span style=''>          </span><span style='background: #AEF1AE'>this.requiredSchema = StructType(Nil)</span><span style=''>
</span>148 <span style=''>          </span><span style='background: #AEF1AE'>this.groupBy = Array()</span><span style=''>
</span>149 <span style=''>          </span><span style='background: #AEF1AE'>this.aggPushedDown = false</span><span style=''>
</span>150 <span style=''>        case _ =&gt; </span><span style='background: #F0ADAD'>throw e</span><span style=''>
</span>151 <span style=''>      }
</span>152 <span style=''>      case e: Exception =&gt; </span><span style='background: #F0ADAD'>throw e</span><span style=''>
</span>153 <span style=''>    }
</span>154 <span style=''>    // if false, the read is continued with no push down.
</span>155 <span style=''>    </span><span style='background: #AEF1AE'>this.aggPushedDown</span><span style=''>
</span>156 <span style=''>  }
</span>157 <span style=''>}
</span>158 <span style=''>
</span>159 <span style=''>/**
</span>160 <span style=''>  * Represents a scan of a Vertica table.
</span>161 <span style=''>  *
</span>162 <span style=''>  * Extends mixin class to represent type of read. Options are Batch or Stream, we are doing a batch read.
</span>163 <span style=''>  */
</span>164 <span style=''>class VerticaScan(config: ReadConfig, readConfigSetup: DSConfigSetupInterface[ReadConfig]) extends Scan with Batch {
</span>165 <span style=''>
</span>166 <span style=''>  private val logger: Logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[VerticaScan])</span><span style=''>
</span>167 <span style=''>
</span>168 <span style=''>  def getConfig: ReadConfig = </span><span style='background: #AEF1AE'>config</span><span style=''>
</span>169 <span style=''>
</span>170 <span style=''>  /**
</span>171 <span style=''>  * Schema of scan (can be different than full table schema)
</span>172 <span style=''>  */
</span>173 <span style=''>  override def readSchema(): StructType = {
</span>174 <span style=''>    (readConfigSetup.getTableSchema(config), config.getRequiredSchema) match {
</span>175 <span style=''>      case (Right(schema), requiredSchema) =&gt; if (</span><span style='background: #AEF1AE'>requiredSchema.nonEmpty</span><span style=''>) { </span><span style='background: #AEF1AE'>requiredSchema</span><span style=''> } else { </span><span style='background: #AEF1AE'>schema</span><span style=''> }
</span>176 <span style=''>      case (Left(err), _) =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, err)</span><span style=''>
</span>177 <span style=''>    }
</span>178 <span style=''>  }
</span>179 <span style=''>
</span>180 <span style=''>/**
</span>181 <span style=''>  * Returns this object as an instance of the Batch interface
</span>182 <span style=''>  */
</span>183 <span style=''>  override def toBatch: Batch = this
</span>184 <span style=''>
</span>185 <span style=''>
</span>186 <span style=''>/**
</span>187 <span style=''>  * Returns an array of partitions. These contain the information necesary for each reader to read it's portion of the data
</span>188 <span style=''>  */
</span>189 <span style=''>  override def planInputPartitions(): Array[InputPartition] = {
</span>190 <span style=''>   </span><span style='background: #AEF1AE'>readConfigSetup
</span>191 <span style=''></span><span style='background: #AEF1AE'>      .performInitialSetup(config)</span><span style=''> match {
</span>192 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, err)</span><span style=''>
</span>193 <span style=''>      case Right(opt) =&gt; opt match {
</span>194 <span style=''>        case None =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, InitialSetupPartitioningError())</span><span style=''>
</span>195 <span style=''>        case Some(partitionInfo) =&gt; </span><span style='background: #AEF1AE'>partitionInfo.partitionSeq</span><span style=''>
</span>196 <span style=''>      }
</span>197 <span style=''>    }
</span>198 <span style=''>  }
</span>199 <span style=''>
</span>200 <span style=''>
</span>201 <span style=''>/**
</span>202 <span style=''>  * Creates the reader factory which will be serialized and sent to workers
</span>203 <span style=''>  *
</span>204 <span style=''>  * @return [[VerticaReaderFactory]]
</span>205 <span style=''>  */
</span>206 <span style=''>  override def createReaderFactory(): PartitionReaderFactory = {
</span>207 <span style=''>    </span><span style='background: #AEF1AE'>new VerticaReaderFactory(config)</span><span style=''>
</span>208 <span style=''>  }
</span>209 <span style=''>}
</span>210 <span style=''>
</span>211 <span style=''>/**
</span>212 <span style=''>  * Factory class for creating the Vertica reader
</span>213 <span style=''>  *
</span>214 <span style=''>  * This class is seriazlized and sent to each worker node. On the worker, createReader will be called with the given partition of data for that worker.
</span>215 <span style=''>  */
</span>216 <span style=''>class VerticaReaderFactory(config: ReadConfig) extends PartitionReaderFactory {
</span>217 <span style=''>/**
</span>218 <span style=''>  * Called from the worker node to get the reader for that node
</span>219 <span style=''>  *
</span>220 <span style=''>  * @return [[VerticaBatchReader]]
</span>221 <span style=''>  */
</span>222 <span style=''>  override def createReader(partition: InputPartition): PartitionReader[InternalRow] =
</span>223 <span style=''>  {
</span>224 <span style=''>    </span><span style='background: #F0ADAD'>new VerticaBatchReader(config, new DSReader(config, partition))</span><span style=''>
</span>225 <span style=''>  }
</span>226 <span style=''>
</span>227 <span style=''>}
</span>228 <span style=''>
</span>229 <span style=''>/**
</span>230 <span style=''>  * Reader class that reads rows from the underlying datasource
</span>231 <span style=''>  */
</span>232 <span style=''>class VerticaBatchReader(config: ReadConfig, reader: DSReaderInterface) extends PartitionReader[InternalRow] {
</span>233 <span style=''>  private val logger: Logger = </span><span style='background: #AEF1AE'>LogProvider.getLogger(classOf[VerticaBatchReader])</span><span style=''>
</span>234 <span style=''>
</span>235 <span style=''>  // Open the read
</span>236 <span style=''>  </span><span style='background: #AEF1AE'>reader.openRead()</span><span style=''> match {
</span>237 <span style=''>    case Right(_) =&gt; </span><span style='background: #AEF1AE'>()</span><span style=''>
</span>238 <span style=''>    case Left(err) =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, err)</span><span style=''>
</span>239 <span style=''>  }
</span>240 <span style=''>
</span>241 <span style=''>  var row: Option[InternalRow] = </span><span style='background: #AEF1AE'>None</span><span style=''>
</span>242 <span style=''>
</span>243 <span style=''>/**
</span>244 <span style=''>  * Returns true if there are more rows to read
</span>245 <span style=''>  */
</span>246 <span style=''>  override def next: Boolean =
</span>247 <span style=''>  {
</span>248 <span style=''>    </span><span style='background: #AEF1AE'>reader.readRow()</span><span style=''> match {
</span>249 <span style=''>      case Left(err) =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, err)</span><span style=''>
</span>250 <span style=''>      case Right(r) =&gt;
</span>251 <span style=''>        </span><span style='background: #AEF1AE'>row = r</span><span style=''>
</span>252 <span style=''>    }
</span>253 <span style=''>    </span><span style='background: #AEF1AE'>row</span><span style=''> match {
</span>254 <span style=''>      case Some(_) =&gt; </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>255 <span style=''>      case None =&gt; </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>256 <span style=''>    }
</span>257 <span style=''>  }
</span>258 <span style=''>
</span>259 <span style=''>/**
</span>260 <span style=''>  * Return the current row
</span>261 <span style=''>  */
</span>262 <span style=''>  override def get: InternalRow = {
</span>263 <span style=''>    </span><span style='background: #AEF1AE'>row</span><span style=''> match {
</span>264 <span style=''>      case None =&gt; </span><span style='background: #F0ADAD'>ErrorHandling.logAndThrowError(logger, ExpectedRowDidNotExistError())</span><span style=''>
</span>265 <span style=''>      case Some(v) =&gt; v
</span>266 <span style=''>    }
</span>267 <span style=''>  }
</span>268 <span style=''>
</span>269 <span style=''>/**
</span>270 <span style=''>  * Calls underlying datasource to do any needed cleanup
</span>271 <span style=''>  */
</span>272 <span style=''>  def close(): Unit = {
</span>273 <span style=''>    </span><span style='background: #AEF1AE'>reader.closeRead()</span><span style=''> match {
</span>274 <span style=''>      case Right(_) =&gt; </span><span style='background: #AEF1AE'>()</span><span style=''>
</span>275 <span style=''>      case Left(e) =&gt; </span><span style='background: #AEF1AE'>ErrorHandling.logAndThrowError(logger, e)</span><span style=''>
</span>276 <span style=''>    }
</span>277 <span style=''>  }
</span>278 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          34
        </td>
        <td>
          1562
        </td>
        <td>
          1632
          -
          1649
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.PushFilter.filterString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.filterString
        </td>
      </tr><tr>
        <td>
          38
        </td>
        <td>
          1563
        </td>
        <td>
          1774
          -
          1796
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.PushdownAggregate.aggregationString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          this.aggregationString
        </td>
      </tr><tr>
        <td>
          44
        </td>
        <td>
          1564
        </td>
        <td>
          1954
          -
          1995
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;Fatal error: expected row did not exist&quot;
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          1566
        </td>
        <td>
          2119
          -
          2137
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; is not supported&quot;
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          1568
        </td>
        <td>
          2107
          -
          2137
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot; is not supported&quot;).s(AggregateNotSupported.this.aggregate)
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          1565
        </td>
        <td>
          2109
          -
          2110
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          48
        </td>
        <td>
          1567
        </td>
        <td>
          2110
          -
          2119
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.AggregateNotSupported.aggregate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateNotSupported.this.aggregate
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1569
        </td>
        <td>
          2245
          -
          2246
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1572
        </td>
        <td>
          2243
          -
          2273
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; could not be found&quot;).s(UnknownColumnName.this.colName)
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1571
        </td>
        <td>
          2246
          -
          2253
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.UnknownColumnName.colName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          UnknownColumnName.this.colName
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1570
        </td>
        <td>
          2253
          -
          2273
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; could not be found&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          1573
        </td>
        <td>
          2579
          -
          2582
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          1575
        </td>
        <td>
          2629
          -
          2644
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.collection.immutable.Nil)
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          1574
        </td>
        <td>
          2640
          -
          2643
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          1576
        </td>
        <td>
          2687
          -
          2692
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          1577
        </td>
        <td>
          2740
          -
          2747
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[org.apache.spark.sql.types.StructField]()((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          1578
        </td>
        <td>
          2774
          -
          2824
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.v2.VerticaScanBuilder])
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          1579
        </td>
        <td>
          2974
          -
          2993
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ReadConfig.copyConfig
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilder.this.config.copyConfig()
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          1581
        </td>
        <td>
          2998
          -
          3038
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ReadConfig.setPushdownFilters
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cfg.setPushdownFilters(this.pushFilters)
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          1580
        </td>
        <td>
          3021
          -
          3037
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.pushFilters
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.pushFilters
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          1583
        </td>
        <td>
          3043
          -
          3085
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ReadConfig.setRequiredSchema
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cfg.setRequiredSchema(this.requiredSchema)
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          1582
        </td>
        <td>
          3065
          -
          3084
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.requiredSchema
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.requiredSchema
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          1584
        </td>
        <td>
          3109
          -
          3127
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.aggPushedDown
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.aggPushedDown
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          1585
        </td>
        <td>
          3090
          -
          3128
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ReadConfig.setPushdownAgg
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cfg.setPushdownAgg(this.aggPushedDown)
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          1587
        </td>
        <td>
          3133
          -
          3161
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.ReadConfig.setGroupBy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cfg.setGroupBy(this.groupBy)
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          1586
        </td>
        <td>
          3148
          -
          3160
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.groupBy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.groupBy
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          1589
        </td>
        <td>
          3166
          -
          3203
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new VerticaScan(cfg, VerticaScanBuilder.this.readConfigSetup)
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          1588
        </td>
        <td>
          3187
          -
          3202
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.readConfigSetup
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilder.this.readConfigSetup
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          1590
        </td>
        <td>
          3344
          -
          3350
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          1592
        </td>
        <td>
          3343
          -
          3359
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[List[Nothing], List[Nothing]](scala.collection.immutable.Nil, scala.collection.immutable.Nil)
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          1591
        </td>
        <td>
          3352
          -
          3358
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          1593
        </td>
        <td>
          3369
          -
          3369
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._1
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          1594
        </td>
        <td>
          3385
          -
          3385
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._2
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          1595
        </td>
        <td>
          3794
          -
          3824
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.pushFilters_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.pushFilters_=(pushFilters)
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          1596
        </td>
        <td>
          3849
          -
          3857
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.NonPushFilter.filter
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$5.filter
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          1598
        </td>
        <td>
          3830
          -
          3866
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nonPushFilters.map[org.apache.spark.sql.sources.Filter, List[org.apache.spark.sql.sources.Filter]](((x$5: com.vertica.spark.datasource.v2.NonPushFilter) =&gt; x$5.filter))(immutable.this.List.canBuildFrom[org.apache.spark.sql.sources.Filter]).toArray[org.apache.spark.sql.sources.Filter]((ClassTag.apply[org.apache.spark.sql.sources.Filter](classOf[org.apache.spark.sql.sources.Filter]): scala.reflect.ClassTag[org.apache.spark.sql.sources.Filter]))
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          1597
        </td>
        <td>
          3848
          -
          3848
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[org.apache.spark.sql.sources.Filter]
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          1599
        </td>
        <td>
          3947
          -
          3955
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.PushFilter.filter
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6.filter
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          1601
        </td>
        <td>
          3926
          -
          3964
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.pushFilters.map[org.apache.spark.sql.sources.Filter, List[org.apache.spark.sql.sources.Filter]](((x$6: com.vertica.spark.datasource.v2.PushFilter) =&gt; x$6.filter))(immutable.this.List.canBuildFrom[org.apache.spark.sql.sources.Filter]).toArray[org.apache.spark.sql.sources.Filter]((ClassTag.apply[org.apache.spark.sql.sources.Filter](classOf[org.apache.spark.sql.sources.Filter]): scala.reflect.ClassTag[org.apache.spark.sql.sources.Filter]))
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          1600
        </td>
        <td>
          3946
          -
          3946
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[org.apache.spark.sql.sources.Filter]
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          1602
        </td>
        <td>
          4040
          -
          4076
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.requiredSchema_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.requiredSchema_=(requiredSchema)
        </td>
      </tr><tr>
        <td>
          107
        </td>
        <td>
          1603
        </td>
        <td>
          4081
          -
          4107
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.aggPushedDown_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.aggPushedDown_=(false)
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1605
        </td>
        <td>
          4112
          -
          4134
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.groupBy_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.groupBy_=(scala.Array.apply[org.apache.spark.sql.types.StructField]()((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1604
        </td>
        <td>
          4127
          -
          4134
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[org.apache.spark.sql.types.StructField]()((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1607
        </td>
        <td>
          4202
          -
          4252
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.find
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilder.this.tableSchema.find(((x$7: org.apache.spark.sql.types.StructField) =&gt; x$7.name.equalsIgnoreCase(colName)))
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1606
        </td>
        <td>
          4219
          -
          4251
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.equalsIgnoreCase
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7.name.equalsIgnoreCase(colName)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1608
        </td>
        <td>
          4285
          -
          4297
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          col.dataType
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1611
        </td>
        <td>
          4317
          -
          4383
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScanBuilder.this.logger, UnknownColumnName.apply(colName))
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1610
        </td>
        <td>
          4356
          -
          4382
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.UnknownColumnName.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          UnknownColumnName.apply(colName)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1609
        </td>
        <td>
          4348
          -
          4354
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.logger
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaScanBuilder.this.logger
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          1613
        </td>
        <td>
          4437
          -
          4475
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupInterface.getTableSchema
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilder.this.readConfigSetup.getTableSchema(VerticaScanBuilder.this.config)
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          1612
        </td>
        <td>
          4468
          -
          4474
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.config
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilder.this.config
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1614
        </td>
        <td>
          4570
          -
          4576
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.logger
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaScanBuilder.this.logger
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1616
        </td>
        <td>
          4539
          -
          4633
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScanBuilder.this.logger, err.context(&quot;Scan builder failed to get table schema&quot;))
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1615
        </td>
        <td>
          4578
          -
          4632
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorError.context
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          err.context(&quot;Scan builder failed to get table schema&quot;)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          1668
        </td>
        <td>
          4923
          -
          6144
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val aggregatesStructFields: Array[org.apache.spark.sql.types.StructField] = scala.Predef.refArrayOps[org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc](aggregation.aggregateExpressions()).map[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](((x0$1: org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc) =&gt; x0$1 match {
    case (_: org.apache.spark.sql.connector.expressions.aggregate.CountStar) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;COUNT(*)&quot;, org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
    case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Count)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
    case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Sum)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
    case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Min)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
    case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Max)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
    case (aggregate @ _) =&gt; com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScanBuilderWithPushdown.this.logger, AggregateNotSupported.apply(aggregate.describe()))
  }))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])));
  val groupByColumnsStructFields: Array[org.apache.spark.sql.types.StructField] = scala.Predef.refArrayOps[org.apache.spark.sql.connector.expressions.NamedReference](aggregation.groupByColumns()).map[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](((col: org.apache.spark.sql.connector.expressions.NamedReference) =&gt; org.apache.spark.sql.types.StructField.apply(col.describe(), VerticaScanBuilderWithPushdown.this.getColType(col.describe()), false, org.apache.spark.sql.types.Metadata.empty)))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])));
  this.requiredSchema_=(org.apache.spark.sql.types.StructType.apply(scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](groupByColumnsStructFields).++[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](aggregatesStructFields))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))));
  this.groupBy_=(groupByColumnsStructFields);
  this.aggPushedDown_=(true)
}
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          1650
        </td>
        <td>
          5011
          -
          5011
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          1617
        </td>
        <td>
          4972
          -
          5006
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Aggregation.aggregateExpressions
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregation.aggregateExpressions()
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          1651
        </td>
        <td>
          4972
          -
          5789
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc](aggregation.aggregateExpressions()).map[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](((x0$1: org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc) =&gt; x0$1 match {
  case (_: org.apache.spark.sql.connector.expressions.aggregate.CountStar) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;COUNT(*)&quot;, org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
  case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Count)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
  case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Sum)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
  case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Min)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
  case (aggregate @ (_: org.apache.spark.sql.connector.expressions.aggregate.Max)) =&gt; org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
  case (aggregate @ _) =&gt; com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScanBuilderWithPushdown.this.logger, AggregateNotSupported.apply(aggregate.describe()))
}))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1620
        </td>
        <td>
          5087
          -
          5092
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1619
        </td>
        <td>
          5066
          -
          5074
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1622
        </td>
        <td>
          5042
          -
          5109
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply(&quot;COUNT(*)&quot;, org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1618
        </td>
        <td>
          5054
          -
          5064
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;COUNT(*)&quot;
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1621
        </td>
        <td>
          5094
          -
          5108
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1623
        </td>
        <td>
          5155
          -
          5175
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Count.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregate.describe()
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1626
        </td>
        <td>
          5205
          -
          5219
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1625
        </td>
        <td>
          5198
          -
          5203
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1624
        </td>
        <td>
          5177
          -
          5185
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          1627
        </td>
        <td>
          5143
          -
          5220
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(aggregate.describe(), org.apache.spark.sql.types.LongType, false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1629
        </td>
        <td>
          5297
          -
          5326
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          aggregate.column().describe()
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1632
        </td>
        <td>
          5347
          -
          5361
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1628
        </td>
        <td>
          5264
          -
          5284
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Sum.describe
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          aggregate.describe()
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1631
        </td>
        <td>
          5340
          -
          5345
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1633
        </td>
        <td>
          5252
          -
          5362
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          1630
        </td>
        <td>
          5286
          -
          5327
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.getColType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe())
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1638
        </td>
        <td>
          5489
          -
          5503
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1635
        </td>
        <td>
          5439
          -
          5468
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          aggregate.column().describe()
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1634
        </td>
        <td>
          5406
          -
          5426
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Min.describe
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          aggregate.describe()
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1637
        </td>
        <td>
          5482
          -
          5487
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1636
        </td>
        <td>
          5428
          -
          5469
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.getColType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe())
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1639
        </td>
        <td>
          5394
          -
          5504
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1641
        </td>
        <td>
          5581
          -
          5610
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregate.column().describe()
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1644
        </td>
        <td>
          5631
          -
          5645
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1643
        </td>
        <td>
          5624
          -
          5629
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1640
        </td>
        <td>
          5548
          -
          5568
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Max.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregate.describe()
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1642
        </td>
        <td>
          5570
          -
          5611
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.getColType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe())
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          1645
        </td>
        <td>
          5536
          -
          5646
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(aggregate.describe(), VerticaScanBuilderWithPushdown.this.getColType(aggregate.column().describe()), false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1647
        </td>
        <td>
          5759
          -
          5779
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregate.describe()
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1646
        </td>
        <td>
          5729
          -
          5735
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilderWithPushdown.this.logger
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1649
        </td>
        <td>
          5698
          -
          5781
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScanBuilderWithPushdown.this.logger, AggregateNotSupported.apply(aggregate.describe()))
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          1648
        </td>
        <td>
          5737
          -
          5780
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.AggregateNotSupported.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateNotSupported.apply(aggregate.describe())
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          1659
        </td>
        <td>
          5859
          -
          5859
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          1652
        </td>
        <td>
          5829
          -
          5855
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.aggregate.Aggregation.groupByColumns
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggregation.groupByColumns()
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          1660
        </td>
        <td>
          5829
          -
          5971
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.connector.expressions.NamedReference](aggregation.groupByColumns()).map[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](((col: org.apache.spark.sql.connector.expressions.NamedReference) =&gt; org.apache.spark.sql.types.StructField.apply(col.describe(), VerticaScanBuilderWithPushdown.this.getColType(col.describe()), false, org.apache.spark.sql.types.Metadata.empty)))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1656
        </td>
        <td>
          5940
          -
          5945
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1653
        </td>
        <td>
          5889
          -
          5901
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          col.describe()
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1655
        </td>
        <td>
          5903
          -
          5927
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.getColType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScanBuilderWithPushdown.this.getColType(col.describe())
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1658
        </td>
        <td>
          5877
          -
          5962
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(col.describe(), VerticaScanBuilderWithPushdown.this.getColType(col.describe()), false, org.apache.spark.sql.types.Metadata.empty)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1657
        </td>
        <td>
          5947
          -
          5961
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Metadata.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.Metadata.empty
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1654
        </td>
        <td>
          5914
          -
          5926
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.connector.expressions.Expression.describe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          col.describe()
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1665
        </td>
        <td>
          5978
          -
          6064
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.requiredSchema_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.requiredSchema_=(org.apache.spark.sql.types.StructType.apply(scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](groupByColumnsStructFields).++[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](aggregatesStructFields))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))))
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1662
        </td>
        <td>
          6038
          -
          6038
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1661
        </td>
        <td>
          6041
          -
          6063
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.Predef.refArrayOps
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](aggregatesStructFields)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1664
        </td>
        <td>
          6000
          -
          6064
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](groupByColumnsStructFields).++[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](aggregatesStructFields))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))))
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1663
        </td>
        <td>
          6011
          -
          6063
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.++
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](groupByColumnsStructFields).++[org.apache.spark.sql.types.StructField, Array[org.apache.spark.sql.types.StructField]](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](aggregatesStructFields))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.StructField]((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          1666
        </td>
        <td>
          6071
          -
          6112
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.groupBy_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.groupBy_=(groupByColumnsStructFields)
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          1667
        </td>
        <td>
          6119
          -
          6144
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.aggPushedDown_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.aggPushedDown_=(true)
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          1669
        </td>
        <td>
          6194
          -
          6201
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.util.error.ConnectorException.error
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          e.error
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          1671
        </td>
        <td>
          6346
          -
          6361
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.collection.immutable.Nil)
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          1670
        </td>
        <td>
          6357
          -
          6360
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          1672
        </td>
        <td>
          6324
          -
          6361
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.requiredSchema_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.requiredSchema_=(org.apache.spark.sql.types.StructType.apply(scala.collection.immutable.Nil))
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          1674
        </td>
        <td>
          6372
          -
          6394
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.groupBy_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.groupBy_=(scala.Array.apply[org.apache.spark.sql.types.StructField]()((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField])))
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          1673
        </td>
        <td>
          6387
          -
          6394
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[org.apache.spark.sql.types.StructField]()((ClassTag.apply[org.apache.spark.sql.types.StructField](classOf[org.apache.spark.sql.types.StructField]): scala.reflect.ClassTag[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          1675
        </td>
        <td>
          6405
          -
          6431
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.aggPushedDown_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.aggPushedDown_=(false)
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          1676
        </td>
        <td>
          6450
          -
          6457
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw e
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          1677
        </td>
        <td>
          6493
          -
          6500
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw e
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1678
        </td>
        <td>
          6569
          -
          6587
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScanBuilder.aggPushedDown
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.aggPushedDown
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          1679
        </td>
        <td>
          6906
          -
          6949
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.v2.VerticaScan])
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          1680
        </td>
        <td>
          6981
          -
          6987
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.config
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.config
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1683
        </td>
        <td>
          7285
          -
          7291
        </td>
        <td>
          Ident
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.schema
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1682
        </td>
        <td>
          7261
          -
          7275
        </td>
        <td>
          Ident
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.requiredSchema
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          requiredSchema
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1681
        </td>
        <td>
          7234
          -
          7257
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          requiredSchema.nonEmpty
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          1685
        </td>
        <td>
          7323
          -
          7366
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScan.this.logger, err)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          1684
        </td>
        <td>
          7354
          -
          7360
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.logger
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          1686
        </td>
        <td>
          7731
          -
          7737
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.config
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.config
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          1687
        </td>
        <td>
          7688
          -
          7738
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSConfigSetupInterface.performInitialSetup
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.readConfigSetup.performInitialSetup(VerticaScan.this.config)
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          1689
        </td>
        <td>
          7771
          -
          7814
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScan.this.logger, err)
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          1688
        </td>
        <td>
          7802
          -
          7808
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.logger
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          1692
        </td>
        <td>
          7873
          -
          7944
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaScan.this.logger, com.vertica.spark.util.error.InitialSetupPartitioningError.apply())
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          1691
        </td>
        <td>
          7912
          -
          7943
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.InitialSetupPartitioningError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.InitialSetupPartitioningError.apply()
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          1690
        </td>
        <td>
          7904
          -
          7910
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.logger
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          1693
        </td>
        <td>
          7981
          -
          8007
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.PartitionInfo.partitionSeq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          partitionInfo.partitionSeq
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          1695
        </td>
        <td>
          8223
          -
          8255
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaReaderFactory.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new VerticaReaderFactory(VerticaScan.this.config)
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          1694
        </td>
        <td>
          8248
          -
          8254
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaScan.config
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaScan.this.config
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1698
        </td>
        <td>
          8797
          -
          8797
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReader.&lt;init&gt;$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          core.this.DSReader.&lt;init&gt;$default$3
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1697
        </td>
        <td>
          8810
          -
          8816
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaReaderFactory.config
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaReaderFactory.this.config
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1700
        </td>
        <td>
          8766
          -
          8829
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new VerticaBatchReader(VerticaReaderFactory.this.config, new com.vertica.spark.datasource.core.DSReader(VerticaReaderFactory.this.config, partition, core.this.DSReader.&lt;init&gt;$default$3))
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1696
        </td>
        <td>
          8789
          -
          8795
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaReaderFactory.config
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaReaderFactory.this.config
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1699
        </td>
        <td>
          8797
          -
          8828
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReader.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new com.vertica.spark.datasource.core.DSReader(VerticaReaderFactory.this.config, partition, core.this.DSReader.&lt;init&gt;$default$3)
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          1701
        </td>
        <td>
          9053
          -
          9103
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.config.LogProvider.getLogger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.config.LogProvider.getLogger(classOf[com.vertica.spark.datasource.v2.VerticaBatchReader])
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          1702
        </td>
        <td>
          9126
          -
          9143
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReaderInterface.openRead
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.reader.openRead()
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          1703
        </td>
        <td>
          9173
          -
          9175
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          1704
        </td>
        <td>
          9229
          -
          9235
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.logger
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          1705
        </td>
        <td>
          9198
          -
          9241
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaBatchReader.this.logger, err)
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          1706
        </td>
        <td>
          9280
          -
          9284
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          1707
        </td>
        <td>
          9382
          -
          9398
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReaderInterface.readRow
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.reader.readRow()
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          1709
        </td>
        <td>
          9431
          -
          9474
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaBatchReader.this.logger, err)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          1708
        </td>
        <td>
          9462
          -
          9468
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.logger
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          1710
        </td>
        <td>
          9506
          -
          9513
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.row_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.row_=(r)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          1711
        </td>
        <td>
          9524
          -
          9527
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.row
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.row
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          1712
        </td>
        <td>
          9558
          -
          9562
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          1713
        </td>
        <td>
          9582
          -
          9587
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          263
        </td>
        <td>
          1714
        </td>
        <td>
          9675
          -
          9678
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.row
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.row
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          1716
        </td>
        <td>
          9745
          -
          9774
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.v2.ExpectedRowDidNotExistError.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ExpectedRowDidNotExistError.apply()
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          1715
        </td>
        <td>
          9737
          -
          9743
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.logger
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          VerticaBatchReader.this.logger
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          1717
        </td>
        <td>
          9706
          -
          9775
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaBatchReader.this.logger, ExpectedRowDidNotExistError.apply())
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          1718
        </td>
        <td>
          9905
          -
          9923
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.datasource.core.DSReaderInterface.closeRead
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.reader.closeRead()
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          1719
        </td>
        <td>
          9955
          -
          9957
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1721
        </td>
        <td>
          9980
          -
          10021
        </td>
        <td>
          Apply
        </td>
        <td>
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.vertica.spark.util.error.ErrorHandling.logAndThrowError(VerticaBatchReader.this.logger, e)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1720
        </td>
        <td>
          10011
          -
          10017
        </td>
        <td>
          Select
        </td>
        <td>
          com.vertica.spark.datasource.v2.VerticaBatchReader.logger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          VerticaBatchReader.this.logger
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>