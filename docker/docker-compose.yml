version: "3.9"
services:
  client:
    build:
      context: ./client
      args:
        SPARK: ${SPARK_INSTALL:-latest}
    ports:
      # JVM Remote debug port
      - "5005:5005"
    volumes:
      - ./..:/spark-connector
      - ./vertica-hdfs-config/hadoop:/etc/hadoop/conf
    environment:
      - HADOOP_VERSION
      - SPARK_VERSION
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - GCS_FILEPATH
      - GCS_HMAC_KEY_ID
      - GCS_HMAC_KEY_SECRET
      - GCS_SERVICE_KEY_ID
      - GCS_SERVICE_KEY
      - GCS_SERVICE_EMAIL

  vertica:
    image: vertica/vertica-k8s:${VERTICA_VERSION:-latest}
    # See original entrypoint under https://github.com/vertica/vertica-kubernetes/blob/main/docker-vertica/Dockerfile
    entrypoint: bash -c "if [[ "$${VERTICA_VERSION:0:2}" == "10" ]]; then /opt/vertica/bin/docker-entrypoint-legacy.sh; else /opt/vertica/bin/docker-entrypoint.sh; fi"
    ports:
      - "5433:5433"
    volumes:
      # Legacy is for Vertica 10, otherwise Vertica 11+ use the same entrypoint script (Vertica 12 expects it under /usr/local/bin, but overridden by entrypoint above)
      - ./vertica/docker-entrypoint-legacy.sh:/opt/vertica/bin/docker-entrypoint-legacy.sh
      - ./vertica/docker-entrypoint.sh:/opt/vertica/bin/docker-entrypoint.sh
      - ./vertica-hdfs-config/hadoop:/etc/hadoop/conf
    environment:
      - VERTICA_VERSION

  hdfs:
    image: mdouchement/hdfs
    # See original command under https://github.com/mdouchement/docker-hdfs/blob/master/Dockerfile
    entrypoint: /usr/local/bin/docker-entrypoint.sh sleep infinity
    ports:
      - "22022:22"
      - "8020:8020"
      - "50010:50010"
      - "50020:50020"
      - "50070:50070"
      - "50075:50075"
    volumes:
      - ./hdfs/docker-entrypoint.sh:/usr/local/bin/docker-entrypoint.sh
      - ./vertica-hdfs-config/hadoop:/hadoop/conf
      - ./../functional-tests/src/main/resources/3.1.1:/partitioned

  spark:
    # Use a modified image to ensure the Spark and Python version are the same for all containers (Client, Spark, Jupyter)
    build:
      context: ./client
      args:
        SPARK: ${SPARK_INSTALL:-latest}
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - ./..:/spark-connector
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

  spark-worker:
    build:
      context: ./client
      args:
        SPARK: ${SPARK_INSTALL:-latest}
    volumes:
      - ./..:/spark-connector
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

  minio:
    image: minio/minio
    # See original entrypoint/command under https://github.com/minio/minio/blob/master/Dockerfile
    entrypoint: sh -c 'mkdir -p /data/test && minio server /data --console-address ":9001"'
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data

  jupyter:
    image: jupyter/all-spark-notebook
    ports:
      - "8888:8888"
    volumes:
      - ./jupyter-data:/home/jovyan/work
      - ./..:/spark-connector
    environment:
      - JUPYTER_TOKEN=test

volumes:
  minio-data:
