{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec60e67f-90e3-44eb-8359-620bc19869a2",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This example contains a demo of using Spark's Linear Regression algorithm along with the Vertica database. \n",
    "\n",
    "Old Faithful is a geyser that sits in Yellowstone National Park. Using Linear Regression we want to train a model that can predict how long an eruption will be based off it's feature value, the time taken between eruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13074165-ea2b-42ca-ac3f-5742a8f4587c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Setup\n",
    "\n",
    "First we start with the basics of setting up Spark to work with Vertica. To do this we need to create a Spark Context that has the Spark Connector passed through it as a configuration option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9fa45a-9d7b-4cfb-8912-b4082032af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark-connector/connector/target/scala-2.12/spark-vertica-connector-assembly-3.3.5.jar\n"
     ]
    }
   ],
   "source": [
    "# Get Connector JAR name\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(\"/spark-connector/connector/target/scala-2.12/spark-vertica-connector-assembly-*\")\n",
    "os.environ[\"CONNECTOR_JAR\"] = files[0]\n",
    "print(os.environ[\"CONNECTOR_JAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49796cae-62fe-4309-ac18-a27a97a47d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark session and context\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .config(\"spark.master\", \"spark://spark:7077\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.executor.memory\", \"1G\")\n",
    "    .config(\"spark.jars\", os.environ[\"CONNECTOR_JAR\"])\n",
    "    .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d4712-857f-4c35-b292-23bdd379d6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Data\n",
    "\n",
    "Our Faithful dataset has been randomly split up into two. One for training the model and one for testing it. Let's use Spark's native .csv reader to load these up into DataFrames. Then we can write each one to Vertica to their respective tables \"faithful_training\" and \"faithful_testing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd82faaf-eb9f-4534-8cb4-e703b4ca8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- eruptions: double (nullable = true)\n",
      " |-- waiting: integer (nullable = true)\n",
      "\n",
      "+---+---------+-------+\n",
      "| id|eruptions|waiting|\n",
      "+---+---------+-------+\n",
      "|  1|      3.6|     79|\n",
      "|  2|      1.8|     54|\n",
      "|  3|    3.333|     74|\n",
      "|  6|    2.883|     55|\n",
      "|  7|      4.7|     88|\n",
      "| 10|     4.35|     85|\n",
      "| 13|      4.2|     78|\n",
      "| 15|      4.7|     83|\n",
      "| 16|    2.167|     52|\n",
      "| 17|     1.75|     62|\n",
      "| 18|      4.8|     84|\n",
      "| 19|      1.6|     52|\n",
      "| 21|      1.8|     51|\n",
      "| 25|    4.533|     74|\n",
      "| 27|    1.967|     55|\n",
      "| 28|    4.083|     76|\n",
      "| 29|     3.85|     78|\n",
      "| 32|    4.467|     77|\n",
      "| 33|    3.367|     66|\n",
      "| 34|    4.033|     80|\n",
      "+---+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Data of the Old Faithful geyser in Yellowstone National Park.\n",
      "eruptions = duration of eruption \n",
      "waiting = time between eruptions\n"
     ]
    }
   ],
   "source": [
    "# Load the training set from a CSV file into a dataframe and show some if its contents\n",
    "df = spark.read.options(header=\"true\", inferschema=\"true\").csv(\"/spark-connector/examples/jupyter/data/faithful_training.csv\")\n",
    "df.printSchema()\n",
    "df.show() # So that we can see a bit of what our training dataset looks like and what the schema is\n",
    "\n",
    "# Write the data into a table in Vertica\n",
    "df.write.mode(\"overwrite\").format(\"com.vertica.spark.datasource.VerticaSource\").options(\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_training\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\").save()\n",
    "\n",
    "# Do the same for the testing set\n",
    "df = spark.read.options(header=\"true\", inferschema=\"true\").csv(\"/spark-connector/examples/jupyter/data/faithful_testing.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"com.vertica.spark.datasource.VerticaSource\").options(\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_testing\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\").save()\n",
    "\n",
    "print(\"Data of the Old Faithful geyser in Yellowstone National Park.\")\n",
    "print(\"eruptions = duration of eruption \\nwaiting = time between eruptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f179751-c865-4f15-ad41-4c20a0bce640",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Now that our data is saved in Vertica. We can read from both tables and store them once again in a Spark DF for processing using PySpark's ML toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca342460-65d8-453b-99ce-7bafaa5aad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data from Vertica into a Spark dataframe\n",
    "df_training = spark.read.load(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_training\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\")\n",
    "\n",
    "df_testing = spark.read.load(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_testing\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c3c48-1bd4-4154-83f3-9192bfb3e8dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select Features\n",
    "\n",
    "Linear Regression analyzes the relationship between an independant and dependant variable using a line of best fit. The dependant variable (eruptions) is what we are trying to predict, whereas the independant variables consists of our features that we are using to make our model. In this case we just have the one variable \"waiting\", and this will compose our features array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cebb786-6ca8-4875-92d7-02cb94d50409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+--------+\n",
      "| id|eruptions|waiting|features|\n",
      "+---+---------+-------+--------+\n",
      "|  1|      3.6|     79|  [79.0]|\n",
      "|  2|      1.8|     54|  [54.0]|\n",
      "|  3|    3.333|     74|  [74.0]|\n",
      "|  6|    2.883|     55|  [55.0]|\n",
      "|  7|      4.7|     88|  [88.0]|\n",
      "| 10|     4.35|     85|  [85.0]|\n",
      "| 13|      4.2|     78|  [78.0]|\n",
      "| 15|      4.7|     83|  [83.0]|\n",
      "| 16|    2.167|     52|  [52.0]|\n",
      "| 17|     1.75|     62|  [62.0]|\n",
      "| 18|      4.8|     84|  [84.0]|\n",
      "| 19|      1.6|     52|  [52.0]|\n",
      "| 21|      1.8|     51|  [51.0]|\n",
      "| 25|    4.533|     74|  [74.0]|\n",
      "| 27|    1.967|     55|  [55.0]|\n",
      "| 28|    4.083|     76|  [76.0]|\n",
      "| 29|     3.85|     78|  [78.0]|\n",
      "| 32|    4.467|     77|  [77.0]|\n",
      "| 33|    3.367|     66|  [66.0]|\n",
      "| 34|    4.033|     80|  [80.0]|\n",
      "+---+---------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Spark's ML Regression tool\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Spark's Linear Regression tool requires an array of the features we want to use. Since we only have one in this case, we add \"waiting\"\n",
    "featureassembler = VectorAssembler(inputCols = [\"waiting\"], outputCol = \"features\")\n",
    "\n",
    "# Show our new table with a features column added. We are also going to do the same with the testing table so we can compare our results later.\n",
    "df_testing = featureassembler.transform(df_testing)\n",
    "df_training = featureassembler.transform(df_training)\n",
    "df_training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042deda0-353d-45b8-8ec3-2f3c4d5abf4b",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We can now train our model against our training set. We specify our new features column as well as our target \"eruptions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f902c865-2a0e-4061-813e-c4a14d6aa052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+--------+------------------+\n",
      "| id|eruptions|waiting|features|        prediction|\n",
      "+---+---------+-------+--------+------------------+\n",
      "|  1|      3.6|     79|  [79.0]| 4.147894377732253|\n",
      "|  2|      1.8|     54|  [54.0]| 2.197896492376823|\n",
      "|  3|    3.333|     74|  [74.0]| 3.757894800661167|\n",
      "|  6|    2.883|     55|  [55.0]|2.2758964077910404|\n",
      "|  7|      4.7|     88|  [88.0]| 4.849893616460207|\n",
      "| 10|     4.35|     85|  [85.0]| 4.615893870217556|\n",
      "| 13|      4.2|     78|  [78.0]| 4.069894462318035|\n",
      "| 15|      4.7|     83|  [83.0]| 4.459894039389121|\n",
      "| 16|    2.167|     52|  [52.0]| 2.041896661548389|\n",
      "| 17|     1.75|     62|  [62.0]| 2.821895815690561|\n",
      "| 18|      4.8|     84|  [84.0]| 4.537893954803338|\n",
      "| 19|      1.6|     52|  [52.0]| 2.041896661548389|\n",
      "| 21|      1.8|     51|  [51.0]|1.9638967461341719|\n",
      "| 25|    4.533|     74|  [74.0]| 3.757894800661167|\n",
      "| 27|    1.967|     55|  [55.0]|2.2758964077910404|\n",
      "| 28|    4.083|     76|  [76.0]| 3.913894631489601|\n",
      "| 29|     3.85|     78|  [78.0]| 4.069894462318035|\n",
      "| 32|    4.467|     77|  [77.0]|3.9918945469038185|\n",
      "| 33|    3.367|     66|  [66.0]| 3.133895477347429|\n",
      "| 34|    4.033|     80|  [80.0]| 4.225894293146469|\n",
      "+---+---------+-------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create our model using the features to predict eruption duration and fit it against our training set\n",
    "lr = LinearRegression(maxIter=10, regParam=0.01, elasticNetParam=1, featuresCol=\"features\", labelCol=\"eruptions\")\n",
    "lr = lr.fit(df_training)\n",
    "\n",
    "\n",
    "training_predictions = lr.evaluate(df_training)\n",
    "training_predictions.predictions.show() # Show our new table with the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cab08-b25d-4f9f-a947-09b7d0b555d5",
   "metadata": {},
   "source": [
    "## Test Model & Results\n",
    "\n",
    "Our test data compromises of the missing eruption points in \"faithful_training.\" We are now going to use this dataset and compare it against our model to see how the predictions stack up. From there we also want to evaluate the model and see some statistics to show how our algorithm holds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b9a5ba0-183d-4bcb-86f8-db89861497d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------------+--------+\n",
      "| id|eruptions|        prediction|features|\n",
      "+---+---------+------------------+--------+\n",
      "|  4|    2.283| 2.821895815690561|  [62.0]|\n",
      "|  5|    4.533| 4.615893870217556|  [85.0]|\n",
      "|  8|      3.6| 4.615893870217556|  [85.0]|\n",
      "|  9|     1.95|1.9638967461341719|  [51.0]|\n",
      "| 11|    1.833| 2.197896492376823|  [54.0]|\n",
      "| 12|    3.917| 4.537893954803338|  [84.0]|\n",
      "| 14|     1.75|1.6518970844773033|  [47.0]|\n",
      "| 20|     4.25| 4.147894377732253|  [79.0]|\n",
      "| 22|     1.75|1.6518970844773033|  [47.0]|\n",
      "| 23|     3.45| 4.069894462318035|  [78.0]|\n",
      "| 24|    3.067|3.3678952235900805|  [69.0]|\n",
      "| 26|      3.6| 4.459894039389121|  [83.0]|\n",
      "| 30|    4.433| 4.147894377732253|  [79.0]|\n",
      "| 31|      4.3|3.6798948852469495|  [73.0]|\n",
      "| 35|    3.833| 3.757894800661167|  [74.0]|\n",
      "| 38|    4.833| 4.225894293146469|  [80.0]|\n",
      "| 42|    1.883| 2.509896154033692|  [58.0]|\n",
      "| 44|     1.75| 2.509896154033692|  [58.0]|\n",
      "| 47|    3.833| 2.977895646518995|  [64.0]|\n",
      "| 49|    4.633| 4.381894123974904|  [82.0]|\n",
      "+---+---------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.795085\n",
      "Root Mean Squared Error (RMSE) on test data = 0.510842\n"
     ]
    }
   ],
   "source": [
    "testing_predictions = lr.transform(df_testing)\n",
    "testing_predictions.select(\"id\", \"eruptions\",\"prediction\",\"features\").show(20)\n",
    "\n",
    "test_result = lr.evaluate(df_testing)\n",
    "print(\"R Squared (R2) on test data = %g\" % test_result.r2)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e035eb9-1d31-442e-963c-8bc2c07f1028",
   "metadata": {},
   "source": [
    "Residuals are a measure of how close our data points are to our regression line.\\\n",
    "R Squared is a statistical measure using residuals that gives us a percentage of variance in the dependant variable.\\\n",
    "RMSE is the root of the average error of these residuals. In general, an RMSE between 0.2 and 0.5 means the model can predict values accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a86e39-9cf6-45ee-bdda-aa1206381ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
