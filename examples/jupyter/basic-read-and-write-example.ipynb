{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf31794-8f24-417e-9778-8ab7125c80a0",
   "metadata": {},
   "source": [
    "# Basic Read/Write\n",
    "\n",
    "This example demonstrates how to perform a basic read and write using the Spark Connector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cdf0a-a607-4e60-9c24-6a86783112f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Setup\n",
    "\n",
    "First we start with the basics of setting up Spark to work with Vertica. To do this we need to create a Spark Context that has the Spark Connector passed through it as a configuration option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fa45a-9d7b-4cfb-8912-b4082032af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Connector JAR name\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(\"/spark-connector/connector/target/scala-2.12/spark-vertica-connector-assembly-*\")\n",
    "os.environ[\"CONNECTOR_JAR\"] = files[0]\n",
    "print(os.environ[\"CONNECTOR_JAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49796cae-62fe-4309-ac18-a27a97a47d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark session and context\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .config(\"spark.master\", \"spark://spark:7077\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.executor.memory\", \"1G\")\n",
    "    .config(\"spark.jars\", os.environ[\"CONNECTOR_JAR\"])\n",
    "    .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f7433-d786-403a-92ba-4e28b0f2caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the context information\n",
    "print(sc.version)\n",
    "print(sc.master)\n",
    "display(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968b2b5-2f4f-42b6-9ed1-e0f4a90c4e20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read/Write\n",
    "\n",
    "We can now build the schema we want. Since this is a basic example we can just use Python's native arrays and populate them with regards to column names as well as nested arrays for the data. \n",
    "\n",
    "We will now create our Spark DataFrame. However as we do that, we will also use the parallelize method to create an [RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html). This is a fundamental data structure that belongs to Spark and is used to parallelize data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebb786-6ca8-4875-92d7-02cb94d50409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a simple write then read using the Spark Connector\n",
    "columns = [\"language\", \"rating\"]\n",
    "data = [(\"Scala\", 71), (\"Java\", 89), (\"C++\", 67), (\"Python\", 94)]\n",
    "rdd = sc.parallelize(data)\n",
    "df = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267d088-688a-4b87-b850-6785117ce42c",
   "metadata": {},
   "source": [
    "Finally we can write our dataframe to the Vertica database \"docker\" to a table named \"jupytertest.\" We then read the table and once again store it into a Spark DataFrame for any processing we want to do with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ba135-76a3-41db-b568-b6ebba182302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").save(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"jupytertest\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/jupytertest\")\n",
    "\n",
    "df = spark.read.load(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"jupytertest\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/jupytertest\")\n",
    "df.rdd.collect()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
