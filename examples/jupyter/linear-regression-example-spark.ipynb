{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec60e67f-90e3-44eb-8359-620bc19869a2",
   "metadata": {},
   "source": [
    "# Linear Regression - Apache Spark\n",
    "\n",
    "This example contains a demo of using Spark's Linear Regression algorithm along with the Vertica database. \n",
    "\n",
    "Old Faithful is a geyser that sits in Yellowstone National Park. Using Linear Regression we want to train a model that can predict how long an eruption will be based off the time taken between eruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13074165-ea2b-42ca-ac3f-5742a8f4587c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Setup\n",
    "\n",
    "First we start with the basics of setting up Spark to work with Vertica. To do this we need to create a Spark Context that has the Spark Connector passed through it as a configuration option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fa45a-9d7b-4cfb-8912-b4082032af61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Connector JAR name\n",
    "import glob\n",
    "import os\n",
    "\n",
    "files = glob.glob(\"/spark-connector/connector/target/scala-2.12/spark-vertica-connector-assembly-*\")\n",
    "os.environ[\"CONNECTOR_JAR\"] = files[0]\n",
    "print(os.environ[\"CONNECTOR_JAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49796cae-62fe-4309-ac18-a27a97a47d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Spark session and context\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .config(\"spark.master\", \"spark://spark:7077\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.executor.memory\", \"1G\")\n",
    "    .config(\"spark.jars\", os.environ[\"CONNECTOR_JAR\"])\n",
    "    .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d4712-857f-4c35-b292-23bdd379d6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Data\n",
    "\n",
    "Our Faithful dataset has been randomly split up into two. One for training the model and one for testing it. Both sets are stored in a local .csv, so let's open them and copy them. We can then write each one to Vertica to their respective tables \"faithful_training\" and \"faithful_testing.\"\\\n",
    "Normally when performing training and testing in ML, we start one with one full dataset and use a function that randomly splits up the dataset. In our case however, we want the datasets the same across Vertica examples to be consistent with our training and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82faaf-eb9f-4534-8cb4-e703b4ca8ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training set from a CSV file into a dataframe and show some if its contents\n",
    "df = spark.read.options(header=\"true\", inferschema=\"true\").csv(\"/spark-connector/examples/jupyter/data/faithful_training.csv\")\n",
    "df.printSchema()\n",
    "df.show() # So that we can see a bit of what our training dataset looks like and what the schema is\n",
    "\n",
    "# Write the training data into a table in Vertica\n",
    "df.write.mode(\"overwrite\").format(\"com.vertica.spark.datasource.VerticaSource\").options(\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_training\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\").save()\n",
    "\n",
    "# Do the same write for the testing set\n",
    "df = spark.read.options(header=\"true\", inferschema=\"true\").csv(\"/spark-connector/examples/jupyter/data/faithful_testing.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"com.vertica.spark.datasource.VerticaSource\").options(\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_testing\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\").save()\n",
    "\n",
    "print(\"Data of the Old Faithful geyser in Yellowstone National Park.\")\n",
    "print(\"eruptions = duration of eruption \\nwaiting = time between eruptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f179751-c865-4f15-ad41-4c20a0bce640",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Now that our data is saved in Vertica. We can read from both tables and store them once again in a Spark DF for processing using PySpark's ML toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca342460-65d8-453b-99ce-7bafaa5aad1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read our training data from Vertica into a Spark dataframe\n",
    "df_training = spark.read.load(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_training\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\")\n",
    "\n",
    "# Read our testing data from Vertica into a Spark dataframe\n",
    "df_testing = spark.read.load(format=\"com.vertica.spark.datasource.VerticaSource\",\n",
    "    host=\"vertica\",\n",
    "    user=\"dbadmin\",\n",
    "    password=\"\",\n",
    "    db=\"docker\",\n",
    "    table=\"faithful_testing\",\n",
    "    staging_fs_url=\"webhdfs://hdfs:50070/linearregression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c3c48-1bd4-4154-83f3-9192bfb3e8dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select Features\n",
    "\n",
    "Linear Regression analyzes the relationship between an independant and dependant variable using a line of best fit. The dependant variable (eruptions) is what we are trying to predict, whereas the independant variables consists of our features that we are using to make our model. In this case we just have the one variable \"waiting\", and this will compose our features array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebb786-6ca8-4875-92d7-02cb94d50409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Spark's ML Regression tool\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Spark's Linear Regression tool requires an array of the features we want to use. Since we only have one in this case, we add \"waiting\"\n",
    "featureassembler = VectorAssembler(inputCols = [\"waiting\"], outputCol = \"features\")\n",
    "\n",
    "# Show our new table with a features column added. We are also going to do the same with the testing table so we can compare our results later.\n",
    "df_testing = featureassembler.transform(df_testing)\n",
    "df_training = featureassembler.transform(df_training)\n",
    "df_training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042deda0-353d-45b8-8ec3-2f3c4d5abf4b",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We can now train our model against our training set. We specify our new features column as well as our target \"eruptions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902c865-2a0e-4061-813e-c4a14d6aa052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our model using the features to predict eruption duration and fit it against our training set\n",
    "lr = LinearRegression(maxIter=10, regParam=0.01, elasticNetParam=1, featuresCol=\"features\", labelCol=\"eruptions\")\n",
    "lr = lr.fit(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cab08-b25d-4f9f-a947-09b7d0b555d5",
   "metadata": {},
   "source": [
    "## Test Model & Results\n",
    "\n",
    "Our test data comprises of the missing eruption points in \"faithful_training.\" We are now going to use this dataset and compare it against our model to see how the predictions stack up. From there we also want to evaluate the model and see some statistics to show how our algorithm holds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a5ba0-183d-4bcb-86f8-db89861497d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_predictions = lr.transform(df_testing)\n",
    "testing_predictions.select(\"id\", \"eruptions\",\"prediction\",\"features\").show(20)\n",
    "\n",
    "test_result = lr.evaluate(df_testing)\n",
    "print(\"R Squared (R^2) on test data = %g\" % test_result.r2)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e035eb9-1d31-442e-963c-8bc2c07f1028",
   "metadata": {},
   "source": [
    "**R Squared** is a calculation that provides us with a way of quantifying the relationship between our variables. \\\n",
    "It is a percentage, with 100% being a 1:1 relationship between our axis.\n",
    "\n",
    "**RMSE** is the average deviation of the dependant variables to the regression line. \\\n",
    "As such, a value closer to 0 means there is less deviation and therefore less error. Given our unit dimensions (minutes) An RMSE under 0.5 means the model can likely predict values accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
